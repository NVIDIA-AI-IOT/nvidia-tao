{"cells":[{"cell_type":"markdown","metadata":{"id":"fB_rZ66GGwPx"},"source":["# Train Adapt Optimize (TAO) Toolkit"]},{"cell_type":"markdown","metadata":{"id":"Sc2Vlu9mGwPz"},"source":["Train Adapt Optimize (TAO) Toolkit  is a python based AI toolkit for taking purpose-built pre-trained AI models and customizing them with your own data. \n","\n","Transfer learning extracts learned features from an existing neural network to a new one. Transfer learning is often used when creating a large training dataset is not feasible. \n","\n","Developers, researchers and software partners building intelligent vision AI apps and services, can bring their own data to fine-tune pre-trained models instead of going through the hassle of training from scratch."]},{"cell_type":"markdown","metadata":{"id":"_SbEFCYrGwPz"},"source":["![Train Adapt Optimize (TAO) Toolkit](https://developer.nvidia.com/sites/default/files/akamai/TAO/tlt-tao-toolkit-bring-your-own-model-diagram.png)"]},{"cell_type":"markdown","metadata":{"id":"_ajsLcQaGwP0"},"source":["The goal of this toolkit is to reduce that 80 hour workload to an 8 hour workload, which can enable data scientist to have considerably more train-test iterations in the same time frame.\n","\n","Let's see this in action with a use case for Automatic Speech Recognition!\n","\n","#### Note\n","1. This notebook uses AN4 dataset by default, which should be around ~91 MB.\n","1. Using the default config/spec file provided in this notebook, each weight file size of speech_to_text-citrinet created during training will be ~5 MB"]},{"cell_type":"markdown","metadata":{"id":"QR3k8OS8GwP0"},"source":["## Automatic Speech Recognition"]},{"cell_type":"markdown","metadata":{"id":"7HOhukVLGwP0"},"source":["Automatic Speech Recognition (ASR) is often the first step in building a Conversational AI model. An ASR model converts audible speech into text. The main metric for these models is to reduce Word Error Rate (WER) while transcribing the text. Simply put, the goal is to take an audio file and transcribe it.\n","\n","In this work, we are going to discuss the CitriNet model, which is an end to end ASR model which take in audio and produce text.\n","\n","CitriNet is a descendent of QuartzNet that features the squeeze-and-excitation(SE) block and subword tokenization and has a better accuracy/performance than QuartzNet.\n","\n","![CitriNet with CTC](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/_images/citrinet_vertical.png)"]},{"cell_type":"markdown","metadata":{"id":"jBdjK77dKOQn"},"source":["## Connect to a GPU Runtime\n","\n","1.   Change Runtime type to GPU by Runtime(Top Left tab)->Change Runtime Type->GPU(Hardware Accelerator)\n","2.   Then click on Connect (Top Right)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"38R-3LAixO-h"},"source":["## Mounting Google drive\n","Mount your Google drive storage to this Colab instance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dt69GSQgo5f3"},"outputs":[],"source":["try:\n","    import google.colab\n","    %env GOOGLE_COLAB=1\n","    from google.colab import drive\n","    drive.mount('/content/drive', force_remount=True)\n","except:\n","    %env GOOGLE_COLAB=0\n","    print(\"Warning: Not a Colab Environment\")"]},{"cell_type":"markdown","metadata":{"id":"iMrKJGSaxnmE"},"source":["## Setup Python Environment\n","Setup the environment necessary to run the TAO Networks by running the bash script"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GsF_BUsxt0Lx"},"outputs":[],"source":["import os\n","#FIXME\n","%env GENERAL_WHL_PATH=/content/drive/MyDrive/pyt/general_whl\n","#FIXME\n","%env CODEBASE_WHL_PATH=/content/drive/MyDrive/pyt/codebase_whl\n","#FIXME\n","%env COLAB_NOTEBOOKS_PATH=/content/drive/MyDrive/ColabNotebooks\n","if not os.path.exists(os.environ[\"COLAB_NOTEBOOKS_PATH\"]):\n","    raise(\"Error, enter the path of the colab notebooks repo correctly\")\n","\n","if os.path.exists(os.environ[\"GENERAL_WHL_PATH\"]) and os.path.exists(os.environ[\"GENERAL_WHL_PATH\"]):\n","    if os.environ[\"GOOGLE_COLAB\"] == \"1\":\n","        os.environ[\"bash_script\"] = \"setup_env.sh\"\n","    else:\n","        os.environ[\"bash_script\"] = \"setup_env_desktop.sh\"\n","\n","    !sed -i \"s|PATH_TO_GENERAL_WHL|$GENERAL_WHL_PATH|g\" $COLAB_NOTEBOOKS_PATH/pytorch/$bash_script\n","    !sed -i \"s|PATH_TO_CODEBASE_WHL|$CODEBASE_WHL_PATH|g\" $COLAB_NOTEBOOKS_PATH/pytorch/$bash_script\n","    !sed -i \"s|PATH_TO_COLAB_NOTEBOOKS|$COLAB_NOTEBOOKS_PATH|g\" $COLAB_NOTEBOOKS_PATH/pytorch/$bash_script\n","\n","    !sh $COLAB_NOTEBOOKS_PATH/pytorch/$bash_script\n","else:\n","    raise(\"Error, enter the whl paths correctly\")"]},{"cell_type":"markdown","metadata":{"id":"_p8F7bceGwP2"},"source":["---\n","## Let's Dig in: ASR using TAO"]},{"cell_type":"markdown","metadata":{"id":"HxOaeMpOGwP2"},"source":["### Set Relevant Paths"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FOtWn0WdGwP3"},"outputs":[],"source":["%env TAO_DOCKER_DISABLE=1\n","\n","# NOTE: The following paths are set from the perspective of the TAO Docker. \n","\n","# The data is saved here\n","%env DATA_DIR=/data/asr\n","!sudo mkdir -p $DATA_DIR && sudo chmod -R 777 $DATA_DIR\n","\n","# The configuration files are stored here\n","%env SPECS_DIR=/specs/asr\n","!sudo mkdir -p $SPECS_DIR && sudo chmod -R 777 $SPECS_DIR\n","\n","# The results are saved at this path\n","%env RESULTS_DIR=/results/asr\n","!sudo mkdir -p $RESULTS_DIR && sudo chmod -R 777 $RESULTS_DIR\n","\n","# Set your encryption key, and use the same key for all commands\n","%env KEY=tlt_encode"]},{"cell_type":"markdown","metadata":{"id":"VaWG4_pJGwP3"},"source":["Now that everything is setup, we would like to take a bit of time to explain the tao interface for ease of use. The command structure can be broken down as follows: `tao <task name> <subcommand>` <br> \n","\n","Let's see this in further detail."]},{"cell_type":"markdown","metadata":{"id":"iqRlgHywGwP3"},"source":["\n","### Downloading Specs\n","TAO's Conversational AI Toolkit works off of spec files which make it easy to edit hyperparameters on the fly. We can proceed to downloading the spec files. The user may choose to modify/rewrite these specs, or even individually override them through the launcher. You can download the default spec files by using the `download_specs` command. <br>\n","\n","The -o argument indicating the folder where the default specification files will be downloaded, and -r that instructs the script where to save the logs. **Make sure the -o points to an empty folder!**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yOtU55fVGwP3"},"outputs":[],"source":["# delete the specs directory if it is already there to avoid errors\n","!tao speech_to_text_citrinet download_specs \\\n","    -r $RESULTS_DIR/speech_to_text_citrinet \\\n","    -o $SPECS_DIR/speech_to_text_citrinet"]},{"cell_type":"markdown","metadata":{"id":"2PVW2W3HGwP3"},"source":["### Download Data"]},{"cell_type":"markdown","metadata":{"id":"YEhppr2JGwP3"},"source":["For the purposes of demonstration we will use the popular AN4 dataset. Let's download it."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Le1awMWGwP4"},"outputs":[],"source":["! wget https://dldata-public.s3.us-east-2.amazonaws.com/an4_sphere.tar.gz  # for the original source, please visit http://www.speech.cs.cmu.edu/databases/an4/an4_sphere.tar.gz"]},{"cell_type":"markdown","metadata":{"id":"jo8juikcGwP4"},"source":["After downloading, untar the dataset, and move it to the correct directory."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sjaG8eguGwP4"},"outputs":[],"source":["! tar -xvf an4_sphere.tar.gz \n","! mv an4 $DATA_DIR"]},{"cell_type":"markdown","metadata":{"id":"MJ1FjbCWGwP4"},"source":["### Pre-Processing"]},{"cell_type":"markdown","metadata":{"id":"nLPaSCPjGwP4"},"source":["This step converts the mp3 files into wav files and splits the data into training and testing sets. It also generates a \"meta-data\" file to be consumed by the dataloader for training and testing."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B0bIKwYSGwP4"},"outputs":[],"source":["!tao speech_to_text_citrinet dataset_convert \\\n","    -e $SPECS_DIR/speech_to_text_citrinet/dataset_convert_an4.yaml \\\n","    -r $RESULTS_DIR/citrinet/dataset_convert \\\n","    source_data_dir=$DATA_DIR/an4 \\\n","    target_data_dir=$DATA_DIR/an4_converted"]},{"cell_type":"markdown","metadata":{"id":"ByTN4Nr9GwP4"},"source":["Let's take a listen to a sample audio file"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FccxLrDiGwP4"},"outputs":[],"source":["# change path of the file here\n","import os\n","import IPython.display as ipd\n","path = os.environ[\"DATA_DIR\"] + '/an4_converted/wavs/an268-mbmg-b.wav'\n","ipd.Audio(path)"]},{"cell_type":"markdown","metadata":{"id":"4tXla8IAGwP5"},"source":["Training commands for CitriNet is similar to those of /QuartzNet. Let's have a look!"]},{"cell_type":"markdown","metadata":{"id":"Gz8XJgLzGwP5"},"source":["### Training "]},{"cell_type":"markdown","metadata":{"id":"Al6vOG3rGwP5"},"source":["#### Create Tokenizer"]},{"cell_type":"markdown","metadata":{"id":"aS86VGstGwP5"},"source":["Before we can do the actual training, we need to pre-process the text. This step is called subword tokenization that creates a subword vocabulary for the text. This is different from Jasper/QuartzNet because only single characters are regarded as elements in the vocabulary in their cases, while in CitriNet the subword can be one or multiple characters. We can use the `create_tokenizer` command to create the tokenizer that can generate the subword vocabulary for us for use in training below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C_Dq2jtxGwP5"},"outputs":[],"source":["!tao speech_to_text_citrinet create_tokenizer \\\n","-e $SPECS_DIR/speech_to_text_citrinet/create_tokenizer.yaml \\\n","-r $RESULTS_DIR/citrinet/create_tokenizer \\\n","manifests=$DATA_DIR/an4_converted/train_manifest.json \\\n","output_root=$DATA_DIR/an4 \\\n","vocab_size=32"]},{"cell_type":"markdown","metadata":{"id":"-YDxB-FLGwP5"},"source":["We have a very neat interface which allows the end user to configure training parameters from the command line interface. <br>\n","\n","The process of opening the training script; finding the parameters of interest (which might be spread across multiple files), making the changes needed, and double checking everything is being replaced by a much more easy to use and visible command line interface.\n","\n","For instance if the number of epochs are needed to be modified along with a change in learning rate, the user can add `trainer.max_epochs=10` and `optim.lr=0.02` and train the model. Sample commands are given below.\n"]},{"cell_type":"markdown","metadata":{"id":"m93deTZyGwP5"},"source":["<b>A list of some of the customizable parameters along with their default values is as follows:</b>"]},{"cell_type":"markdown","metadata":{"id":"126lELSvGwP5"},"source":["trainer:<br>\n","<ul>  \n","  <li>gpus: 1 </li>\n","  <li>num_nodes: 1 </li>\n","  <li>max_epochs: 5 </li>\n","  <li>max_steps: null </li>\n","  <li>checkpoint_callback: false </li>\n","</ul>\n","\n","training_ds:\n","<ul>  \n","  <li>sample_rate: 16000 </li>\n","  <li>batch_size: 32 </li>\n","  <li>trim_silence: true </li>\n","  <li>max_duration: 16.7 </li>\n","  <li>shuffle: true </li>\n","  <li>is_tarred: false </li>\n","  <li>tarred_audio_filepaths: null </li>\n","</ul>  \n","\n","validation_ds:\n","<ul>  \n","  <li>sample_rate: 16000 </li>\n","  <li>batch_size: 32 </li>\n","  <li>shuffle: false </li>\n","</ul>  \n","optim:\n","<ul>\n","  <li>name: adam </li>\n","  <li>lr: 0.1 </li>\n","  <li>betas: [0.9, 0.999] </li>\n","  <li>weight_decay: 0.0001 </li>\n","</ul>\n","\n","The steps below might take considerable time depending on the GPU being used. For best experience, we recommend using an A100 GPU."]},{"cell_type":"markdown","metadata":{"id":"YUNBds4HGwP5"},"source":["For training an ASR CitriNet model in TAO, we use the `tao speech_to_text_citrinet train` command with the following args:\n","<ul>\n","    <li> <b>-e</b> : Path to the spec file </li>\n","    <li> <b>-g</b> : Number of GPUs to use </li>\n","    <li> <b>-r</b> : Path to the results folder </li>\n","    <li> <b>-m</b> : Path to the model </li>\n","    <li> <b>-k</b> : User specified encryption key to use while saving/loading the model </li>\n","    <li> Any overrides to the spec file eg. trainer.max_epochs </li>\n","</ul>"]},{"cell_type":"markdown","metadata":{"id":"rREpsN0lGwP6"},"source":["#### Training CitriNet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CbABm947GwP6"},"outputs":[],"source":["!tao speech_to_text_citrinet train \\\n","     -e $SPECS_DIR/speech_to_text_citrinet/train_citrinet_bpe.yaml \\\n","     -g 1 \\\n","     -k $KEY \\\n","     -r $RESULTS_DIR/citrinet/train \\\n","     training_ds.manifest_filepath=$DATA_DIR/an4_converted/train_manifest.json \\\n","     validation_ds.manifest_filepath=$DATA_DIR/an4_converted/test_manifest.json \\\n","     trainer.max_epochs=1 \\\n","     training_ds.num_workers=4 \\\n","     validation_ds.num_workers=4 \\\n","     model.tokenizer.dir=$DATA_DIR/an4/tokenizer_spe_unigram_v32"]},{"cell_type":"markdown","metadata":{"id":"dj35EI1XGwP6"},"source":["### ASR evaluation"]},{"cell_type":"markdown","metadata":{"id":"r1EJUlWYGwP6"},"source":["Now that we have a model trained, we need to check how well it performs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hfqJaE7JGwP6"},"outputs":[],"source":["!tao speech_to_text_citrinet evaluate \\\n","     -e $SPECS_DIR/speech_to_text_citrinet/evaluate.yaml \\\n","     -g 1 \\\n","     -k $KEY \\\n","     -m $RESULTS_DIR/citrinet/train/checkpoints/trained-model.tlt \\\n","     -r $RESULTS_DIR/citrinet/evaluate \\\n","     test_ds.manifest_filepath=$DATA_DIR/an4_converted/test_manifest.json"]},{"cell_type":"markdown","metadata":{"id":"N6ScJhhZGwP6"},"source":["### ASR finetuning"]},{"cell_type":"markdown","metadata":{"id":"c8iquD9qGwP6"},"source":["Once the model is trained and evaluated and there is a need for fine tuning, the following command can be used to fine tune the ASR model. This step can also be used for transfer learning by making changes in the `train.json` and `dev.json` files to add new data.\n","\n","The list for customizations is same as the training parameters with the exception for parameters which affect the model architecture. Also, instead of `training_ds` we have `finetuning_ds`\n","\n","Note: If you wish to proceed with a trained dataset for better inference results, you can find a .nemo model [here](\n","https://ngc.nvidia.com/catalog/collections/nvidia:nemotrainingframework).\n","\n","Simply re-name the .nemo file to .tlt and pass it through the finetune pipeline.\n","\n","**Note: The finetune spec files contain specifics to finetune the English model we just trained to Russian. If you wish to proceed with English, please make the changes in the spec file *finetune.yaml* which you can find in the SPEC_DIR folder you mapped. Be sure to delete older finetuning checkpoints if you choose to change the language after finetuning it as is.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6W6z7b3AGwP6"},"outputs":[],"source":["!tao speech_to_text_citrinet finetune \\\n","     -e $SPECS_DIR/speech_to_text_citrinet/finetune.yaml \\\n","     -g 1 \\\n","     -k $KEY \\\n","     -m $RESULTS_DIR/citrinet/train/checkpoints/trained-model.tlt \\\n","     -r $RESULTS_DIR/citrinet/finetune \\\n","     finetuning_ds.manifest_filepath=$DATA_DIR/an4_converted/train_manifest.json \\\n","     validation_ds.manifest_filepath=$DATA_DIR/an4_converted/test_manifest.json \\\n","     trainer.max_epochs=1 \\\n","     finetuning_ds.num_workers=20 \\\n","     validation_ds.num_workers=20 \\\n","     trainer.gpus=1 \\\n","     tokenizer.dir=$DATA_DIR/an4/tokenizer_spe_unigram_v32"]},{"cell_type":"markdown","metadata":{"id":"qsM_GrvrGwP7"},"source":["## What's Next?"]},{"cell_type":"markdown","metadata":{"id":"Qa7UEvMSGwP7"},"source":[" You could use TAO to build custom models for your own applications, or you could deploy the custom model to Nvidia Riva!"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"speech-to-text-training.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.10.2 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.2"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":0}
