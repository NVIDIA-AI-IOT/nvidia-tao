{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Adapt Optimize (TAO) Toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Adapt Optimize (TAO) Toolkit  is a python based AI toolkit for taking purpose-built pre-trained AI models and customizing them with your own data.\n",
    "\n",
    "Transfer learning extracts learned features from an existing neural network to a new one. Transfer learning is often used when creating a large training dataset is not feasible.\n",
    "\n",
    "Developers, researchers and software partners building intelligent AI apps and services, can bring their own data to fine-tune pre-trained models instead of going through the hassle of training from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Train Adapt Optimize (TAO) Toolkit](https://developer.nvidia.com/sites/default/files/akamai/TAO/tlt-tao-toolkit-bring-your-own-model-diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this toolkit is to reduce that 80 hour workload to an 8 hour workload, which can enable data scientist to have considerably more train-test iterations in the same time frame.\n",
    "\n",
    "Let's see this in action with a use case for Speech Synthesis!\n",
    "\n",
    "#### Note\n",
    "1. This notebook uses Librispeech dataset by default, which should be around ~3.75 GB.\n",
    "1. Using the default config/spec file provided in this notebook, each weight file size of spectrogen created during training will be ~1.76 GB and, each weight file size of vocoder created during training will be around ~324 MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text to Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text to Speech (TTS) is often the last step in building a Conversational AI model. A TTS model converts text into audible speech. The main objective is to synthesize reasonable and natural speech for given text. Since there are no universal standard to measure quality of synthesized speech, you will need to listen to some inferred speech to tell whether a TTS model is well trained.\n",
    "\n",
    "In TAO Toolkit, TTS is made up with two models: [FastPitch](https://arxiv.org/pdf/2006.06873.pdf) for spectrogram generation and [HiFiGAN](https://arxiv.org/pdf/2010.05646.pdf) as vocoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBdjK77dKOQn"
   },
   "source": [
    "## Connect to a GPU Runtime\n",
    "\n",
    "1.   Change Runtime type to GPU by Runtime(Top Left tab)->Change Runtime Type->GPU(Hardware Accelerator)\n",
    "2.   Then click on Connect (Top Right)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38R-3LAixO-h"
   },
   "source": [
    "## Mounting Google drive\n",
    "Mount your Google drive storage to this Colab instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dt69GSQgo5f3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    %env GOOGLE_COLAB=1\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "except:\n",
    "    %env GOOGLE_COLAB=0\n",
    "    print(\"Warning: Not a Colab Environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMrKJGSaxnmE"
   },
   "source": [
    "## Setup Python Environment\n",
    "Setup the environment necessary to run the TAO Networks by running the bash script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FIXME\n",
    "1. COLAB_NOTEBOOKS_PATH - for Google Colab environment, set this path where you want to clone the repo to; for local system environment, set this path to the already cloned repo\n",
    "1. NUM_GPUS - set this to <= number of GPU's availble on the instance\n",
    "1. DATA_DIR - set this path to a folder location where you want to dataset to be present\n",
    "1. SPECS_DIR - set this path to a folder location where the configuration/spec files will be saved\n",
    "1. RESULTS_DIR - set this path to a folder location where pretrained models, checkpoints and log files during different model actions will be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GsF_BUsxt0Lx",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#FIXME1\n",
    "%env COLAB_NOTEBOOKS_PATH=/content/drive/MyDrive/nvidia-tao\n",
    "if os.environ[\"GOOGLE_COLAB\"] == \"1\":\n",
    "    os.environ[\"bash_script\"] = \"setup_env.sh\"\n",
    "    if not os.path.exists(os.path.join(os.environ[\"COLAB_NOTEBOOKS_PATH\"])):\n",
    "        !git clone https://github.com/NVIDIA-AI-IOT/nvidia-tao.git $COLAB_NOTEBOOKS_PATH\n",
    "else:\n",
    "    os.environ[\"bash_script\"] = \"setup_env_desktop.sh\"\n",
    "    if not os.path.exists(os.environ[\"COLAB_NOTEBOOKS_PATH\"]):\n",
    "        raise Exception(\"Error, enter the path of the colab notebooks repo correctly\")\n",
    "\n",
    "!sed -i \"s|PATH_TO_COLAB_NOTEBOOKS|$COLAB_NOTEBOOKS_PATH|g\" $COLAB_NOTEBOOKS_PATH/pytorch/$bash_script\n",
    "!sh $COLAB_NOTEBOOKS_PATH/pytorch/$bash_script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Let's Dig in: TTS using TAO\n",
    "\n",
    "This notebook assumes that you are already familiar with TTS Training using TAO, as described in the [text-to-speech-training](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/resources/texttospeech_notebook) notebook, and that you have a pretrained TTS model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Relevant Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%env TAO_DOCKER_DISABLE=1\n",
    "\n",
    "#FIXME2\n",
    "%env NUM_GPUS=1\n",
    "\n",
    "#FIXME3\n",
    "%env DATA_DIR=/data/tts\n",
    "!sudo mkdir -p $DATA_DIR && sudo chmod -R 777 $DATA_DIR\n",
    "\n",
    "#FIXME4\n",
    "%env SPECS_DIR=/specs/tts\n",
    "!sudo mkdir -p $SPECS_DIR && sudo chmod -R 777 $SPECS_DIR\n",
    "\n",
    "#FIXME5\n",
    "%env RESULTS_DIR=/results/tts\n",
    "!sudo mkdir -p $RESULTS_DIR && sudo chmod -R 777 $RESULTS_DIR\n",
    "\n",
    "\n",
    "# Set your encryption key, and use the same key for all commands\n",
    "%env KEY=tlt_encode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that everything is setup, we would like to take a bit of time to explain the tao interface for ease of use. The command structure can be broken down as follows: `tao <task name> <subcommand>` <br> \n",
    "\n",
    "Let's see this in further detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Downloading Specs\n",
    "TAO's Conversational AI Toolkit works off of spec files which make it easy to edit hyperparameters on the fly. We can proceed to downloading the spec files. The user may choose to modify/rewrite these specs, or even individually override them through the launcher. You can download the default spec files by using the `download_specs` command. <br>\n",
    "\n",
    "The -o argument indicating the folder where the default specification files will be downloaded, and -r that instructs the script where to save the logs. **Make sure the -o points to an empty folder!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# download spec files for FastPitch\n",
    "! tao spectro_gen download_specs \\\n",
    "    -r $RESULTS_DIR/spectro_gen \\\n",
    "    -o $SPECS_DIR/spectro_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# download spec files for HiFiGAN\n",
    "! tao vocoder download_specs \\\n",
    "    -r $RESULTS_DIR/vocoder \\\n",
    "    -o $SPECS_DIR/vocoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "For the rest of this notebook, it is assumed that you have:\n",
    "\n",
    " - Pretrained FastPitch and HiFiGAN models that were trained on LJSpeech sampled at 22kHz\n",
    " \n",
    "In the case that you are not using a TTS model trained on LJSpeech at the correct sampling rate. Please ensure that you have the original data, including wav files and a .json manifest file. If you have a TTS model but not at 22kHz, please ensure that you set the correct sampling rate, and fft parameters.\n",
    "\n",
    "For the rest of the notebook, we will be using a toy dataset consisting of 5 mins of audio. This dataset is for demo purposes only. For a good quality model, we recommend at least 30 minutes of audio. We recommend using the [NVIDIA Custom Voice Recorder](https://developer.nvidia.com/riva-voice-recorder-early-access) tool, to generate a good dataset for finetuning.\n",
    "\n",
    "Let's first download the original LJSpeech dataset and set variables that point to this as the original data's `.json` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! wget -O $DATA_DIR/ljspeech.tar.bz2 https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After downloading, untar the dataset, and move it to the correct directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extracting and moving the data to the correct directories.\n",
    "! tar -xvf $DATA_DIR/ljspeech.tar.bz2\n",
    "! sudo rm -rf $DATA_DIR/ljspeech\n",
    "! mv LJSpeech-1.1 $DATA_DIR/ljspeech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step downloads audio to text file lists from NVIDIA for LJSpeech and generates the manifest files. If you use your own dataset, you have to generate three files: `ljs_audio_text_train_filelist.txt`, `ljs_audio_text_val_filelist.txt`, `ljs_audio_text_test_filelist.txt` yourself. Those files correspond to your train / val / test split. For each text file, the number of rows should be equal to number of samples in this split and each row should be like:\n",
    "\n",
    "```\n",
    "DUMMY/<file_name>.wav|<text_of_the_audio>\n",
    "```\n",
    "\n",
    "An example row is:\n",
    "\n",
    "```\n",
    "DUMMY/LJ045-0096.wav|Mrs. De Mohrenschildt thought that Oswald,\n",
    "```\n",
    "\n",
    "After having those three files in your `data_dir`, you can run following command as you would do for LJSpeech dataset.\n",
    "\n",
    "Be patient! This step can take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! tao spectro_gen dataset_convert \\\n",
    "      -e $SPECS_DIR/spectro_gen/dataset_convert_ljs.yaml \\\n",
    "      -r $RESULTS_DIR/spectro_gen/dataset_convert \\\n",
    "      data_dir=$DATA_DIR/ljspeech \\\n",
    "      dataset_name=ljspeech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "original_data_json = os.path.join(os.environ[\"DATA_DIR\"], \"ljspeech/ljspeech_train.json\")\n",
    "os.environ[\"original_data_json\"] = original_data_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now download the data from the NVIDIA Custom Voice Recorder tool, and place the data in the `$DATA_DIR`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Name of the untarred dataset from the NVIDIA Custom Voice Recorder.\n",
    "finetune_data_name = FIXME\n",
    "finetune_data_path = os.path.join(os.environ[\"DATA_DIR\"], finetune_data_name)\n",
    "\n",
    "os.environ[\"finetune_data_name\"] = finetune_data_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have downloaded the data, let's make sure that the audio clips and sample at the same sampling frequency as the clips used to train the pretrained model. For the course of this notebook, NVIDIA recommends using a model trained on the LJSpeech dataset. The sampling rate for this model is 22.05kHz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pip install soundfile\n",
    "\n",
    "import soundfile\n",
    "import librosa\n",
    "import json\n",
    "import os\n",
    "\n",
    "def resample_audio(input_file_path, output_path, target_sampling_rate=22050):\n",
    "    \"\"\"Resample a single audio file.\n",
    "    \n",
    "    Args:\n",
    "        input_file_path (str): Path to the input audio file.\n",
    "        output_path (str): Path to the output audio file.\n",
    "        target_sampling_rate (int): Sampling rate for output audio file.\n",
    "        \n",
    "    Returns:\n",
    "        No explicit returns\n",
    "    \"\"\"\n",
    "    if not input_file_path.endswith(\".wav\"):\n",
    "        raise NotImplementedError(\"Loading only implemented for wav files.\")\n",
    "    if not os.path.exists(input_file_path):\n",
    "        raise FileNotFoundError(f\"Cannot file input file at {input_file_path}\")\n",
    "    audio, sampling_rate = librosa.load(\n",
    "      input_file_path,\n",
    "      sr=target_sampling_rate\n",
    "    )\n",
    "    # Filtering out empty audio files.\n",
    "    if librosa.get_duration(y=audio, sr=sampling_rate) == 0:\n",
    "        print(f\"0 duration audio file encountered at {input_file_path}\")\n",
    "        return None\n",
    "    filename = os.path.basename(input_file_path)\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    soundfile.write(\n",
    "        os.path.join(output_path, filename),\n",
    "        audio,\n",
    "        samplerate=target_sampling_rate,\n",
    "        format=\"wav\"\n",
    "    )\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pip install tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "relative_path = f\"{finetune_data_name}/clips_resampled\"\n",
    "resampled_manifest_file = os.path.join(\n",
    "    os.environ[\"DATA_DIR\"],\n",
    "    f\"{finetune_data_name}/manifest_resampled.json\"\n",
    ")\n",
    "input_manifest_file = os.path.join(\n",
    "    os.environ[\"DATA_DIR\"],\n",
    "    f\"{finetune_data_name}/manifest.json\"\n",
    ")\n",
    "sampling_rate = 22050\n",
    "output_path = os.path.join(os.environ[\"DATA_DIR\"], relative_path)\n",
    "\n",
    "# Resampling the audio clip.\n",
    "with open(input_manifest_file, \"r\") as finetune_file:\n",
    "    with open(resampled_manifest_file, \"w\") as resampled_file:\n",
    "        for line in tqdm(finetune_file.readlines()):\n",
    "            data = json.loads(line)\n",
    "            filename = resample_audio(\n",
    "                os.path.join(\n",
    "                    os.environ[\"DATA_DIR\"],\n",
    "                    finetune_data_name,\n",
    "                    data[\"audio_filepath\"]\n",
    "                ),\n",
    "                output_path,\n",
    "                target_sampling_rate=sampling_rate\n",
    "            )\n",
    "            if not filename:\n",
    "                print(\"Skipping clip {} from training dataset\")\n",
    "                continue\n",
    "            data[\"audio_filepath\"] = os.path.join(\n",
    "                os.environ[\"DATA_DIR\"],\n",
    "                relative_path, filename\n",
    "            )\n",
    "            resampled_file.write(f\"{json.dumps(data)}\\n\")\n",
    "\n",
    "assert resampled_file.closed, \"Output file wasn't closed properly\"\n",
    "assert finetune_file.closed, \"Input file wasn't closed properly\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Splitting the dataset to train and val set.\n",
    "! cat $finetune_data_path/manifest_resampled.json | tail -n 2 > $finetune_data_path/manifest_val.json\n",
    "! cat $finetune_data_path/manifest_resampled.json | head -n -2 > $finetune_data_path/manifest_train.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "finetune_data_json = os.path.join(os.environ[\"DATA_DIR\"], f'{finetune_data_name}/manifest_train.json')\n",
    "os.environ[\"finetune_data_json\"] = finetune_data_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to create a json that contains data from both the original data and the finetuning data. We can do this using dataset_convert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! tao spectro_gen dataset_convert \\\n",
    "      dataset_name=merge \\\n",
    "      original_json=$original_data_json \\\n",
    "      finetune_json=$finetune_data_json \\\n",
    "      save_path=$DATA_DIR/$finetune_data_name/merged_train.json \\\n",
    "      -r $DATA_DIR/dataset_convert/merge \\\n",
    "      -e $SPECS_DIR/spectro_gen/dataset_convert_ljs.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "finetune_val_json = os.path.join(\n",
    "    os.getenv(\"DATA_DIR\"), f'{finetune_data_name}/manifest_val.json'\n",
    ")\n",
    "finetune_val_dataset = os.path.join(\n",
    "    os.getenv(\"DATA_DIR\"), f'{finetune_data_name}/merged_val.json'\n",
    ")\n",
    "os.environ[\"finetune_val_dataset\"] = finetune_val_dataset\n",
    "\n",
    "with open(finetune_val_json, \"r\") as val_json:\n",
    "    with open(finetune_val_dataset, \"w\") as out_file:\n",
    "        for line in val_json.readlines():\n",
    "            data = json.loads(line)\n",
    "            data[\"speaker\"] = 1\n",
    "            out_file.write(f\"{json.dumps(data)}\\n\")\n",
    "\n",
    "# You may uncomment this line to view the file after the modification.\n",
    "# ! cat $finetune_val_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Pitch Statistics\n",
    "\n",
    "Training Fastpitch requires you to set 4 values for pitch extraction:\n",
    "  - `fmin`: The minimum frequency value in Hz used to estimate the fundamental frequency (f0)\n",
    "  - `fmax`: The maximum frequency value in Hz used to estimate the fundamental frequency (f0)\n",
    "  - `avg`: The average used to normalize the pitch\n",
    "  - `std`: The std deviation used to normalize the pitch\n",
    "\n",
    "In order to get these, we first find a good `fmin` and `fmax` which are hyperparameters to librosa's pyin function.\n",
    "After we set those, we can iterate over the finetuning dataset to extract the pitch mean and standard deviation.\n",
    "\n",
    "#### Obtain fmin and fmax\n",
    "\n",
    "To get fmin and fmax, we start with some defaults, and iterate through random samples of the dataset to ensure that pyin is correctly extracting the pitch.\n",
    "\n",
    "We look at the plotted spectrogram as well as the predicted fundamental frequency, f0. We want the predicted f0 (the cyan line) to match the lowest energy band in the spectrogram. Here is an example of a good match between the predicted f0 and the spectrogram:\n",
    "\n",
    "![good_pitch.png](https://github.com/vpraveen-nv/model_card_images/raw/main/conv_ai/samples/texttospeech/good_pitch.png)\n",
    "\n",
    "Here is an example of a bad match between the f0 and the spectrogram. The fmin was likely set too high. The f0 algorithm is missing the first two vocalizations, and is correctly matching the last half of speech. To fix this, the fmin should be set lower.\n",
    "\n",
    "![bad_pitch.png](https://github.com/vpraveen-nv/model_card_images/raw/main/conv_ai/samples/texttospeech/bad_pitch.png)\n",
    "\n",
    "Here is an example of samples that have low frequency noise. In order to eliminate the effects of noise, you have to set fmin above the noise frequency. Unfortunately, this will result in degraded TTS quality. It would be best to re-record the data in a environment with less noise.\n",
    "\n",
    "![noise.png](https://github.com/vpraveen-nv/model_card_images/raw/main/conv_ai/samples/texttospeech/noise.png)\n",
    "\n",
    "\n",
    "*Note: You will have to run the below cell multiple times with different hyperparameters before you are able to find a good value for fmin and fmax.*\n",
    "\n",
    "*We set the `num_files` parameter to 10, so as to visualize only 10 plots in the dataset. You may choose to increase or decrease this value to generate more or fewer plots*\n",
    "\n",
    "*Note: As a starting point, we have set `fmin` to `80Hz` and `fmax` to `2094` Hz.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip3 install matplotlib==3.3.3\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "from math import ceil\n",
    "from IPython.display import Image\n",
    "\n",
    "valid_image_ext = ['.jpg', '.png', '.jpeg', '.ppm']\n",
    "\n",
    "pitch_fmin = 65    # in Hz\n",
    "pitch_fmax = 2094    # in Hz\n",
    "\n",
    "os.environ[\"pitch_fmin\"] = str(pitch_fmin)\n",
    "os.environ[\"pitch_fmax\"] = str(pitch_fmax)\n",
    "\n",
    "def visualize_images(image_dir, num_cols=2, num_images=10):\n",
    "    \"\"\"Visualize images in the notebook.\n",
    "    \n",
    "    Args:\n",
    "        image_dir (str): Path to the directory containing images.\n",
    "        num_cols (int): Number of columns.\n",
    "        num_images (int): Number of images.\n",
    "\n",
    "    \"\"\"\n",
    "    output_path = os.path.join(os.environ['RESULTS_DIR'], image_dir)\n",
    "    num_rows = int(ceil(float(num_images) / float(num_cols)))\n",
    "    f, axarr = plt.subplots(num_rows, num_cols, figsize=[240,90])\n",
    "    f.tight_layout()\n",
    "    a = [os.path.join(output_path, image) for image in os.listdir(output_path) \n",
    "         if os.path.splitext(image)[1].lower() in valid_image_ext]\n",
    "    for idx, img_path in enumerate(a[:num_images]):\n",
    "        col_id = idx % num_cols\n",
    "        row_id = idx // num_cols\n",
    "        img = plt.imread(img_path)\n",
    "        axarr[row_id, col_id].imshow(img)\n",
    "        \n",
    "\n",
    "# Computing f0 with a default fmin=64 and fmax=512\n",
    "!tao spectro_gen pitch_stats num_files=10 \\\n",
    "     pitch_fmin=$pitch_fmin \\\n",
    "     pitch_fmax=$pitch_fmax \\\n",
    "     output_path=results/spectro_gen/pitch_stats \\\n",
    "     compute_stats=false \\\n",
    "     render_plots=true \\\n",
    "     manifest_filepath=$DATA_DIR/$finetune_data_name/manifest_train.json \\\n",
    "     --results_dir $RESULTS_DIR/spectro_gen/pitch_stats\n",
    "\n",
    "visualize_images(\"spectro_gen/pitch_stats\", num_cols=5, num_images=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have chosen a good value for your `pitch_fmin` and `pitch_fmax`, the cell below will compute the pitch statistics (`pitch_mean` and `pitch_std`) to be used to finetune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! tao spectro_gen pitch_stats num_files=10 \\\n",
    "      pitch_fmin=$pitch_fmin \\\n",
    "      pitch_fmax=$pitch_fmax \\\n",
    "      output_path=results/spectro_gen/pitch_stats \\\n",
    "      compute_stats=true \\\n",
    "      render_plots=false \\\n",
    "      manifest_filepath=$DATA_DIR/$finetune_data_name/manifest_train.json \\\n",
    "      --results_dir $RESULTS_DIR/spectro_gen/pitch_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the `pitch_fmean` and `pitch_fmax` based on the results from the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Please set the fmin, fmax, pitch_mean and pitch_std values based on\n",
    "# the output from the tao spectro_gen pitch_stats task.\n",
    "pitch_mean = FIXME\n",
    "pitch_std = FIXME\n",
    "\n",
    "os.environ[\"pitch_mean\"] = str(pitch_mean)\n",
    "os.environ[\"pitch_std\"] = str(pitch_std)\n",
    "\n",
    "print(f\"pitch fmin: {pitch_fmin}\")\n",
    "print(f\"pitch fmax: {pitch_fmax}\")\n",
    "print(f\"pitch mean: {pitch_mean}\")\n",
    "print(f\"pitch std: {pitch_std}\")\n",
    "\n",
    "assert pitch_fmin < pitch_fmax , f\"pitch_fmin [{pitch_fmin}] > pitch_fmax [{pitch_fmax}]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning\n",
    "\n",
    "For finetuning TTS models in TAO, we use the `tao spectro_gen finetune` and `tao vocoder finetune` command with the following args:\n",
    "<ul>\n",
    "    <li> <b>-m</b> : Path to the model weights we want to finetune from </li>\n",
    "    <li> <b>-e</b> : Path to the spec file </li>\n",
    "    <li> <b>-g</b> : Number of GPUs to use </li>\n",
    "    <li> <b>-r</b> : Path to the results folder </li>\n",
    "    <li> <b>-k</b> : User specified encryption key to use while saving/loading the model </li>\n",
    "    <li> Any overrides to the spec file </li>\n",
    "</ul>\n",
    "\n",
    "In order to get a finetuned TTS pipeline, you need to finetune FastPitch. For best results, you need to finetune HiFiGAN as well.\n",
    "\n",
    "Please update the `-m` parameter to the path of your pre-trained checkpoint. This can be a previously trained `.tlt` or `.nemo` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Downloading the pretrained model\n",
    "\n",
    "NVIDIA recommends using these [FastPitch](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/tts_en_fastpitch) and [HiFiGAN](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/tts_hifigan) checkpoints on [NGC](https://ngc.nvidia.com)\n",
    "\n",
    "Cells below execute commands to install the NGC CLI on your local environment, and used said CLI to download the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Installing NGC CLI on the local machine.\n",
    "## Download and install\n",
    "%env LOCAL_PROJECT_DIR=/ngc_content/\n",
    "%env CLI=ngccli_cat_linux.zip\n",
    "!sudo mkdir -p $LOCAL_PROJECT_DIR/ngccli && sudo chmod -R 777 $LOCAL_PROJECT_DIR\n",
    "\n",
    "# Remove any previously existing CLI installations\n",
    "!sudo rm -rf $LOCAL_PROJECT_DIR/ngccli/*\n",
    "!wget \"https://ngc.nvidia.com/downloads/$CLI\" -P $LOCAL_PROJECT_DIR/ngccli\n",
    "!unzip -u -q \"$LOCAL_PROJECT_DIR/ngccli/$CLI\" -d $LOCAL_PROJECT_DIR/ngccli/\n",
    "!rm $LOCAL_PROJECT_DIR/ngccli/*.zip \n",
    "os.environ[\"PATH\"]=\"{}/ngccli/ngc-cli:{}\".format(os.getenv(\"LOCAL_PROJECT_DIR\", \"\"), os.getenv(\"PATH\", \"\"))\n",
    "!cp /usr/lib/x86_64-linux-gnu/libstdc++.so.6 $LOCAL_PROJECT_DIR/ngccli/ngc-cli/libstdc++.so.6\n",
    "\n",
    "!ngc registry model download-version \"nvidia/nemo/tts_en_fastpitch:1.8.1\" --dest $DATA_DIR/\n",
    "!ngc registry model download-version \"nvidia/nemo/tts_hifigan:1.0.0rc1\" --dest $DATA_DIR/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pretrained_fastpitch_model = os.path.join(os.environ[\"DATA_DIR\"], \"tts_en_fastpitch_v1.8.1/tts_en_fastpitch_align.nemo\")\n",
    "os.environ[\"pretrained_fastpitch_model\"] = pretrained_fastpitch_model\n",
    "pretrained_hifigan_model = os.path.join(os.environ[\"DATA_DIR\"], \"tts_hifigan_v1.0.0rc1/tts_hifigan.nemo\")\n",
    "os.environ[\"pretrained_hifigan_model\"] = pretrained_hifigan_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finetuning FastPitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Prior is needed for FastPitch training. If empty folder is provided, prior will generate on-the-fly\n",
    "# Please be patient especially if you provided an empty prior folder.\n",
    "! mkdir -p $RESULTS_DIR/spectro_gen/finetune/prior_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Downloading auxillary files to train.\n",
    "!wget -O $DATA_DIR/cmudict-0.7b_nv22.01 https://github.com/NVIDIA/NeMo/raw/v1.9.0/scripts/tts_dataset_files/cmudict-0.7b_nv22.01\n",
    "!wget -O $DATA_DIR/heteronyms-030921 https://github.com/NVIDIA/NeMo/raw/v1.9.0/scripts/tts_dataset_files/heteronyms-030921\n",
    "!wget -O $DATA_DIR/lj_speech.tsv https://github.com/NVIDIA/NeMo/raw/v1.9.0//nemo_text_processing/text_normalization/en/data/whitelist/lj_speech.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!tao spectro_gen finetune \\\n",
    "     -e $SPECS_DIR/spectro_gen/finetune.yaml \\\n",
    "     -g $NUM_GPUS \\\n",
    "     -k tlt_encode \\\n",
    "     -r $RESULTS_DIR/spectro_gen/finetune \\\n",
    "     -m $pretrained_fastpitch_model \\\n",
    "     train_dataset=$DATA_DIR/$finetune_data_name/merged_train.json \\\n",
    "     validation_dataset=$DATA_DIR/$finetune_data_name/merged_val.json \\\n",
    "     prior_folder=$RESULTS_DIR/spectro_gen/finetune/prior_folder \\\n",
    "     trainer.max_epochs=2 \\\n",
    "     n_speakers=2 \\\n",
    "     pitch_fmin=$pitch_fmin \\\n",
    "     pitch_fmax=$pitch_fmax \\\n",
    "     pitch_avg=$pitch_mean \\\n",
    "     pitch_std=$pitch_std \\\n",
    "     trainer.precision=16 \\\n",
    "     phoneme_dict_path=$DATA_DIR/cmudict-0.7b_nv22.01 \\\n",
    "     heteronyms_path=$DATA_DIR/heteronyms-030921 \\\n",
    "     whitelist_path=$DATA_DIR/lj_speech.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finetuning HiFiGAN\n",
    "\n",
    "In order to get the best audio from HiFiGAN, we need to finetune it:\n",
    "  - on the new speaker\n",
    "  - using mel spectrograms from our finetuned FastPitch Model\n",
    "\n",
    "Let's first generate mels from our FastPitch model, and save it to a new .json manifest for use with HiFiGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!sudo mkdir -p /raid && sudo chmod -R 777 /raid\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "def infer_and_save_json(infer_json, save_json, subdir=\"train\"):\n",
    "    # Get records from the training manifest\n",
    "    manifest_path = os.path.join(os.environ[\"DATA_DIR\"], infer_json)\n",
    "    os.environ[\"tao_manifest_path\"] = os.path.join(os.environ[\"DATA_DIR\"], infer_json)\n",
    "    os.environ[\"subdir\"] = subdir\n",
    "    save_json = os.path.join(os.environ[\"DATA_DIR\"], save_json)\n",
    "    records = []\n",
    "    text = {\"input_batch\": []}\n",
    "    print(\"Appending mel spectrogram paths to the dataset.\")\n",
    "    with open(manifest_path, \"r\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            manifest_info = json.loads(line)\n",
    "            manifest_info[\"mel_filepath\"] = f\"{os.environ['RESULTS_DIR']}/spectro_gen/infer/spectro/{subdir}/{i}.npy\"\n",
    "            records.append(manifest_info)\n",
    "            text[\"input_batch\"].append(manifest_info[\"text\"])\n",
    "\n",
    "    !tao spectro_gen infer \\\n",
    "         -e $SPECS_DIR/spectro_gen/infer.yaml \\\n",
    "         -g 1 \\\n",
    "         -k $KEY \\\n",
    "         -m $RESULTS_DIR/spectro_gen/finetune/checkpoints/finetuned-model.tlt \\\n",
    "         -r $RESULTS_DIR/spectro_gen/infer \\\n",
    "         output_path=$RESULTS_DIR/spectro_gen/infer/spectro/$subdir \\\n",
    "         speaker=1 \\\n",
    "         mode=\"infer_hifigan_ft\" \\\n",
    "         input_json=$tao_manifest_path\n",
    "\n",
    "    # Save to a new json\n",
    "    with open(save_json, \"w\") as f:\n",
    "        for r in records:\n",
    "            f.write(json.dumps(r) + '\\n')\n",
    "\n",
    "# Infer for train\n",
    "infer_and_save_json(f\"{finetune_data_name}/manifest_train.json\", f\"{finetune_data_name}/hifigan_train_ft.json\")\n",
    "# Infer for dev\n",
    "infer_and_save_json(f\"{finetune_data_name}/manifest_val.json\", f\"{finetune_data_name}/hifigan_dev_ft.json\", \"dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's finetune hifigan.\n",
    "\n",
    "Please update the `-m` parameter to the path of your pre-trained checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!tao vocoder finetune \\\n",
    "     -e $SPECS_DIR/vocoder/finetune.yaml \\\n",
    "     -g $NUM_GPUS \\\n",
    "     -k $KEY \\\n",
    "     -r $RESULTS_DIR/vocoder/finetune \\\n",
    "     -m $pretrained_hifigan_model \\\n",
    "     train_dataset=$DATA_DIR/$finetune_data_name/hifigan_train_ft.json \\\n",
    "     validation_dataset=$DATA_DIR/$finetune_data_name/hifigan_dev_ft.json \\\n",
    "     trainer.max_epochs=2 \\\n",
    "     training_ds.dataloader_params.batch_size=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TTS Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As aforementioned, since there are no universal standard to measure quality of synthesized speech, you will need to listen to some inferred speech to tell whether a TTS model is well trained. Therefore, we do not provide `evaluate` functionality in TAO Toolkit for TTS but only provide `infer` functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate spectrogram\n",
    "\n",
    "The first step for inference is generating spectrogram. That's a numpy array (saved as `.npy` file) for a sentence which can be converted to voice by a vocoder. We use FastPitch we just trained to generate spectrogram\n",
    "\n",
    "Please update the infer.yaml configuration file in the `$DATA_DIR/specs` to add new sentences. The sample infer.yaml file, contains 3 sentence texts.\n",
    "\n",
    "```yaml\n",
    "\n",
    "input_batch:\n",
    "  - \"by the end of no such thing the audience , like beatrice , has a watchful affection for the monster .\"\n",
    "  - \"director rob marshall went out gunning to make a great one .\"\n",
    "  - \"uneasy mishmash of styles and genres .\"\n",
    "```\n",
    "\n",
    "You may add new sentences by adding new lines to the `input_batch` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!tao spectro_gen infer \\\n",
    "     -e $SPECS_DIR/spectro_gen/infer.yaml \\\n",
    "     -g 1 \\\n",
    "     -k $KEY \\\n",
    "     -m $RESULTS_DIR/spectro_gen/finetune/checkpoints/finetuned-model.tlt \\\n",
    "     -r $RESULTS_DIR/spectro_gen/infer_output \\\n",
    "     output_path=$RESULTS_DIR/spectro_gen/infer_output/spectro \\\n",
    "     speaker=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate sound file\n",
    "\n",
    "The second step for inference is generating wav sound file based on spectrogram you generated in last step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!tao vocoder infer \\\n",
    "     -e $SPECS_DIR/vocoder/infer.yaml \\\n",
    "     -g 1 \\\n",
    "     -k $KEY \\\n",
    "     -m $RESULTS_DIR/vocoder/finetune/checkpoints/finetuned-model.tlt \\\n",
    "     -r $RESULTS_DIR/vocoder/infer_output \\\n",
    "     input_path=$RESULTS_DIR/spectro_gen/infer_output/spectro \\\n",
    "     output_path=$RESULTS_DIR/vocoder/infer_output/wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import IPython.display as ipd\n",
    "# change path of the file here\n",
    "ipd.Audio(os.environ[\"RESULTS_DIR\"] + '/vocoder/infer_output/wav/0.wav')\n",
    "# ipd.Audio(os.environ[\"RESULTS_DIR\"] + '/vocoder/infer_output/wav/1.wav')\n",
    "# ipd.Audio(os.environ[\"RESULTS_DIR\"] + '/vocoder/infer_output/wav/2.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Debug\n",
    "\n",
    "The data provided is only meant to be a sample to understand how finetuning works in TAO. In order to generate better speech quality, we recommend recording at least 30 mins of audio, and increasing the number of finetuning steps from the current `trainer.max_steps=1000` to `trainer.max_steps=5000` for both models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's Next ?\n",
    "\n",
    " You could use TAO to build custom models for your own applications, and deploy them to Nvidia Riva!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "851240db530085c397391f2f949356c4eb3a8832b55aabc3de74eae9cba050e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
