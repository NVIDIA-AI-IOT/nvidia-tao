{"cells":[{"cell_type":"markdown","metadata":{"id":"gYVyGJFn9tSE"},"source":["# Question Answering using Train Adapt Optimize (TAO) Toolkit"]},{"cell_type":"markdown","metadata":{"id":"W-iQrQpZ9tSG"},"source":["*Transfer learning* is the process of transferring learned features from one application to another. It is a commonly used training technique where you use a model trained on one task and re-train to use it on a different task.\n","\n","**Train Adapt Optimize (TAO) Toolkit ** is a simple and easy-to-use Python based AI toolkit for taking purpose-built AI models and customizing them with users' own data.\n","\n","<center><img src=\"https://developer.nvidia.com/sites/default/files/akamai/embedded-transfer-learning-toolkit-software-stack-1200x670px.png\"><\\center>"]},{"cell_type":"markdown","metadata":{"id":"IkvBfHxF9tSH"},"source":["## Learning Objectives\n","In this notebook, you will learn how to leverage the simplicity and convenience of TAO to:\n","- Take a [BERT](https://arxiv.org/pdf/1810.04805.pdf) QA model and [**Train/Finetune**](#training) it on the [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) dataset\n","- Run [**Inference**](#inference)\n","- [**Export**](#export-onnx) the model for the [ONNX](https://onnx.ai/) format, or [export](#export-riva) in a format suitable for deployment in [Riva](https://developer.nvidia.com/riva).\n","\n","The earlier sections in the notebook give a brief introduction to the QA task, the SQuAD dataset and BERT. If you are already familiar with these, and want to jump right into the meat of the matter, you can start at section on [Data Preparation](#prepare-data)."]},{"cell_type":"markdown","metadata":{"id":"jBdjK77dKOQn"},"source":["## Connect to a GPU Runtime\n","\n","1.   Change Runtime type to GPU by Runtime(Top Left tab)->Change Runtime Type->GPU(Hardware Accelerator)\n","2.   Then click on Connect (Top Right)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"38R-3LAixO-h"},"source":["## Mounting Google drive\n","Mount your Google drive storage to this Colab instance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dt69GSQgo5f3"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"iMrKJGSaxnmE"},"source":["## Setup Python Environment\n","Setup the environment necessary to run the TAO Networks by running the bash script"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GsF_BUsxt0Lx"},"outputs":[],"source":["!sh /content/drive/MyDrive/ColabNotebooks/pytorch/setup_env.sh"]},{"cell_type":"markdown","metadata":{"id":"HhykDyMA9tSK"},"source":["---\n","## Question Answering (QA)\n","\n","### Task Description\n","The Question Answering task in NLP pertains to building a model which can answer questions posed in natural language. Many datasets (including [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/), the dataset we use in this notebook) pose this as a reading comprehension task i.e. given a question and a context, the goal is to predict the span within the context with a start and end position which indicates the answer to the question. For every word in the training dataset we predict:\n","- likelihood this word is the start of the span\n","- likelihood this word is the end of the span"]},{"cell_type":"markdown","metadata":{"id":"3aFrgnNK9tSK"},"source":["### The SQuAD Dataset\n","\n","[SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) is a large dataset for QA consisting of reading passages obtained from high-quality Wikipedia articles. With each passage, the dataset contains accompanying reading comprehension questions based on the content of the passage. For each question, there are one or more answers. These questions and corresponding answers were obtained through crowdsourcing.\n","\n","\n","The SQuAD format consists of a JSON file for each dataset split. Each title has one or multiple paragraph entries, each consisting of the text - \"context\", and question-answer entries. Each question-answer entry has:\n","\n","- a question\n","- a boolean flag \"is_impossible\" which shows if the question is answerable or not\n","- a globally unique id\n","- in case the question is answerable one answer entry, which contains the text span and its starting character index in the context. If not answerable, the \"answers\" list is empty"]},{"cell_type":"markdown","metadata":{"id":"AoQt2voF9tSL"},"source":["\n","\n","```\n","{\n","    \"data\": [\n","        {\n","            \"title\": \"Super_Bowl_50\", \n","            \"paragraphs\": [\n","                {\n","                    \"context\": \"Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24\\u201310 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \\\"golden anniversary\\\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \\\"Super Bowl L\\\"), so that the logo could prominently feature the Arabic numerals 50.\", \n","                    \"qas\": [\n","                        {\n","                            \"question\": \"Where did Super Bowl 50 take place?\", \n","                            \"is_impossible\": \"false\", \n","                            \"id\": \"56be4db0acb8001400a502ee\", \n","                            \"answers\": [\n","                                {\n","                                    \"answer_start\": \"403\", \n","                                    \"text\": \"Santa Clara, California\"\n","                                }\n","                            ]\n","                        },\n","                        {\n","                            \"question\": \"What was the winning score of the Super Bowl 50?\", \n","                            \"is_impossible\": \"true\", \n","                            \"id\": \"56be4db0acb8001400a502ez\", \n","                            \"answers\": [\n","                            ]\n","                        }\n","                    ]\n","                }\n","            ]\n","        }\n","    ]\n","}\n","...\n","```"]},{"cell_type":"markdown","metadata":{"id":"rdkHEMay9tSL"},"source":["The evaluation files (for validation and testing) follow the above format except for it can provide more than one answer to the same question. The inference file follows the above format except for it does not require the \"answers\" and \"is_impossible\" keywords."]},{"cell_type":"markdown","metadata":{"id":"zYUhdmQR9tSL"},"source":["### BERT Model for QA\n","In this notebook, we will show how to use a pre-trained [BERT](https://arxiv.org/pdf/1810.04805.pdf) (Bidirectional Encoder Representations from Transformers) model for QA leveraging TAO. The BERT model has made major breakthroughs in Natural Language Understanding in recent years. For most applications, the model is typically trained in two phases, pre-training and fine-tuning. \n","- The BERT core model can be pre-trained on large, generic datasets to generate dense vector representations of input sentence(s). \n","- It can be quickly fine-tuned to perform a wide variety of tasks such as question/answering, sentiment analysis, or named entity recognition.\n","\n","The figure below shows a high-level block diagram of pre-training and fine-tuning BERT for QA.\n","<center><img src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2020/05/bert-model-625x268.png\"></center>"]},{"cell_type":"markdown","metadata":{"id":"PsrGU5bv9tSM"},"source":["In alignment with the above, for pre-training we can take one of two approaches. We can either pre-train the BERT model with our own data, or use a model pre-trained by Nvidia. After we obtain a pre-trained model, the next step would be to fine-tune it for the QA task and run inference on the fine-tuned model."]},{"cell_type":"markdown","metadata":{"id":"h6t8p4H19tSM"},"source":["<center><img src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2020/06/Fig4revised-625x340.png\"></center>"]},{"cell_type":"markdown","metadata":{"id":"MRZpLEac9tSO"},"source":["---\n","### Set Relevant Paths\n","Set these paths according to your environment."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gMr3BKcb9tSO"},"outputs":[],"source":["# NOTE: The following paths are set from the perspective of the TAO Docker. \n","\n","# The data is saved here\n","%env DATA_DIR=/data/squad\n","\n","# The configuration files are stored here\n","%env SPECS_DIR=/specs/question_answering\n","\n","# The results are saved at this path\n","%env RESULTS_DIR=/results/question_answering\n","\n","%env CACHE_DIR=/.cache\n","\n","# Set your encryption key, and use the same key for all commands\n","%env KEY=tlt_encode"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XkRDqqG89tSO"},"outputs":[],"source":["# Make sure the source directories exist, if not, create them\n","! mkdir -p $DATA_DIR\n","! mkdir -p $SPECS_DIR\n","! mkdir -p $RESULTS_DIR\n","! mkdir -p $CACHE_DIR"]},{"cell_type":"markdown","metadata":{"id":"Abp2zEMV9tSM"},"source":["---\n","<a id='prepare-data'></a>\n","### Preparing the dataset\n","The SQuAD dataset is available [here](https://rajpurkar.github.io/SQuAD-explorer/). You will find that there are 2 versions of the Squad datasets: v1.1 and v2.0. \n","\n","- SQuAD 1.1, the older version of the SQuAD dataset, contains 100,000+ question-answer pairs on 500+ articles.\n","- SQuAD 2.0 dataset combines the 100,000 questions in SQuAD 1.1 with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones."]},{"cell_type":"markdown","metadata":{"id":"QOXVDGdi9tSM"},"source":["#### Downloading the dataset"]},{"cell_type":"markdown","metadata":{"id":"kfFaL6Fg9tSM"},"source":["For convenience, you may use the code below to download the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"geLyLaBo9tSN"},"outputs":[],"source":["import urllib.request\n","\n","# simple utility function to download the squad dataset\n","def download_squad(save_path):\n","    save_path = save_path + '/squad'\n","\n","    if not os.path.exists(save_path):\n","        os.makedirs(save_path)\n","\n","    if not os.path.exists(save_path + '/v1.1'):\n","        os.makedirs(save_path + '/v1.1')\n","\n","    if not os.path.exists(save_path + '/v2.0'):\n","        os.makedirs(save_path + '/v2.0')\n","        \n","    # urls for both SQuAD v1.1 and v2.0. You may modify the dict below if you choose to download only one of the two.\n","    download_urls = {\n","            'https://rajpurkar.github.io/SQuAD-explorer' '/dataset/train-v1.1.json': 'v1.1/train-v1.1.json',\n","            'https://rajpurkar.github.io/SQuAD-explorer' '/dataset/dev-v1.1.json': 'v1.1/dev-v1.1.json',\n","            'https://rajpurkar.github.io/SQuAD-explorer' '/dataset/train-v2.0.json': 'v2.0/train-v2.0.json',\n","            'https://rajpurkar.github.io/SQuAD-explorer' '/dataset/dev-v2.0.json': 'v2.0/dev-v2.0.json',\n","    }\n","         \n","    for item in download_urls:\n","        url = item\n","        file = download_urls[item]\n","        print('Downloading:', url)\n","        if os.path.isfile(save_path + '/' + file):\n","            print('** Download file already exists, skipping download **')\n","        else:\n","            response = urllib.request.urlopen(url)\n","            with open(save_path + '/' + file, \"wb\") as handle:\n","                handle.write(response.read())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KOCCcZZ_9tSN"},"outputs":[],"source":["# This will download both v1.1 and v2.0 versions of SQuAD dataset\n","download_squad($DATA_DIR)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SCMJWgj39tSN"},"outputs":[],"source":["# Verify that the data is present\n","!ls $DATA_DIR/squad/v1.1"]},{"cell_type":"markdown","metadata":{"id":"F75VpVBc9tSN"},"source":["---\n","## TAO workflow\n","The rest of the notebook shows what a sample TAO workflow looks like."]},{"cell_type":"markdown","metadata":{"id":"gFeZ3gJ69tSN"},"source":["The rest of the notebook exemplifies the simplicity of the TAO workflow. Users with basic knowledge of Deep Learning can get started building their own custom models using a simple specification file. It's essentially just one command each to run data preprocessing, training, fine-tuning, evaluation, inference, and export! All configurations happen through YAML spec files <br>"]},{"cell_type":"markdown","metadata":{"id":"xXvbhfHU9tSN"},"source":["---\n","### Configuration/Specification Files"]},{"cell_type":"markdown","metadata":{"id":"Gk62LdxU9tSN"},"source":["The essence of all commands in TAO lies in the YAML spec files. There are sample spec files already available for you to use directly or as reference to create your own.  Through these spec files, you can tune many knobs like the model, dataset, hyperparameters, optimizer etc. Each command (like train, finetune, evaluate etc.) should have a dedicated spec file with configurations pertinent to it. <br>\n","\n","Here is an example of the training spec file:\n","\n","---\n","```\n","trainer:\n","  max_epochs: 100\n","\n","model:\n","  tokenizer:\n","      tokenizer_name: ${model.language_model.pretrained_model_name} # or sentencepiece\n","      vocab_file: null # path to vocab file \n","      tokenizer_model: null # only used if tokenizer is sentencepiece\n","      special_tokens: null\n","\n","  language_model:\n","    pretrained_model_name: bert-base-uncased\n","    lm_checkpoint: null\n","    config_file: null # json file, precedence over config\n","    config: null\n","\n","  token_classifier:\n","    num_layers: 1\n","    dropout: 0.0\n","    num_classes: 2\n","    activation: relu\n","    log_softmax: false\n","    use_transformer_init: true\n","\n","\n","training_ds:\n","  file: ??? # e.g. squad/v1.1/train-v2.0.json\n","  batch_size: 12 # per GPU\n","\n","...\n","```\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"FsSYP6SC9tSO"},"source":["### Data Convert\n","For the QA task, the commands in TAO accepts data in the SQuAD JSON format (refer `Preparing the dataset` section above). If you have your data in any other format, be sure to convert it in the SQuAD format. Since we are using the SQuAD dataset for this notebook, we don't need to convert the data into any other format. We can proceed with training/fine-tuning directly."]},{"cell_type":"markdown","metadata":{"id":"abTTpNBQ9tSO"},"source":["---\n","### Downloading Specs\n","We can proceed to downloading the spec files. The user may choose to modify/rewrite these specs, or even individually override them through the launcher. You can download the default spec files by using the `download_specs` command. <br>\n","\n","The -o argument indicating the folder where the default specification files will be downloaded, and -r that instructs the script where to save the logs. **Make sure the -o points to an empty folder!**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fIkBIypB9tSO"},"outputs":[],"source":["!tao question_answering download_specs \\\n","    -r $RESULTS_DIR \\\n","    -o $SPECS_DIR"]},{"cell_type":"markdown","metadata":{"id":"twX2x8Ir9tSO"},"source":["---\n","<a id='training'></a>\n","### Training\n"]},{"cell_type":"markdown","metadata":{"id":"vSMy8f6Q9tSP"},"source":["Training a model using TAO is as simple as configuring your spec file and running the train command. The code cell below uses the default train.yaml available for users as reference. It is configured by default to use the `bert-base-uncased` pretrained model. Additionally, these configurations could easily be overridden using the tao-launcher CLI as shown below. For instance, below we override the `training_ds.file`, `validation_ds.file`, `trainer.max_epochs`, `training_ds.num_workers` and `validation_ds.num_workers` configurations to suit our needs. We encourage you to take a look at the .yaml spec files we provide! <br>\n","\n","For training a QA model in TAO, we use the `tao question_answering train` command with the following args:\n","- `-e`: Path to the spec file\n","- `-g`: Number of GPUs to use\n","- `-k`: User specified encryption key to use while saving/loading the model\n","- `-r`: Path to a folder where the outputs should be written. Make sure this is mapped in tlt_mounts.json\n","- Any overrides to the spec file eg. trainer.max_epochs <br>\n","\n","More details about these arguments are present in the [TAO Getting Started Guide](https://docs.nvidia.com/tao/tao-toolkit/index.html) <br>\n","`NOTE:` All file paths corresponds to the destination mounted directory that is visible in the TAO docker container used in backend.<br>\n","\n","Also worth noting is that the first time you run training on the dataset, it will run pre-processing and save that processed data in the same directory as the dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"25i1RD5q9tSP"},"outputs":[],"source":["# Since this is a demonstration, we just train for 1 epoch below. You may need to train for more depending on your dataset.\n","!tao question_answering train \\\n","                        -e $SPECS_DIR/train.yaml \\\n","                        -g 1  \\\n","                        -k $KEY \\\n","                        -r $RESULTS_DIR/train \\\n","                        model.language_model.pretrained_model_name=bert-base-uncased \\\n","                        training_ds.file=$DATA_DIR/v1.1/train-v1.1.json \\\n","                        validation_ds.file=$DATA_DIR/v1.1/dev-v1.1.json \\\n","                        trainer.max_epochs=1 \\\n","                        trainer.precision=32 \\\n","                        training_ds.num_workers=4 \\\n","                        validation_ds.num_workers=4 \\\n","                        training_ds.batch_size=4 \\\n","                        validation_ds.num_samples=4"]},{"cell_type":"markdown","metadata":{"id":"bMrPAhKK9tSP"},"source":["The train command produces a .tlt file called `trained-model.tlt` saved at `$RESULTS_DIR/train/checkpoints/trained-model.tlt`. This file can be fed directly into the fine-tuning stage as we see in the next block."]},{"cell_type":"markdown","metadata":{"id":"qqsi6xFH9tSP"},"source":["#### Other tips and tricks:\n","- To accelerate the training without loss of quality, it is possible to train with these parameters:  `trainer.amp_level=\"O1\"` and `trainer.precision=16` for reduced precision.\n","- The batch size (`training_ds.batch_size`) may influence the validation accuracy. Larger batch sizes are faster to train with, however, you may get slightly better results with smaller batches.\n","- You can use the parameter: `trainer.val_check_interval` to define how many times per epoch to see validation accuracy metric calculated and printed. For instance, using `trainer.val_check_interval=0.25` will show the metric 4 times per epoch."]},{"cell_type":"markdown","metadata":{"id":"qmbu5LK49tSP"},"source":["---\n","### Fine-Tuning\n","Like many other NLP tasks, since we begin with a pretrained BERT model the step shown above for (re)training with your custom data should do the trick. However, TAO does provide a command for fine-tuning if your use-case demands that. Instead of `tao question_answering train`, we use `tao question_answering finetune` instead. We also specify the spec file corresponding to fine-tuning. All commands in TAO follow a similar pattern, streamlining the workflow even further!\n","\n","Note: If you wish to proceed with a trained dataset for better inference results, you can find a .nemo model [here](\n","https://ngc.nvidia.com/catalog/collections/nvidia:nemotrainingframework).\n","\n","Simply re-name the .nemo file to .tlt and pass it through the finetune pipeline."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bd3kRTR79tSP"},"outputs":[],"source":["!tao question_answering finetune \\\n","                        -e $SPECS_DIR/finetune.yaml \\\n","                        -g 1 \\\n","                        -m $RESULTS_DIR/train/checkpoints/trained-model.tlt \\\n","                        -k $KEY \\\n","                        -r $RESULTS_DIR/finetune \\\n","                        finetuning_ds.file=$DATA_DIR/v2.0/train-v2.0.json \\\n","                        validation_ds.file=$DATA_DIR/v2.0/dev-v2.0.json \\\n","                        trainer.max_epochs=1 \\\n","                        finetuning_ds.num_workers=4 \\\n","                        validation_ds.num_workers=4"]},{"cell_type":"markdown","metadata":{"id":"LNsnlopP9tSP"},"source":["This command will generate a fine-tuned model `finetuned-model.tlt` at `$RESULTS_DIR/finetune/checkpoints`"]},{"cell_type":"markdown","metadata":{"id":"eJg7dZJg9tSP"},"source":["---\n","<a id='evaluation'></a>\n","### Evaluation\n","The evaluation spec .yaml is as simple as:\n","\n","```\n","test_ds:\n","  file: ??? # e.g. squad/v1.1/dev-v1.1.json \n","  batch_size: 32\n","  shuffle: false\n","  num_samples: 500\n","```\n","\n","Below, we use `tao question_answering evaluate` and override the test data configuration by specifying `test_ds.file`. Other arguments follow the same pattern as before!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-1cyq_Nr9tSP"},"outputs":[],"source":["!tao question_answering evaluate \\\n","                        -e $SPECS_DIR/evaluate.yaml \\\n","                        -g 1 \\\n","                        -m $RESULTS_DIR/train/checkpoints/trained-model.tlt \\\n","                        -k $KEY \\\n","                        -r $RESULTS_DIR/evaluate \\\n","                        test_ds.file=$DATA_DIR/v2.0/dev-v2.0.json \\\n","                        test_ds.batch_size=32 \\\n","                        test_ds.num_samples=500"]},{"cell_type":"markdown","metadata":{"id":"UAXlLFAa9tSQ"},"source":["The output of Evaluation should give the exact match/f1 scores for each data point. Remember that we had trained for just 1 epoch since this is a demonstration!"]},{"cell_type":"markdown","metadata":{"id":"xWlfeAan9tSQ"},"source":["---\n","<a id='inference'></a>\n","### Inference\n","Inference using a .tlt trained or fine-tuned model uses the `tao question_answering infer` command.  <br>\n","The infer.yaml is also very uncomplicated:\n","```\n","# Name of  file containing data used as inputs during the inference.\n","infer_ds:\n","    file: ???  # e.g. squad/v1.1/dev-v1.1.json\n","\n","```\n","\n","We use the SQuAD 2.0 evaluation file for the sake of demonstration, you can also try out your own custom inputs as an exercise!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tv09uIge9tSQ"},"outputs":[],"source":["!tao question_answering infer \\\n","                        -e $SPECS_DIR/infer.yaml \\\n","                        -g 1 \\\n","                        -m $RESULTS_DIR/train/checkpoints/trained-model.tlt \\\n","                        -k $KEY \\\n","                        -r $RESULTS_DIR/infer \\\n","                        infer_ds.file=$DATA_DIR/v2.0/dev-v2.0.json"]},{"cell_type":"markdown","metadata":{"id":"CK_b4bOr9tSR"},"source":["The results of inference are saved at `$RESULTS_DIR/infer/prediction.txt` in the format:\n","```\n","{\n","    <question_id>: <answer>,\n","    ...\n","}\n","```\n","A file with n-best results for each question is also saved at `$RESULTS_DIR/infer/nbest.txt` in the format:\n","```\n","{\n","    <question_id>: [\n","        {\n","            \"text\": <answer-1>,\n","            \"probability\": 0.9576789114427999,\n","            \"start_logit\": 7.168248653411865,\n","            \"end_logit\": 6.93817138671875\n","        },\n","        {\n","            \"text\": <answer-2>,\n","            \"probability\": 0.02714239211951417,\n","            \"start_logit\": 7.168248653411865,\n","            \"end_logit\": 3.374755620956421\n","        },\n","    ...\n","```"]},{"cell_type":"markdown","metadata":{"id":"cYTZl7lE9tSR"},"source":["---\n","## What's Next?"]},{"cell_type":"markdown","metadata":{"id":"_1Oq6-YC9tSR"},"source":["You could use TAO to build custom models for your own applications, or you could deploy the custom model to Nvidia Riva!"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"question-answering-training.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.10.2 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.2"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":0}
