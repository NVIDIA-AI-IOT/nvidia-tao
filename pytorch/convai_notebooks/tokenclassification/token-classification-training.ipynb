{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAO - Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Adapt Optimize (TAO) Toolkit  is a python based AI toolkit for taking purpose-built pre-trained AI models and customizing them with your own data. \n",
    "\n",
    "Transfer learning extracts learned features from an existing neural network to a new one. Transfer learning is often used when creating a large training dataset is not feasible. \n",
    "\n",
    "Developers, researchers and software partners building intelligent vision AI apps and services, can bring their own data to fine-tune pre-trained models instead of going through the hassle of training from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Train Adapt Optimize (TAO) Toolkit](https://developer.nvidia.com/sites/default/files/akamai/embedded-transfer-learning-toolkit-software-stack-1200x670px.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this toolkit is to reduce that 80 hour workload to an 8 hour workload, which can enable data scientist to have considerably more train-test iterations in the same time frame.\n",
    "\n",
    "Let's see this in action with a use case for Named Entity Recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBdjK77dKOQn"
   },
   "source": [
    "## Connect to a GPU Runtime\n",
    "\n",
    "1.   Change Runtime type to GPU by Runtime(Top Left tab)->Change Runtime Type->GPU(Hardware Accerlerator)\n",
    "2.   Then click on Connect (Top Right)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38R-3LAixO-h"
   },
   "source": [
    "## Mounting Google drive\n",
    "Mount your Google drive storage to this Colab instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dt69GSQgo5f3"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMrKJGSaxnmE"
   },
   "source": [
    "## Setup Python Environment\n",
    "Setup the environment necessary to run the TAO Networks by running the bash script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GsF_BUsxt0Lx"
   },
   "outputs": [],
   "source": [
    "!sh /content/drive/MyDrive/pyt/setup_env.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named entity recognition (NER), also referred to as entity chunking, identification or extraction, is the task of detecting and classifying key information (entities) in text. \n",
    "\n",
    "For example, in a sentence: Mary lives in Santa Clara and works at NVIDIA, we should detect that **Mary** is a person, **Santa Clara** is a location and **NVIDIA** is a company."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition using TAO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Relevant Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: The following paths are set from the perspective of the TAO Docker. \n",
    "\n",
    "# The data is saved here\n",
    "%env DATA_DIR=/data/token_classification\n",
    "\n",
    "# The configuration files are stored here\n",
    "%env SPECS_DIR=/specs/token_classification\n",
    "\n",
    "# The results are saved at this path\n",
    "%env RESULTS_DIR=/results/token_classification\n",
    "\n",
    "%env CACHE_DIR=/.cache\n",
    "\n",
    "# Set your encryption key, and use the same key for all commands\n",
    "%env KEY=tlt_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the source directories exist, if not, create them\n",
    "! mkdir -p $DATA_DIR\n",
    "! mkdir -p $SPECS_DIR\n",
    "! mkdir -p $RESULTS_DIR\n",
    "! mkdir -p $CACHE_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Dataset Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we going to use [GMB(Groningen Meaning Bank)](http://www.let.rug.nl/bjerva/gmb/about.php) corpus for entity recognition.\n",
    "\n",
    "GMB is a fairly large corpus with a lot of annotations. Note, that GMB is not completely human annotated and itâ€™s not considered 100% correct.\n",
    "\n",
    "The data is labeled using the [IOB format](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging) (short for inside, outside, beginning). \n",
    "\n",
    "The following classes appear in the dataset:\n",
    "\n",
    "* LOC = Geographical Entity\n",
    "* ORG = Organization\n",
    "* PER = Person\n",
    "* GPE = Geopolitical Entity\n",
    "* TIME = Time indicator\n",
    "* ART = Artifact\n",
    "* EVE = Event\n",
    "* NAT = Natural Phenomenon\n",
    "\n",
    "For this tutorial, classes ART, EVE, and NAT were combined into a MISC class due to small number of examples for these classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://dldata-public.s3.us-east-2.amazonaws.com/gmb_v_2.2.0_clean.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! unzip gmb_v_2.2.0_clean.zip -d $DATA_DOWNLOAD_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the data folder should contain 5 files:\n",
    "\n",
    "- labels_dev.txt\n",
    "- labels_train.txt\n",
    "- text_dev.txt\n",
    "- text_train.txt\n",
    "- label_ids.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -l $DATA_DOWNLOAD_DIR/gmb_v_2.2.0_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take a look at the data \n",
    "print('Train text:')\n",
    "! head -n 5 {DATA_DOWNLOAD_DIR}/gmb_v_2.2.0_clean/text_train.txt\n",
    "\n",
    "print('\\nTrain label:')\n",
    "! head -n 5 {DATA_DOWNLOAD_DIR}/gmb_v_2.2.0_clean/labels_train.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Set Relevant Paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that everything is setup, we would like to take a bit of time to explain the tao interface for ease of use. The command structure can be broken down as follows: `tao <task name> <subcommand>` <br> \n",
    "\n",
    "Let's see this in further detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Downloading Specs\n",
    "TAO's Conversational AI Toolkit works off of spec files which make it easy to edit hyperparameters on the fly. We can proceed to downloading the spec files. The user may choose to modify/rewrite these specs, or even individually override them through the launcher. You can download the default spec files by using the `download_specs` command. <br>\n",
    "\n",
    "The -o argument indicating the folder where the default specification files will be downloaded, and -r that instructs the script where to save the logs. **Make sure the -o points to an empty folder!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!token_classification download_specs \\\n",
    "    -r $RESULTS_DIR/token_classification \\\n",
    "    -o $SPECS_DIR/token_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TokenClassification Model in NeMo](https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/nlp/models/token_classification/token_classification_model.py) supports NER and other token level classification tasks, as long as the data follows the format specified below. \n",
    "\n",
    "Token Classification Model requires the data to be split into 2 files: \n",
    "* text.txt and \n",
    "* labels.txt. \n",
    "\n",
    "Each line of the **text.txt** file contains text sequences, where words are separated with spaces, i.e.: \n",
    "[WORD] [SPACE] [WORD] [SPACE] [WORD].\n",
    "\n",
    "The **labels.txt** file contains corresponding labels for each word in text.txt, the labels are separated with spaces, i.e.:\n",
    "[LABEL] [SPACE] [LABEL] [SPACE] [LABEL].\n",
    "\n",
    "Example of a text.txt file:\n",
    "```\n",
    "Jennifer is from New York City .\n",
    "She likes ...\n",
    "...\n",
    "```\n",
    "Corresponding labels.txt file:\n",
    "```\n",
    "B-PER O O B-LOC I-LOC I-LOC O\n",
    "O O ...\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert an IOB format data to the format required for training, we can use the `dataset_convert` command in TAO on your train and dev files\n",
    "\n",
    "For this tutorial, we are using the preprocessed GMB dataset, thus we won't be required to convert the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Train the NER model from scratch using pre-trained BERT base uncased model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Named Entity Recognition model is comprised of the pretrained BERT model followed by a Token Classification layer.\n",
    "\n",
    "The model is defined in a config file which are already available for you to use directly or as reference to create your own. \n",
    "\n",
    "Through these spec files, the user can tune many knobs like the model, dataset, hyperparameters, optimizer etc. Each command (like train, finetune, evaluate etc.) should have a dedicated spec file. These sample spec files are available at `$SPECS_DIR/token_classification`\n",
    "\n",
    "The important sections to look out for are:\n",
    "\n",
    "- model: All arguments that are related to the model - language model, token classifier, optimizer and schedulers, datasets and any other related information\n",
    "- trainer: Any argument to be passed to PyTorch Lightning\n",
    "\n",
    "The model config file for token classification is available at [Github](https://github.com/NVIDIA/NeMo/blob/main/examples/nlp/token_classification/conf/token_classification_config.yaml). A similar file (at the above location inside TAO docker image) is used for TAO.\n",
    "\n",
    "Among other things, the config file contains dictionaries called **dataset**, **train_ds** and **validation_ds**. These are configurations used to setup the Dataset and DataLoaders of the corresponding config.\n",
    "\n",
    "We assume that both training and evaluation files are located in the same directory, and use the default names mentioned during the data download step. So, to start model training, we simply need to specify `model.dataset.data_dir`, like we are going to do below.\n",
    "\n",
    "Also notice that some config lines, including `model.dataset.data_dir` have ??? in place of paths, which means that the values for these fields are required to be specified by the user.\n",
    "\n",
    "Thus, we must provide the following parameters to TAO:\n",
    "1. **data_dir** - path to the processed data\n",
    "2. **model.label_ids** - mapping of labels with their numerical ids\n",
    "\n",
    "We can pass these parameters while using the TAO train command. In addition to these, we can also change other parameters like max_epochs.\n",
    "\n",
    "A similar version of model config file we saw above is also passed to the TAO train command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!token_classification train \\\n",
    "                        -e $SPECS_DIR/token_classification/train.yaml \\\n",
    "                        -g 1  \\\n",
    "                        -k $KEY \\\n",
    "                        -r $RESULTS_DIR/token_classification/train \\\n",
    "                        data_dir=$DATA_DIR/gmb_v_2.2.0_clean \\\n",
    "                        training_ds.num_samples=-1 \\\n",
    "                        validation_ds.num_samples=-1 \\\n",
    "                        model.label_ids=$DATA_DIR/gmb_v_2.2.0_clean/label_ids.csv \\\n",
    "                        trainer.max_epochs=1 \\\n",
    "                        model.language_model.pretrained_model_name=bert-base-uncased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Finetune NER model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we were training from scratch, the datasets were prepared for training during the model initialization. When we are using a pretrained NER model, before training, we need to setup training and evaluation data, and also provide path to the pre-trained model.tlt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity of this tutorial, we will be using the `trained-model.tlt` that was saved during the last section (Train the NER model from scratch using pre-trained BERT Base uncased model) and finetune it again on the GMB dataset.\n",
    "\n",
    "Note: If you wish to proceed with a trained dataset for better inference results, you can find a .nemo model [here](\n",
    "https://ngc.nvidia.com/catalog/collections/nvidia:nemotrainingframework).\n",
    "\n",
    "Simply re-name the .nemo file to .tlt and pass it through the finetune pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!token_classification finetune \\\n",
    "                        -e $SPECS_DIR/token_classification/finetune.yaml \\\n",
    "                        -g 1 \\\n",
    "                        -m $RESULTS_DIR/token_classification/train/checkpoints/trained-model.tlt \\\n",
    "                        -k $KEY \\\n",
    "                        -r $RESULTS_DIR/token_classification/finetune-bert-base \\\n",
    "                        data_dir=$DATA_DIR/gmb_v_2.2.0_clean \\\n",
    "                        trainer.max_epochs=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command will generate a fine-tuned model `finetuned-model.tlt` at $RESULTS_DIR/finetune-bert-base/checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Evaluate NER on the Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how the model performs, we can generate prediction the same way we did it earlier or we can use our model to generate predictions for a dataset from a file, for example, to perform final evaluation or to do error analysis. Below, we are using a subset of dev set, but it could be any text file as long as it follows the data format described above. \n",
    "\n",
    "`labels_file` is optional here, and if provided will be used to get metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the below command, TAO will evaluate on `text_dev.txt` present in the data_dir, using `trained-model.tlt` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!token_classification evaluate \\\n",
    "                        -e $SPECS_DIR/token_classification/evaluate.yaml \\\n",
    "                        -g 1 \\\n",
    "                        -m $RESULTS_DIR/token_classification/finetune-bert-base/checkpoints/finetuned-model.tlt \\\n",
    "                        -k $KEY \\\n",
    "                        -r $RESULTS_DIR/token_classification/evaluate \\\n",
    "                        data_dir=$DATA_DIR/gmb_v_2.2.0_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Infer on a custom input sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the model's predictions on a given sentence of your choice, you can save those sentences in infer.yaml and use TAO infer command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The infer.yaml contains:\n",
    "\n",
    "```\n",
    "# Copyright (c) 2020, NVIDIA CORPORATION. All rights reserved.\n",
    "# TAO Spec file for inference using the previously pretrained Token Classification model.\n",
    "\n",
    "# \"Simulate\" user input: batch with two samples.\n",
    "input_batch:\n",
    "  - 'We bought four shirts from the Nvidia gear store in Santa Clara.'\n",
    "  - 'Nvidia is a company.'\n",
    "\n",
    "```\n",
    "Please edit `input_batch` with your own sentences to measure the output of this NER model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!token_classification infer \\\n",
    "                        -e $SPECS_DIR/token_classification/infer.yaml \\\n",
    "                        -g 1 \\\n",
    "                        -m $RESULTS_DIR/token_classification/finetune-bert-base/checkpoints/finetuned-model.tlt \\\n",
    "                        -k $KEY \\\n",
    "                        -r $RESULTS_DIR/token_classification/infer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### What's Next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could use TAO to build custom models for your own applications, or you could deploy the custom model to Nvidia Riva!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
