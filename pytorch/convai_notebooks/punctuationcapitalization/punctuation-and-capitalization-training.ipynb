{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Punctuation And Capitalization using Train Adapt Optimize (TAO) Toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Adapt Optimize (TAO) Toolkit  is a python based AI toolkit for taking purpose-built pre-trained AI models and customizing them with your own data. \n",
    "\n",
    "Transfer learning extracts learned features from an existing neural network to a new one. Transfer learning is often used when creating a large training dataset is not feasible. \n",
    "\n",
    "Developers, researchers and software partners building intelligent vision AI apps and services, can bring their own data to fine-tune pre-trained models instead of going through the hassle of training from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Train Adapt Optimize (TAO) Toolkit](https://developer.nvidia.com/sites/default/files/akamai/embedded-transfer-learning-toolkit-software-stack-1200x670px.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this toolkit is to reduce that 80 hour workload to an 8 hour workload, which can enable data scientist to have considerably more train-test iterations in the same time frame.\n",
    "\n",
    "In this notebook, you will learn how to leverage the simplicity and convenience of TAO to:\n",
    "- Take a BERT model and __Train/Finetune__ it on a dataset for Punctuation and Capitalization task\n",
    "- Run __Inference__\n",
    "- __Export__ the model to the ONNX format, or export in the format that is suitable for deployment in Riva\n",
    "\n",
    "The earlier section in this notebook gives a brief introduction to the Punctuation and Capitalization task and the dataset being used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBdjK77dKOQn"
   },
   "source": [
    "## Connect to a GPU Runtime\n",
    "\n",
    "1.   Change Runtime type to GPU by Runtime(Top Left tab)->Change Runtime Type->GPU(Hardware Accerlerator)\n",
    "2.   Then click on Connect (Top Right)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38R-3LAixO-h"
   },
   "source": [
    "## Mounting Google drive\n",
    "Mount your Google drive storage to this Colab instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dt69GSQgo5f3"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMrKJGSaxnmE"
   },
   "source": [
    "## Setup Python Environment\n",
    "Setup the environment necessary to run the TAO Networks by running the bash script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GsF_BUsxt0Lx"
   },
   "outputs": [],
   "source": [
    "!sh /content/drive/MyDrive/pyt/setup_env.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punctuation And Capitalization using TAO \n",
    "### Task Description\n",
    "\n",
    "Automatic Speech Recognition (ASR) systems typically generate text with no punctuation and capitalization of the words. This tutorial explains how to implement a model that will predict punctuation and capitalization for each word in a sentence to make ASR output more readable and to boost performance of the named entity recognition, machine translation or text-to-speech models. We'll show how to train a model for this task using a pre-trained BERT model. For every word in our training dataset weâ€™re going to predict:\n",
    "\n",
    "- punctuation mark that should follow the word \n",
    "- whether the word should be capitalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the notebook exemplifies the simplicity of the TAO workflow. \n",
    "Users with any level of Deep Learning knowledge can get started building their own custom models using a simple specification file. It's essentially just one command each to run data download and preprocessing, training, fine-tuning, evaluation, inference, and export. \n",
    "All configurations happen through YAML specification files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Configuration/Specification Files\n",
    "\n",
    "All commands in TAO lies in the YAML specification files. There are sample specification files already available for you to use directly or as reference to create your own YAML specification files.  \n",
    "\n",
    "Through these specification files, you can tune many a lot of things like the model, dataset, hyperparameters, optimizer etc.\n",
    "\n",
    "Each command (like download_and_convert, train, finetune, evaluate etc.) should have a dedicated specification file with configurations pertinent to it.\n",
    "\n",
    "Here is an example of the training spec file:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "save_to: trained-model.tlt\n",
    "\n",
    "trainer:\n",
    "  max_epochs: 5\n",
    "  \n",
    "model:\n",
    "  punct_label_ids:\n",
    "    O: 0\n",
    "    ',': 1\n",
    "    '.': 2\n",
    "    '?': 3\n",
    "\n",
    "  capit_label_ids:\n",
    "    O: 0\n",
    "    U: 1 \n",
    "\n",
    "  tokenizer:\n",
    "      tokenizer_name: ${model.language_model.pretrained_model_name} # or sentencepiece =\n",
    "      vocab_file: null # path to vocab file \n",
    "      tokenizer_model: null # only used if tokenizer is sentencepiece\n",
    "      special_tokens: null\n",
    "\n",
    "  language_model:\n",
    "    pretrained_model_name: bert-base-uncased\n",
    "    lm_checkpoint: null\n",
    "    config_file: null # json file, precedence over config\n",
    "    config: null \n",
    "\n",
    "  punct_head:\n",
    "    punct_num_fc_layers: 1\n",
    "    fc_dropout: 0.1\n",
    "    activation: 'relu'\n",
    "    use_transformer_init: true\n",
    "\n",
    "  capit_head:\n",
    "    capit_num_fc_layers: 1\n",
    "    fc_dropout: 0.1\n",
    "    activation: 'relu'\n",
    "    use_transformer_init: true\n",
    "\n",
    "# Data dir containing dataset.\n",
    "data_dir: ???\n",
    "\n",
    "training_ds:\n",
    "  text_file: text_train.txt\n",
    "  labels_file: labels_train.txt\n",
    "  shuffle: true\n",
    "  num_samples: -1 # number of samples to be considered, -1 means the whole the dataset\n",
    "  batch_size: 64\n",
    "\n",
    "validation_ds:\n",
    "  text_file: text_dev.txt\n",
    "  labels_file: labels_dev.txt\n",
    "  shuffle: false\n",
    "  num_samples: -1 # number of samples to be considered, -1 means the whole the dataset\n",
    "  batch_size: 64\n",
    "\n",
    "optim:\n",
    "  name: adam\n",
    "  lr: 1e-5\n",
    "  weight_decay: 0.00\n",
    "\n",
    "  sched:\n",
    "    name: WarmupAnnealing\n",
    "    # Scheduler params\n",
    "    warmup_steps: null\n",
    "    warmup_ratio: 0.1\n",
    "    last_epoch: -1\n",
    "\n",
    "    # pytorch lightning args\n",
    "    monitor: val_loss\n",
    "    reduce_on_plateau: false \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Set Relevant Paths\n",
    "Please set these paths according to your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: The following paths are set from the perspective of the TAO Docker. \n",
    "\n",
    "# The data is saved here\n",
    "%env DATA_DIR=/data\n",
    "\n",
    "# The configuration files are stored here\n",
    "%env SPECS_DIR=/specs/punctuation_and_capitalization\n",
    "\n",
    "# The results are saved at this path\n",
    "%env RESULTS_DIR=/results/punctuation_and_capitalization\n",
    "\n",
    "%env CACHE_DIR=/.cache\n",
    "\n",
    "# Set your encryption key, and use the same key for all commands\n",
    "%env KEY=tlt_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the source directories exist, if not, create them\n",
    "! mkdir -p $DATA_DIR\n",
    "! mkdir -p $SPECS_DIR\n",
    "! mkdir -p $RESULTS_DIR\n",
    "! mkdir -p $CACHE_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Downloading Specs\n",
    "We can proceed to downloading the spec files. The user may choose to modify/rewrite these specs, or even individually override them through the launcher. You can download the default spec files by using the `download_specs` command. <br>\n",
    "\n",
    "The -o argument indicating the folder where the default specification files will be downloaded, and -r that instructs the script where to save the logs. **Make sure the -o points to an empty folder!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!punctuation_and_capitalization download_specs \\\n",
    "    -r $RESULTS_DIR/ \\\n",
    "    -o $SPECS_DIR/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Dataset \n",
    "\n",
    "This model can work with any dataset as long as it follows the format specified below. The training and evaluation data is divided into _2 files: text.txt and labels.txt_. Each line of the __text.txt__ file contains text sequences, where words are separated with spaces: [WORD] [SPACE] [WORD] [SPACE] [WORD], for example:\n",
    "\n",
    "when is the next flight to new york<br>\n",
    "the next flight is ...<br>\n",
    "...<br>\n",
    "\n",
    "The __labels.txt__ file contains corresponding labels for each word in text.txt, the labels are separated with spaces. Each label in labels.txt file consists of 2 symbols:\n",
    "\n",
    "- the first symbol of the label indicates what punctuation mark should follow the word (where O means no punctuation needed);\n",
    "- the second symbol determines if a word needs to be capitalized or not (where U indicates that the word should be upper cased, and O - no capitalization needed.)\n",
    "In this tutorial, we are considering only commas, periods, and question marks the rest punctuation marks were removed. To use more punctuation marks, update the dataset to include desired labels, no changes to the model needed.\n",
    "\n",
    "Each line of the __labels.txt__ should follow the format: [LABEL] [SPACE] [LABEL] [SPACE] [LABEL] (for labels.txt). For example, labels for the above text.txt file should be:\n",
    "\n",
    "OU OO OO OO OO OO OU ?U<br>\n",
    "OU OO OO OO ...<br>\n",
    "...\n",
    "\n",
    "The complete list of all possible labels for this task used in this tutorial is: OO, ,O, .O, ?O, OU, ,U, .U, ?U."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and preprocess the data \n",
    "\n",
    "In this notebook we are going to use a subset of English examples from the [Tatoeba collection of sentences](https://tatoeba.org/eng). \n",
    "\n",
    "Downloading and preprocessing the data using TAO is as simple as configuring YAML specification file and running the ``download_and_convert_tatoeba`` command. The code cell below uses the default `download_and_convert_tatoeba.yaml` available for the users as a reference. \n",
    "\n",
    "The configurations in the specification file can be easily overridden using the tao-launcher CLI as shown below. For instance, we override the ``source_data_dir`` and ``target_data_dir`` configurations.\n",
    "\n",
    "We encourage you to take a look at the YAML files we have provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After executing the cell below, your data folder will contain the following 4 files:\n",
    "- labels_dev.txt\n",
    "- labels_train.txt\n",
    "- text_dev.txt\n",
    "- text_train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### To download and convert the dataset\n",
    "!punctuation_and_capitalization download_and_convert_tatoeba \\\n",
    "    -e $SPECS_DIR/download_and_convert_tatoeba.yaml \\\n",
    "    -r $RESULTS_DIR/download_and_convert_tatoeba \\\n",
    "    source_data_dir=$DATA_DIR \\\n",
    "    target_data_dir=$DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Training \n",
    "\n",
    "In the Punctuation and Capitalization Model, we are jointly training two token-level classifiers on top of the pretrained [BERT](https://arxiv.org/pdf/1810.04805.pdf) model:\n",
    "\n",
    "- one classifier to predict punctuation and\n",
    "- the other one - capitalization.\n",
    "\n",
    "Training a model using TAO is as simple as configuring your spec file and running the train command. The code cell below uses the default train.yaml available for users as reference. It is configured by default to use the ``bert-base-uncased`` pretrained model. Additionally, these configurations could easily be overridden using the tao-launcher CLI as shown below. For instance, below we override the trainer.max_epochs, training_ds.num_samples and validation_ds.num_samples configurations to suit our needs. We encourage you to take a look at the .yaml spec files we provide!\n",
    "\n",
    "The command for training is very similar to the of ``download_and_convert_tatoeba``. Instead of ``tao punctuation_and_capitalization download_and_convert_tatoeba``, we use ``tao punctuation_and_capitalization train`` instead. The  ``tao punctuation_and_capitalization train`` command has the following arguments:\n",
    "\n",
    "- ``-e`` : Path to the spec file\n",
    "- ``-g`` : Number of GPUs to use\n",
    "- ``-k`` : User specified encryption key to use while saving/loading the model\n",
    "- ``-r`` : Path to the folder where the outputs should be written. Make sure this is mapped in the tlt_mounts.json\n",
    "- Any overrides to the spec file eg. trainer.max_epochs\n",
    "\n",
    "More details about these arguments are present in the  [TAO Getting Started Guide](https://docs.nvidia.com/tao/tao-toolkit/index.html).\n",
    "\n",
    "``NOTE:`` All file paths corresponds to the destination mounted directory that is visible in the TAO docker container used in backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### To train the dataset with BERT-base-uncased model\n",
    "!punctuation_and_capitalization train \\\n",
    "    -e $SPECS_DIR/train.yaml \\\n",
    "    -g 4 \\\n",
    "    -r $RESULTS_DIR/train \\\n",
    "    data_dir=$DATA_DIR \\\n",
    "    trainer.max_epochs=2 \\\n",
    "    training_ds.num_samples=-1  \\\n",
    "    validation_ds.num_samples=-1 \\\n",
    "    -k $KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train command produces a .tlt file called ``trained-model.tlt`` saved at ``$RESULTS_DIR/checkpoints/trained-model.tlt``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other tips and tricks:\n",
    "\n",
    "- To accelerate the training without loss of quality, it is possible to train with these parameters: ``trainer.amp_level=\"O1\"`` and ``trainer.precision=16`` for reduced precision.\n",
    "- The batch size ``training_ds.batch_size`` may influence the validation accuracy. Larger batch sizes are faster to train with, however, you may get slightly better results with smaller batches.\n",
    "- You can also change the optimizer parameter ``optim.name`` and can see its effect on the punctuation and capitalization task by the change in accuracy.\n",
    "- You can specify the number of layers in the head of the model ``model.punct_head.punct_num_fc_layers`` and ``model.capit_head.capit_num_fc_layers``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Finetuning\n",
    "\n",
    "As stated above the command for all the tasks are very similar but have different YAML specification files that can be tweaked.\n",
    "\n",
    "Note: If you wish to proceed with a trained dataset for better inference results, you can find a .nemo model [here](\n",
    "https://ngc.nvidia.com/catalog/collections/nvidia:nemotrainingframework).\n",
    "\n",
    "Simply re-name the .nemo file to .tlt and pass it through the finetune pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### To finetune on the dataset\n",
    "!punctuation_and_capitalization finetune \\\n",
    "    -e $SPECS_DIR/finetune.yaml \\\n",
    "    -g 4 \\\n",
    "    -m $RESULTS_DIR/train/checkpoints/trained-model.tlt \\\n",
    "    -r $RESULTS_DIR/finetune \\\n",
    "    data_dir=$DATA_DIR \\\n",
    "    trainer.max_epochs=3 \\\n",
    "    -k $KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train command produces a .tlt file called ``finetuned-model.tlt`` saved in the results directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Evaluation\n",
    "\n",
    "To evaluate our TAO model we will run the command below. It is always advisable to look at the YAML file for evaluate to understand the command in a better way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### For evaluation\n",
    "!punctuation_and_capitalization evaluate \\\n",
    "    -e $SPECS_DIR/evaluate.yaml \\\n",
    "    -g 4 \\\n",
    "    -m $RESULTS_DIR/finetune/checkpoints/finetuned-model.tlt \\\n",
    "    data_dir=$DATA_DIR \\\n",
    "    -r $RESULTS_DIR/evaluate \\\n",
    "    -k $KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On evaluating the model you will get some results and based on that we can either retrain the model for more epochs or continue with the inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Inference\n",
    "\n",
    "Inference using a TAO trained and fine-tuned model can be done by ``tao punctuation_and_capitalization infer`` command. It is again advisable to look at the infer.yaml file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### For inference\n",
    "!punctuation_and_capitalization infer \\\n",
    "    -e $SPECS_DIR/infer.yaml \\\n",
    "    -g 4 \\\n",
    "    -m $RESULTS_DIR/finetune/checkpoints/finetuned-model.tlt \\\n",
    "    -r $RESULTS_DIR/infer \\\n",
    "    -k $KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### What next?\n",
    "\n",
    "You can use TAO to build custom models for your own NLP applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
