{"cells":[{"cell_type":"markdown","metadata":{"id":"wVmYUoXv9zH-"},"source":["# Text Classification using Train Adapt Optimize (TAO) Toolkit"]},{"cell_type":"markdown","metadata":{"id":"6DcGAces9zH_"},"source":["Transfer learning is a technique of extracting learned features from an existing neural network to a new one. It is a commonly used training technique where you use a model trained on one task and re-train to use it on a different task, when creating a large training dataset is not feasible. \n","\n","**Train Adapt Optimize (TAO) Toolkit ** is a simple and easy-to-use python based AI toolkit for taking purpose-built pre-trained AI models and customizing them with users' own data.\n","\n","\n","![Train Adapt Optimize (TAO) Toolkit](https://developer.nvidia.com/sites/default/files/akamai/embedded-transfer-learning-toolkit-software-stack-1200x670px.png)\n","\n","\n","Let's see this in action with a use case for text classification.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"jBdjK77dKOQn"},"source":["## Connect to a GPU Runtime\n","\n","1.   Change Runtime type to GPU by Runtime(Top Left tab)->Change Runtime Type->GPU(Hardware Accelerator)\n","2.   Then click on Connect (Top Right)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"38R-3LAixO-h"},"source":["## Mounting Google drive\n","Mount your Google drive storage to this Colab instance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dt69GSQgo5f3"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"iMrKJGSaxnmE"},"source":["## Setup Python Environment\n","Setup the environment necessary to run the TAO Networks by running the bash script"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GsF_BUsxt0Lx"},"outputs":[],"source":["!sh /content/drive/MyDrive/ColabNotebooks/pytorch/setup_env.sh"]},{"cell_type":"markdown","metadata":{"id":"mJActfEP9zIB"},"source":["## Learning Objectives\n","In this notebook, you will learn how to leverage the simplicity and convenience of TAO to:\n","- Take a [BERT](https://arxiv.org/pdf/1810.04805.pdf) Text Classification model, [**Train**](#training) and [**Finetune**](#ft) it on the [SST-2 dataset](https://dl.fbaipublicfiles.com/glue/data/SST-2.zip)\n","- Run [**Evaluation**](#evaluation) and [**Inference**](#inference)\n","- [**Export**](#export-onnx) the model for the [ONNX](https://onnx.ai/) format, or [export](#export-riva) to deployment on [Riva](https://developer.nvidia.com/riva).\n","\n","The earlier sections in the notebook give a brief introduction to the Text Classification task and the SST-2 dataset. If you are already familiar with these, and want to jump right into the meat of the matter, you can start at section on [Download and preprocess the dataset](#prepare-data)."]},{"cell_type":"markdown","metadata":{"id":"MR33GMo69zIC"},"source":["## Text Classification\n","\n","### Task Description\n","Text Classification is one of the most common tasks in NLP, which is the process of categorizing the text into a group of words. By using NLP, text classification can automatically analyze text and then assign a set of predefined tags or categories based on its context. It is applied in a wide variety of applications, including sentiment analysis, spam filtering, news categorization, domain/intent detection for dialogue systems, etc. The dataset we use in this notebook, [SST-2](https://dl.fbaipublicfiles.com/glue/data/SST-2.zip), pose this as a sentiment analysis task, i.e. given a piece of text, the goal is to estimate its sentiment polarity based solely on its content. \n","\n","For every entry in the training dataset, we predict the sentiment label of the sentence."]},{"cell_type":"markdown","metadata":{"id":"Maz4jPYY9zIC"},"source":["### Datasets\n","This model supports text classification problems such as sentiment analysis or domain/intent detection for dialogue systems, as long as the data follows the format specified below.\n","\n","The Text Classification Model requires the data to be stored in TAB separated files (.tsv) with two columns of sentence and label. Each line of the data file contains text sequences, where words are separated with spaces and label separated with [TAB], i.e.:\n","\n","    [WORD] [SPACE] [WORD] [SPACE] [WORD] [TAB] [LABEL]\n","\n","For example:\n","\n","    hide new secretions from the parental units [TAB] 0\n","\n","    that loves its characters and communicates something rather beautiful about human nature [TAB] 1\n","\n","    ...\n","If your dataset is stored in another format, you need to convert it to this format to use the Text Classification Model."]},{"cell_type":"markdown","metadata":{"id":"3wwRjsln9zID"},"source":["#### The SST-2 Dataset\n","\n","The Stanford Sentiment Treebank dataset contains 215,154 phrases with fine-grained sentiment labels in the parse trees of 11,855 sentences from movie reviews. Model performances are evaluated either based on a fine-grained (5-way) or binary classification model based on accuracy. The [SST-2](https://dl.fbaipublicfiles.com/glue/data/SST-2.zip) Binary classification version is identical to SST-1, but with neutral reviews deleted.\n","\n","The SST-2 format consists of a .tsv file for each dataset split, i.e. train, dev, and test data. Each entry has a space-separated sentence, followed by a tab and a label.\n","\n","For example:\n","\n","    sentence\tlabel\n","    \n","    excruciatingly unfunny and pitifully unromantic \t0\n","    \n","    enriched by an imaginatively mixed cast of antic spirits \t1\n","    \n","    ..."]},{"cell_type":"markdown","metadata":{"id":"luUh9H-79zID"},"source":["<a id='prepare-data'></a>\n","### Download and preprocess the dataset \n","\n","We provide a function, download_sst2, for downloading the data. Please refer to the TAO workflow section for the [data preprocessing and conversion part](#dataset-convert)."]},{"cell_type":"markdown","metadata":{"id":"i49fgM6X9zID"},"source":["#### Downloading the dataset\n","\n","For convenience, you may use the code below to download the dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"waKyUFtI9zID"},"outputs":[],"source":["import urllib.request\n","import os\n","\n","# simple utility function to download the SST-2 dataset\n","def download_sst2(save_path):\n","\n","    if not os.path.exists(save_path):\n","        os.makedirs(save_path)\n","        \n","    # url and filename of the dataset\n","    download_urls = {\n","            'https://dl.fbaipublicfiles.com/glue/data/SST-2.zip': 'SST-2.zip',\n","    }\n","         \n","    for item in download_urls:\n","        url = item\n","        file = download_urls[item]\n","        print('Downloading:', url)\n","        if os.path.isfile(save_path + '/' + file):\n","            print('** Download file already exists, skipping download **')\n","        else:\n","            response = urllib.request.urlopen(url)\n","            with open(save_path + '/' + file, \"wb\") as handle:\n","                handle.write(response.read())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zqUzFtRQ9zID"},"outputs":[],"source":["# IMPORTANT Set the following path\n","DATA_DIR = \"<YOUR_PATH_TO_DATA_DIR>\"\n","\n","# This will download the SST-2 dataset\n","download_sst2(DATA_DIR)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EQgkC__w9zID"},"outputs":[],"source":["! unzip -o $DATA_DIR/SST-2.zip -d {DATA_DIR}"]},{"cell_type":"markdown","metadata":{"id":"n_DB0Hhw9zIE"},"source":["After executing the cell below, $DATA_DIR/SST-2 will contain the following 3 files and 1 folder:\n","- dev.tsv\n","- test.tsv\n","- train.tsv\n","- original"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_FOcs1Bg9zIE"},"outputs":[],"source":["# Verify that the data is present\n","!ls $DATA_DIR/SST-2"]},{"cell_type":"markdown","metadata":{"id":"C0oA4GnK9zIE"},"source":["## TAO workflow\n","The rest of the notebook shows what a sample TAO workflow looks like.\n","\n","### Setting Relevant paths"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"83bGWgum9zIE"},"outputs":[],"source":["# NOTE: The following paths are set from the perspective of the TAO Docker. \n","\n","# The data is saved here\n","%env DATA_DIR=/data/text_classification\n","\n","# The configuration files are stored here\n","%env SPECS_DIR=/specs/text_classification\n","\n","# The results are saved at this path\n","%env RESULTS_DIR=/results/text_classification\n","\n","%env CACHE_DIR=/.cache\n","\n","# Set your encryption key, and use the same key for all commands\n","%env KEY=tlt_encode"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QYssC19v9zIE"},"outputs":[],"source":["# Make sure the source directories exist, if not, create them\n","! mkdir -p $DATA_DIR\n","! mkdir -p $SPECS_DIR\n","! mkdir -p $RESULTS_DIR\n","! mkdir -p $CACHE_DIR"]},{"cell_type":"markdown","metadata":{"id":"V0YS1g1z9zIE"},"source":["The rest of the notebook exemplifies the simplicity of the TAO workflow. \n","\n","Users with basic knowledge of Deep Learning can get started building their own custom models using a simple specification file. It's essentially just one command each to run data preprocessing, training, fine-tuning, evaluation, inference, and export! All configurations happen through YAML spec files. <br>"]},{"cell_type":"markdown","metadata":{"id":"MIohnh3H9zIE"},"source":["### Configuration/Specification Files\n","\n","The essence of all commands in TAO lies in the **YAML specification files**. There are sample specification files already available for you to use directly or as reference to create your own.\n","\n","Through these specification files, you can tune many knobs like the model, dataset, hyperparameters, optimizers, etc.\n","\n","Each command (like download_and_convert, train, finetune, evaluate, infer, etc.) should have a dedicated specification file with configurations pertinent to it. \n","\n","Here is an example of the training spec file:\n","\n","\n","```\n","trainer:\n","  max_epochs: 100\n","\n","# Name of the .tlt file where trained model will be saved.\n","save_to: trained-model.tlt\n","\n","model:\n","  # Labels that will be used to \"decode\" predictions.\n","  labels:\n","    \"0\": \"negative\"\n","    \"1\": \"positive\"\n","\n","  tokenizer:\n","      tokenizer_name: ${model.language_model.pretrained_model_name} # or sentencepiece\n","      vocab_file: null # path to vocab file \n","      tokenizer_model: null # only used if tokenizer is sentencepiece\n","      special_tokens: null\n","\n"," language_model:\n","    pretrained_model_name: bert-base-uncased\n","    lm_checkpoint: null\n","    config_file: null # json file, precedence over config\n","    config: null \n","\n","  classifier_head:\n","    # This comes directly from number of labels/target classes.\n","    num_output_layers: 2\n","    fc_dropout: 0.1\n","\n","\n","training_ds:\n","  file_path: ???\n","  batch_size: 64\n","  shuffle: true\n","  num_samples: -1 # number of samples to be considered, -1 means all the dataset\n","\n","...\n","```"]},{"cell_type":"markdown","metadata":{"id":"SIYCJBlV9zIF"},"source":["Now that everything is setup, we would like to take a bit of time to explain the tao interface for ease of use. The command structure can be broken down as follows: `tao <task name> <subcommand>` <br> \n","\n","Let's see this in further detail."]},{"cell_type":"markdown","metadata":{"id":"hafVpT1O9zIF"},"source":["### Downloading Specs\n","We can proceed to downloading the spec files. The user may choose to modify/rewrite these specs, or even individually override them through the launcher. You can download the default spec files by using the `download_specs` command. <br>\n","\n","The -o argument indicating the folder where the default specification files will be downloaded, and -r that instructs the script where to save the logs. **Make sure the -o points to an empty folder!**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EB18FPuM9zIF"},"outputs":[],"source":["!tao text_classification download_specs \\\n","    -r $RESULTS_DIR \\\n","    -o $SPECS_DIR"]},{"cell_type":"markdown","metadata":{"id":"7zXUeaqR9zIF"},"source":["<a id='dataset-convert'></a>\n","### Dataset Convert\n","\n","The dataset conversion feature currently supports SST-2 and IMDB dataset. You need to specify the following parameters in the command:\n","\n","- dataset_name: \"sst2\" or \"imdb\"\n","- source_data_dir: directory path for the downloaded dataset\n","- target_data_dir: directory path for the processed dataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_VJYzkec9zIF"},"outputs":[],"source":["# 1. For BERT dataset conversion:\n","!tao text_classification dataset_convert \\\n","    -e $SPECS_DIR/dataset_convert.yaml \\\n","    -r $SPECS_DIR/dataset_convert \\\n","    dataset_name=sst2 source_data_dir=$DATA_DIR/SST-2 target_data_dir=$DATA_DIR/sst2"]},{"cell_type":"markdown","metadata":{"id":"YJ2-YuKc9zIF"},"source":["<a id='training'></a>\n","### Training\n","\n","Training a model using TAO is as simple as configuring your spec file and running the train command. The code cell below uses the default train.yaml available for users as reference. It is configured by default to use the `bert-base-uncased` pretrained model. Additionally, these configurations could easily be overridden using the tao-launcher CLI as shown below. For instance, below we override the `training_ds.file`, `validation_ds.file`, `trainer.max_epochs`, `training_ds.num_workers` and `validation_ds.num_workers` configurations to suit our needs. We encourage you to take a look at the .yaml spec files we provide! <br>\n","\n","In order to get good results, you need to train for 20-50 epochs (depends on the size of the data). Training with 1 epoch in the tutorial is just for demonstration purposes.\n","\n","\n","For training a Text Classification model in TAO, we use the `tao text_classification train` command with the following args:\n","- `-e`: Path to the spec file\n","- `-g`: Number of GPUs to use\n","- `-k`: User specified encryption key to use while saving/loading the model\n","- `-r`: Path to a folder where the outputs should be written. Make sure this is mapped in tlt_mounts.json\n","- Any overrides to the spec file eg. trainer.max_epochs\n","\n","More details about these arguments are present in the [TAO Getting Started Guide](https://docs.nvidia.com/tao/tao-toolkit/index.html) <br>\n","`NOTE:` All file paths corresponds to the destination mounted directory that is visible in the TAO docker container used in backend.<br>\n","\n","Also worth noting is that the first time you run training on the dataset, it will run pre-processing and save that processed data in the same directory as the dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2vXnZ1oL9zIF"},"outputs":[],"source":["# 2. For BERT training on SST-2:\n","!tao text_classification train \\\n","    -e $SPECS_DIR/train.yaml \\\n","    -g 1  \\\n","    -k $KEY \\\n","    -r $RESULTS_DIR/train \\\n","    training_ds.file_path=$DATA_DIR/sst2/train.tsv \\\n","    validation_ds.file_path=$DATA_DIR/sst2/dev.tsv \\\n","    model.class_labels.class_labels_file=$DATA_DIR/sst2/label_ids.csv \\\n","    trainer.max_epochs=1"]},{"cell_type":"markdown","metadata":{"id":"JrGplDPP9zIG"},"source":["The train command produces a .tlt file called `trained-model.tlt` saved at `$RESULTS_DIR/train/checkpoints/trained-model.tlt`. This file can be fed directly into the fine-tuning stage as we see in the next block."]},{"cell_type":"markdown","metadata":{"id":"1ItkbJB49zIG"},"source":["#### Other tips and tricks:\n","- To accelerate the training without loss of quality, it is possible to train with these parameters:  `trainer.amp_level=\"O1\"` and `trainer.precision=16` for reduced precision.\n","- The batch size (`training_ds.batch_size`) may influence the validation accuracy. Larger batch sizes are faster to train with, however, you may get slightly better results with smaller batches."]},{"cell_type":"markdown","metadata":{"id":"Zlg9uGYx9zIG"},"source":["<a id='ft'></a>\n","### Fine-Tuning\n","\n","The command for fine-tuning is very similar to that of training. Instead of `tao text_classification train`, we use `tao text_classification finetune` instead. We also specify the spec file corresponding to fine-tuning. All commands in TAO follow a similar pattern, streamlining the workflow even further!\n","\n","`Note:` we use SST-2 dataset here to showcase how to use the fine-tuning command, however we recommend bringing your own data for fine-tuning on the pre-trained models. You would need to make sure your dataset is downloaded and pre-processed so that it's ready for use.\n","\n","`Note:` If you wish to proceed with a trained dataset for better inference results, you can find a .nemo model [here](\n","https://ngc.nvidia.com/catalog/collections/nvidia:nemotrainingframework).\n","\n","Simply re-name the .nemo file to .tlt and pass it through the finetune pipeline."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xpv4C-ZG9zIG"},"outputs":[],"source":["# 3. For BERT finetuning on SST-2:\n","!tao text_classification finetune \\\n","    -e $SPECS_DIR/finetune.yaml \\\n","    -g 1 \\\n","    -m $RESULTS_DIR/train/checkpoints/trained-model.tlt \\\n","    -k $KEY \\\n","    -r $RESULTS_DIR/finetune \\\n","    finetuning_ds.file_path=$DATA_DIR/sst2/train.tsv \\\n","    validation_ds.file_path=$DATA_DIR/sst2/dev.tsv \\\n","    trainer.max_epochs=1"]},{"cell_type":"markdown","metadata":{"id":"vpEhKKAV9zIG"},"source":["This command will generate a fine-tuned model `finetuned-model.tlt` at `$RESULTS_DIR/finetune/checkpoints`"]},{"cell_type":"markdown","metadata":{"id":"emq5ipiv9zIG"},"source":["<a id='evaluation'></a>\n","### Evaluation\n","The evaluation spec .yaml is as simple as:\n","\n","```\n","test_ds:\n","  file: ??? # e.g. $DATA_DIR/test.tsv\n","  batch_size: 32\n","  shuffle: false\n","  num_samples: 500\n","```\n","\n","Below, we use `tao text_classification evaluate` and override the test data configuration by specifying `test_ds.file_path`. Other arguments follow the same pattern as before!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rhUMUuxa9zIG"},"outputs":[],"source":["# 4. For BERT evaluation on SST-2:\n","!tao text_classification evaluate \\\n","    -e $SPECS_DIR/evaluate.yaml \\\n","    -g 1 \\\n","    -m $RESULTS_DIR/train/checkpoints/trained-model.tlt \\\n","    -k $KEY \\\n","    -r $RESULTS_DIR/evaluate \\\n","    test_ds.file_path=$DATA_DIR/SST-2/dev.tsv \\\n","    test_ds.batch_size=32"]},{"cell_type":"markdown","metadata":{"id":"VMntSiEl9zIG"},"source":["On evaluating the model you will get some results, and based on that you can either retrain the model for more epochs, or continue with the inference."]},{"cell_type":"markdown","metadata":{"id":"azZj9v5G9zIG"},"source":["<a id='inference'></a>\n","### Inference\n","Inference using a .tlt trained or fine-tuned model uses the `tao text_classification infer` command.  <br>\n","The infer.yaml is also straightforward, which includes some \"simulated\" user input, here we start with a batch of four samples. \n","\n","- \"by the end of no such thing the audience , like beatrice , has a watchful affection for the monster .\"\n","- \"director rob marshall went out gunning to make a great one .\"\n","- \"uneasy mishmash of styles and genres .\"\n","- \"I love exotic science fiction / fantasy movies but this one was very unpleasant to watch . Suggestions and images of child abuse , mutilated bodies (live or dead) , other gruesome scenes , plot holes , boring acting made this a regrettable experience , The basic idea of entering another person's mind is not even new to the movies or TV (An Outer Limits episode was better at exploring this idea) . i gave it 4 / 10 since some special effects were nice .\"\n","\n","We encourage you to try out custom inputs as an exercise."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"liw0h2i99zIG"},"outputs":[],"source":["# 5. For BERT inference on user data:\n","!tao text_classification infer \\\n","    -e $SPECS_DIR/infer.yaml \\\n","    -g 1 \\\n","    -m $RESULTS_DIR/finetune/checkpoints/finetuned-model.tlt \\\n","    -k $KEY \\\n","    -r $RESULTS_DIR/infer"]},{"cell_type":"markdown","metadata":{"id":"vt-ionX-9zIG"},"source":["### What's Next?\n","\n","You can use TAO to build custom models for your own applications, or deploy the custom models to NVIDIA Riva."]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"text-classification-training.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.10.2 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.2"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":0}
