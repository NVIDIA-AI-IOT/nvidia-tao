{"cells":[{"cell_type":"markdown","metadata":{"id":"LQAzse2fUf3L"},"source":["# Skeleton-based action recognition using TAO PoseClassificationNet\n","\n","Transfer learning is the process of transferring learned features from one application to another. It is a commonly used training technique where you use a model trained on one task and re-train to use it on a different task. \n","\n","Train Adapt Optimize (TAO) Toolkit  is a simple and easy-to-use Python based AI toolkit for taking purpose-built AI models and customizing them with users' own data.\n","\n","<img align=\"center\" src=\"https://developer.nvidia.com/sites/default/files/akamai/embedded-transfer-learning-toolkit-software-stack-1200x670px.png\" width=\"1080\">\n"]},{"cell_type":"markdown","metadata":{"id":"HSoaMw2CUf3O"},"source":["## Learning Objectives\n","\n","In this notebook, you will learn how to leverage the simplicity and convenience of TAO to:\n","\n","* Train a model for skeleton-based action recognition on the [Kinetics](https://deepmind.com/research/open-source/kinetics) dataset.\n","* Evaluate the trained model.\n","* Run Inference on the trained model.\n","* Export the trained model to a .etlt file (encrypted ONNX model) for deployment to DeepStream or TensorRT.\n","* Convert the pose data from [deepstream-bodypose-3d](https://github.com/NVIDIA-AI-IOT/deepstream_reference_apps/tree/master/deepstream-bodypose-3d) to skeleton arrays for inference.\n","\n","## Table of Contents\n","\n","This notebook shows an example usecase of PoseClassificationNet using Train Adapt Optimize (TAO) Toolkit.\n","\n","1. [Set up env variables and map drives](#head-1)\n","2. [Prepare dataset and pre-trained model](#head-2)\n","3. [Provide training specification](#head-3)\n","4. [Run TAO training](#head-4)\n","5. [Evaluate trained models](#head-5)\n","6. [Inferences](#head-6)\n","8. [Convert pose data](#head-8)\n"]},{"cell_type":"markdown","metadata":{"id":"Sh1xXFCRVVux"},"source":["## Connect to a GPU Runtime\n","\n","1.   Change Runtime type to GPU by Runtime(Top Left tab)->Change Runtime Type->GPU(Hardware Accelerator)\n","2.   Then click on Connect (Top Right)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"LkSKFwimVcRq"},"source":["## Mounting Google drive\n","Mount your Google drive storage to this Colab instance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cfT-erWsVbnv"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"10rZQPhUViHV"},"source":["##Setup Python Environment\n","Setup the environment necessary to run the TAO Networks by running the bash script"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b_B7AJjjVi6K"},"outputs":[],"source":["!sh /content/drive/MyDrive/ColabNotebooks/pytorch/setup_env.sh"]},{"cell_type":"markdown","metadata":{"id":"JIFZYUctUf3P"},"source":["## 1. Set up env variables and map drives <a class=\"anchor\" id=\"head-1\"></a>\n","\n","When using the purpose-built pretrained models from NGC, please make sure to set the `$KEY` environment variable to the key as mentioned in the model overview. Failing to do so, can lead to errors when trying to load them as pretrained models.\n","\n","The TAO launcher uses docker containers under the hood, and **for our data and results directory to be visible to the docker, they need to be mapped**. The launcher can be configured using the config file `~/.tao_mounts.json`. Apart from the mounts, you can also configure additional options like the Environment Variables and amount of Shared Memory available to the TAO launcher. <br>\n","\n","`IMPORTANT NOTE:` The code below creates a sample `~/.tao_mounts.json`  file. Here, we can map directories in which we save the data, specs, results and cache. You should configure it for your specific case so these directories are correctly visible to the docker container.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BAwnNda7Uf3Q"},"outputs":[],"source":["%env DATA_DIR=/content/data\n","# note: You could set the SPECS_DIR to folder of the experiments specs downloaded with the notebook\n","%env LOCAL_PROJECT_DIR=/content\n","%env SPECS_DIR=/content/drive/MyDrive/ColabNotebooks/pytorch/cv_notebooks/pose_classification_net/specs\n","%env RESULTS_DIR=/content/results\n","\n","# Set your encryption key, and use the same key for all commands\n","%env KEY = nvidia_tao"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kSwi-SCjUf3R"},"outputs":[],"source":["! mkdir -p $DATA_DIR\n","! mkdir -p $SPECS_DIR\n","! mkdir -p $RESULTS_DIR"]},{"cell_type":"markdown","metadata":{"id":"Y7MNbPXqUf3T"},"source":["## 2. Prepare dataset and pre-trained model <a class=\"anchor\" id=\"head-2\"></a>\n","We will be using the NVIDIA dataset generated by [deepstream-bodypose-3d](https://github.com/NVIDIA-AI-IOT/deepstream_reference_apps/tree/master/deepstream-bodypose-3d). \n","\n","Download the NVIDIA dataset and extract the files."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2MYEKGGYUf3V"},"outputs":[],"source":["# Download the dataset\n","!pip3 install gdown\n","!gdown https://drive.google.com/uc?id=1GhSt53-7MlFfauEZ2YkuzOaZVNIGo_c- -O $DATA_DIR/data_3dbp_nvidia.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F-i3GWa_Uf3V"},"outputs":[],"source":["# Extract the files\n","!mkdir -p $DATA_DIR/nvidia\n","!unzip $DATA_DIR/data_3dbp_nvidia.zip -d $DATA_DIR/nvidia\n","!rm $DATA_DIR/data_3dbp_nvidia.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wOqCwhn8Uf3V"},"outputs":[],"source":["# Verify\n","!ls -l $DATA_DIR/nvidia"]},{"cell_type":"markdown","metadata":{"id":"pvORUJDvUf3U"},"source":[" We also provide scripts to process the [Kinetics](https://deepmind.com/research/open-source/kinetics) dataset for the tutorial. Download the pre-processed data of Kinetics-Skeleton [here](https://drive.google.com/uc?id=1dmzCRQsFXJ18BlXj1G9sbDnsclXIdDdR) and extract them first:\n"," The following cells for processing the Kinetics dataset is `Optional`.\n"," Kinetics is the bigger dataset when compared to the NVIDIA dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z4Lp2EclUf3T"},"outputs":[],"source":["# # download the dataset.\n","# !pip3 install gdown\n","# !gdown https://drive.google.com/uc?id=1dmzCRQsFXJ18BlXj1G9sbDnsclXIdDdR -O $DATA_DIR/st-gcn-processed-data.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_TIvTTY_Uf3T","tags":[]},"outputs":[],"source":["# # extract the files\n","# !unzip $DATA_DIR/st-gcn-processed-data.zip -d $DATA_DIR\n","# !mv $DATA_DIR/data/Kinetics/kinetics-skeleton $DATA_DIR/kinetics\n","# !rm -r $DATA_DIR/data\n","# !rm $DATA_DIR/st-gcn-processed-data.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qzYAYmuEUf3U","tags":[]},"outputs":[],"source":["# # verify\n","# !ls -l $DATA_DIR/kinetics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9cW1BM0QUf3U"},"outputs":[],"source":["# # select actions\n","# import os\n","# import pickle\n","# import numpy as np\n","\n","# data_dir = os.path.join(os.environ[\"DATA_DIR\"], \"kinetics\")\n","\n","# # front_raises: 134\n","# # pull_ups: 255\n","# # clean_and_jerk: 59\n","# # presenting_weather_forecast: 254\n","# # deadlifting: 88\n","# selected_actions = {\n","#     134: 0,\n","#     255: 1,\n","#     59: 2,\n","#     254: 3,\n","#     88: 4\n","# }\n","\n","# def select_actions(selected_actions, data_dir, split_name):\n","#     \"\"\"Select a subset of actions and their corresponding labels.\n","    \n","#     Args:\n","#         selected_actions (dict): Map from selected class IDs to new class IDs.\n","#         data_dir (str): Path to the directory of data arrays (.npy) and labels (.pkl).\n","#         split_name (str): Name of the split to be processed, e.g., \"train\" and \"val\".\n","        \n","#     Returns:\n","#         No explicit returns\n","#     \"\"\"\n","#     data_path = os.path.join(data_dir, f\"{split_name}_data.npy\")\n","#     label_path = os.path.join(data_dir, f\"{split_name}_label.pkl\")\n","\n","#     data_array = np.load(file=data_path)\n","#     with open(label_path, \"rb\") as label_file:\n","#         labels = pickle.load(label_file)\n","\n","#     assert(len(labels) == 2)\n","#     assert(data_array.shape[0] == len(labels[0]))\n","#     assert(len(labels[0]) == len(labels[1]))\n","\n","#     print(f\"No. total samples for {split_name}: {data_array.shape[0]}\")\n","\n","#     selected_indices = []\n","#     for i in range(data_array.shape[0]):\n","#         if labels[1][i] in selected_actions.keys():\n","#             selected_indices.append(i)\n","\n","#     data_array = data_array[selected_indices, :, :, :, :]\n","#     selected_sample_names = [labels[0][x] for x in selected_indices]\n","#     selected_labels = [selected_actions[labels[1][x]] for x in selected_indices]\n","#     labels = (selected_sample_names, selected_labels)\n","\n","#     print(f\"No. selected samples for {split_name}: {data_array.shape[0]}\")\n","\n","#     np.save(file=data_path, arr=data_array, allow_pickle=False)\n","#     with open(label_path, \"wb\") as label_file:\n","#         pickle.dump(labels, label_file, protocol=4)\n","\n","# select_actions(selected_actions, data_dir, \"train\")\n","# select_actions(selected_actions, data_dir, \"val\")"]},{"cell_type":"markdown","metadata":{"id":"k1egc0VSUf3W"},"source":["`OPTIONAL:` Download the pretrained model from NGC. We will use NGC CLI to get the data and model. For more details, go to https://ngc.nvidia.com and click the SETUP on the navigation bar."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u6VxL22NUf3W"},"outputs":[],"source":["# Installing NGC CLI on the local machine.\n","## Download and install\n","import os\n","%env CLI=ngccli_cat_linux.zip\n","!mkdir -p $LOCAL_PROJECT_DIR/ngccli\n","\n","# Remove any previously existing CLI installations\n","!rm -rf $LOCAL_PROJECT_DIR/ngccli/*\n","!wget \"https://ngc.nvidia.com/downloads/$CLI\" -P $LOCAL_PROJECT_DIR/ngccli\n","!unzip -u \"$LOCAL_PROJECT_DIR/ngccli/$CLI\" -d $LOCAL_PROJECT_DIR/ngccli/\n","!rm $LOCAL_PROJECT_DIR/ngccli/*.zip \n","os.environ[\"PATH\"]=\"{}/ngccli/ngc-cli:{}\".format(os.getenv(\"LOCAL_PROJECT_DIR\", \"\"), os.getenv(\"PATH\", \"\"))\n","!cp /usr/lib/x86_64-linux-gnu/libstdc++.so.6 /content/ngccli/ngc-cli/libstdc++.so.6"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dxcEJruxUf3W"},"outputs":[],"source":["!ngc registry model list nvidia/tao/poseclassificationnet:*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y_Q5tpzOUf3X"},"outputs":[],"source":["!mkdir -p $RESULTS_DIR/pretrained"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VXA9oZfNUf3X"},"outputs":[],"source":["# Pull pretrained model from NGC \n","!ngc registry model download-version \"nvidia/tao/poseclassificationnet:trainable_v1.0\" --dest $RESULTS_DIR/pretrained"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZYyI1hIcUf3X"},"outputs":[],"source":["print(\"Check that model is downloaded into dir.\")\n","!ls -l $RESULTS_DIR/pretrained/poseclassificationnet_vtrainable_v1.0"]},{"cell_type":"markdown","metadata":{"id":"rkstsj5TUf3X"},"source":["## 3. Provide training specification <a class=\"anchor\" id=\"head-3\"></a>\n","\n","We provide specification files to configure the training parameters including:\n","\n","* model_config: configure the model setting\n","    * model_type: type of model, ST-GCN\n","    * in_channels: number of input channels\n","    * num_class: number of classes\n","    * dropout: probability to drop the hidden units\n","    * graph_layout: type of graph layout, nvidia/openpose/human3.6m/ntu-rgb+d/ntu_edge/coco\n","    * graph_strategy: type of graph strategy, uniform/distance/spatial\n","    * edge_importance_weighting: enabling edge importance weighting\n","* train_config: configure the training hyperparameters\n","    * optim\n","    * epochs\n","    * checkpoint_interval\n","    * grad_clip\n","* dataset_config: configure the dataset and augmentation methods\n","    * train_data_path\n","    * train_label_path\n","    * val_data_path\n","    * val_label_path\n","    * label_map\n","    * random_choose\n","    * random_move\n","    * window_size\n","    * batch_size\n","    * workers: number of workers to do data loading\n","\n","Please refer to the TAO documentation about PoseClassificationNet to get all the parameters that are configurable."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oCoNBhZfUf3Y","scrolled":true},"outputs":[],"source":["!cat $SPECS_DIR/train_nvidia.yaml"]},{"cell_type":"markdown","metadata":{"id":"ThwVRqmkUf3Y"},"source":["## 4. Run TAO training <a class=\"anchor\" id=\"head-4\"></a>\n","* Provide the sample spec file and the output directory location for models.\n","* WARNING: Training will take several hours or one day to complete."]},{"cell_type":"markdown","metadata":{"id":"i3jm5mhKUf3a"},"source":["### 4.1 Train NVIDIA model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nmiol3maUf3b"},"outputs":[],"source":["print(\"Train model from scratch\")\n","!tao pose_classification train \\\n","                  -e $SPECS_DIR/train_nvidia.yaml \\\n","                  -r $RESULTS_DIR/nvidia \\\n","                  -k $KEY"]},{"cell_type":"markdown","metadata":{"id":"zrgCqaSoUf3b"},"source":["We provide pre-trained ST-GCN model trained on the NVIDIA dataset. With the pre-trained model, we can even get better accuracy with less epochs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yYSed9QKUf3b"},"outputs":[],"source":["# print(\"To resume training from a checkpoint, set the resume_training_checkpoint_path option to be the .tlt you want to resume from\")\n","# print(\"remember to remove the `=` in the checkpoint's file name\")\n","# !tao pose_classification train \\\n","#                   -e $SPECS_DIR/train_nvidia.yaml \\\n","#                   -r $RESULTS_DIR/nvidia \\\n","#                   -k $KEY \\\n","#                   resume_training_checkpoint_path="]},{"cell_type":"code","execution_count":null,"metadata":{"id":"62Im6-dNUf3b"},"outputs":[],"source":["print('Encrypted checkpoints:')\n","print('---------------------')\n","!ls -ltrh $RESULTS_DIR/nvidia"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fZqXPVjZUf3c"},"outputs":[],"source":["print('Rename a model:')\n","print('---------------------')\n","# NOTE: The following command may require `sudo`. You can run the command outside the notebook.\n","!find $RESULTS_DIR/nvidia/ -name *epoch=69* | xargs realpath | xargs -I {} mv {} $RESULTS_DIR/nvidia/nvidia_model.tlt\n","!ls -ltrh $RESULTS_DIR/nvidia/nvidia_model.tlt"]},{"cell_type":"markdown","metadata":{"id":"3JVY5ltqUf3Y"},"source":["### `OPTIONAL` 4.2 Train Kinetics model"]},{"cell_type":"markdown","metadata":{"id":"VoWfcm33Uf3Y"},"source":["We will train a Kinetics model from scratch."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4fUSrMW4Uf3Z","tags":[]},"outputs":[],"source":["# print(\"Train model\")\n","# !tao pose_classification train \\\n","#                   -e $SPECS_DIR/train_kinetics.yaml \\\n","#                   -r $RESULTS_DIR/kinetics \\\n","#                   -k $KEY"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1wzb4CXUUf3Z"},"outputs":[],"source":["# print('Encrypted checkpoints:')\n","# print('---------------------')\n","# !ls -ltrh $RESULTS_DIR/kinetics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FSD8pBsmUf3a"},"outputs":[],"source":["# print('Rename a model:')\n","# print('---------------------')\n","# # NOTE: The following command may require `sudo`. You can run the command outside the notebook.\n","# !find $RESULTS_DIR/kinetics/ -name *epoch=49* | xargs realpath | xargs -I {} mv {} $RESULTS_DIR/kinetics/kinetics_model.tlt\n","# !ls -ltrh $RESULTS_DIR/kinetics/kinetics_model.tlt"]},{"cell_type":"markdown","metadata":{"id":"7RetOQy9Uf3c"},"source":["## 5. Evaluate trained models <a class=\"anchor\" id=\"head-5\"></a>\n","Evaluate trained model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lxzNI1p7Uf3c"},"outputs":[],"source":["!tao pose_classification evaluate \\\n","                    -e $SPECS_DIR/evaluate_nvidia.yaml \\\n","                    -k $KEY \\\n","                    model=$RESULTS_DIR/nvidia/nvidia_model.tlt \\\n","                    data=$DATA_DIR/nvidia/val_data.npy \\\n","                    label=$DATA_DIR/nvidia/val_label.pkl"]},{"cell_type":"markdown","metadata":{"id":"F24T_YjvUf3d"},"source":["## 6. Inferences <a class=\"anchor\" id=\"head-6\"></a>\n","In this section, we run the pose classification inference tool to generate inferences with the trained models and save the results under `$RESULTS_DIR`. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0eP8apH8Uf3d"},"outputs":[],"source":["!tao pose_classification inference \\\n","                    -e $SPECS_DIR/infer_nvidia.yaml \\\n","                    -k $KEY \\\n","                    model=$RESULTS_DIR/nvidia/nvidia_model.tlt \\\n","                    data=$DATA_DIR/nvidia/val_data.npy \\\n","                    output_file=$RESULTS_DIR/results.txt"]},{"cell_type":"markdown","metadata":{"id":"bGzoS7IUUf3e"},"source":["## `OPTIONAL` 8. Convert pose data <a class=\"anchor\" id=\"head-8\"></a>\n","Convert the JSON pose data from [deepstream-bodypose-3d](https://github.com/NVIDIA-AI-IOT/deepstream_reference_apps/tree/master/deepstream-bodypose-3d) to NumPy arrays for inference."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0wrWr2iMUf3e"},"outputs":[],"source":["# !mkdir -p $RESULTS_DIR/dataset_convert"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m8pYXg9vUf3e"},"outputs":[],"source":["# !tao pose_classification dataset_convert \\\n","#                    -e $SPECS_DIR/dataset_convert_nvidia.yaml \\\n","#                    -k $KEY \\\n","#                    data=/absolute/path/to/your/json/pose/data \\\n","#                    output_dir=$RESULTS_DIR/dataset_convert"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JT1ih0f_Uf3e"},"outputs":[],"source":["# print('Converted pose data:')\n","# print('------------')\n","# !ls -lth $RESULTS_DIR/dataset_convert"]},{"cell_type":"markdown","metadata":{"id":"T8hAelqjUf3f"},"source":["This notebook has come to an end."]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["3JVY5ltqUf3Y"],"name":"poseclassificationnet.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.10.2 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.2"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":0}
