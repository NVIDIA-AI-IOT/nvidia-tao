{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection using TAO Deformable Detr\n",
    "\n",
    "Transfer learning is the process of transferring learned features from one application to another. It is a commonly used training technique where you use a model trained on one task and re-train to use it on a different task. \n",
    "\n",
    "Train Adapt Optimize (TAO) Toolkit  is a simple and easy-to-use Python based AI toolkit for taking purpose-built AI models and customizing them with users' own data.\n",
    "\n",
    "<img align=\"center\" src=\"https://developer.nvidia.com/sites/default/files/akamai/embedded-transfer-learning-toolkit-software-stack-1200x670px.png\" width=\"1080\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "In this notebook, you will learn how to leverage the simplicity and convenience of TAO to:\n",
    "\n",
    "* Take a pretrained model and train an Deformable DetrNet model on COCO dataset\n",
    "* Evaluate the trained model\n",
    "* Run inference with the trained model and visualize the result\n",
    "* Export the trained model to a .etlt file for deployment to DeepStream\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "This notebook shows an example usecase of ActionRecognitionNet using Train Adapt Optimize (TAO) Toolkit.\n",
    "\n",
    "1. [Set up env variables and map drives](#head-1)\n",
    "2. [Prepare dataset](#head-2)\n",
    "3. [Provide training specification](#head-3)\n",
    "4. [Run TAO training](#head-4)\n",
    "5. [Evaluate a trained model](#head-5)\n",
    "6. [Visualize inferences](#head-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nn-fQiy7Ie8E"
   },
   "source": [
    "## Connect to a GPU Runtime\n",
    "\n",
    "1.   Change Runtime type to GPU by Runtime(Top Left tab)->Change Runtime Type->GPU(Hardware Accerlerator)\n",
    "2.   Then click on Connect (Top Right)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FddmPjlnIkZJ"
   },
   "source": [
    "## Mounting Google drive\n",
    "Mount your Google drive storage to this Colab instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xEAUh_LEIi7w"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T3lgWtn6IogV"
   },
   "source": [
    "## Setup Python Environment\n",
    "Setup the environment necessary to run the TAO Networks by running the bash script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YUwE-21vIuvN"
   },
   "outputs": [],
   "source": [
    "!sh /content/drive/MyDrive/ColabNotebooks/pytorch/setup_env_customops_networks.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up env variables and map drives <a class=\"anchor\" id=\"head-1\"></a>\n",
    "\n",
    "When using the purpose-built pretrained models from NGC, please make sure to set the `$KEY` environment variable to the key as mentioned in the model overview. Failing to do so, can lead to errors when trying to load them as pretrained models.\n",
    "\n",
    "The TAO launcher uses docker containers under the hood, and **for our data and results directory to be visible to the docker, they need to be mapped**. The launcher can be configured using the config file `~/.tao_mounts.json`. Apart from the mounts, you can also configure additional options like the Environment Variables and amount of Shared Memory available to the TAO launcher. <br>\n",
    "\n",
    "`IMPORTANT NOTE:` The code below creates a sample `~/.tao_mounts.json`  file. Here, we can map directories in which we save the data, specs, results and cache. You should configure it for your specific case so these directories are correctly visible to the docker container.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defaulted to volatile colab instance memory\n",
    "# depending on the available storage left between google drive and colab instance, choose the data_dir path\n",
    "%env DATA_DIR=/content/data\n",
    "\n",
    "%env LOCAL_PROJECT_DIR=/content\n",
    "\n",
    "# note: You could set the SPECS_DIR to folder of the experiments specs downloaded with the notebook\n",
    "%env SPECS_DIR=/content/drive/MyDrive/ColabNotebooks/pytorch/cv_notebooks/deformable_detr/specs\n",
    "%env RESULTS_DIR=/content/results\n",
    "\n",
    "# Set your encryption key, and use the same key for all commands\n",
    "%env KEY = nvidia_tao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p $DATA_DIR\n",
    "! mkdir -p $SPECS_DIR\n",
    "! mkdir -p $RESULTS_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare dataset <a class=\"anchor\" id=\"head-2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We will be using the COCO dataset for the tutorial. The following script will download COCO dataset automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create local dir\n",
    "!mkdir -p $DATA_DIR\n",
    "# Download the data\n",
    "!bash $SPECS_DIR/download_coco.sh $DATA_DIR/coco2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "!ls -l $DATA_DIR/coco2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Provide training specification <a class=\"anchor\" id=\"head-3\"></a>\n",
    "\n",
    "We provide specification files to configure the training parameters including:\n",
    "\n",
    "* experiment config: configure the train experiments setting\n",
    "    * num_gpus: number of gpus \n",
    "    * num_nodes: number of nodes (num_nodes=1 for single node)\n",
    "    * val_interval: validation interval\n",
    "* dataset_config: configure the dataset and augmentation methods\n",
    "    * train_data_sources:\n",
    "        * image_dir: annoation file for train data. required to be in COCO json format\n",
    "        * json_file: the root directory for train images\n",
    "    * val_data_sources: \n",
    "        * image_dir: the root directory for validation images\n",
    "        * json_file: annoation file for validation data. required to be in COCO json format\n",
    "    * num_classes: number of classes of you training data\n",
    "    * batch_size: batch size for dataloader\n",
    "    * workers: number of workers to do data loading\n",
    "* model_config: configure the model setting\n",
    "    * pretrained_backbone_path: path to the pretrained backbone model. Only ResNet50 backbone is supported\n",
    "    * num_feature_levels: number of feature levels used from backbone\n",
    "    * dec_layers: number of decoder layers\n",
    "    * enc_layers: number of encoder layers\n",
    "    * num_queries: number of queries for the model\n",
    "    * with_box_refine: flag to enable bbox refinement\n",
    "    * dropout_ratio: drop out ratio\n",
    "* train_config: configure the training hyperparameters\n",
    "    * optim_config:\n",
    "        * lr_backbone: learning rate for backbone\n",
    "        * lr: learning rate for the rest of the model\n",
    "        * lr_steps: learning rate decay step milestone (MultiStep)\n",
    "    * epochs\n",
    "\n",
    "* **Note that the sample spec is not meant to produce SOTA accuracy on COCO. To reproduce SOTA, you might want to use TAO to train an ImageNet model first and follow the original parameters for COCO.**\n",
    "\n",
    "Please refer to the TAO documentation about Deformable Detr to get all the parameters that are configurable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cat $SPECS_DIR/train.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run TAO training <a class=\"anchor\" id=\"head-4\"></a>\n",
    "* Provide the sample spec file and the output directory location for models\n",
    "* Evaluation uses COCO metrics. For more info, please refer to: https://cocodataset.org/#detection-eval\n",
    "* WARNING: training will take several hours or one day to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"For multi-GPU, change num_gpus in train.yaml based on your machine.\")\n",
    "print(\"For multi-node, change num_gpus and num_nodes in train.yaml based on your machine.\")\n",
    "!tao deformable_detr train \\\n",
    "                  -e $SPECS_DIR/train.yaml \\\n",
    "                  -r $RESULTS_DIR/train/ \\\n",
    "                  -k $KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Encrypted checkpoints:')\n",
    "print('---------------------')\n",
    "!ls -ltrh $RESULTS_DIR/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: The following command require `sudo`. You can run the command outside the notebook. Changed the tlt file name based on train results.\n",
    "print('Rename a trained model: ')\n",
    "print('---------------------')\n",
    "!!echo <passwd> | sudo -S mv $RESULTS_DIR/train/dd_model_epoch=50-val_loss=XXX.tlt $RESULTS_DIR/train/dd_model.tlt \n",
    "!ls -ltrh $RESULTS_DIR/train/dd_model.tlt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate a trained model <a class=\"anchor\" id=\"head-5\"></a>\n",
    "\n",
    "In this section, we run the `evaluate` tool to evaluate the trained model and produce the mAP metric.\n",
    "\n",
    "We provide evaluate.yaml specification files to configure the evaluate parameters including:\n",
    "* exeperiment config\n",
    "    * num_gpus: number of gpus\n",
    "    * conf_threshold: a threshold for confidence scores\n",
    "* model_config: configure the model setting\n",
    "    * this config should remain same as your trained model's configuration.\n",
    "* dataset_config: configure the dataset and augmentation methods\n",
    "    * test_data_sources:\n",
    "        * image_dir: annoation file for evaluatation data. required to be in COCO json format.\n",
    "        * json_file: the root directory for evaluatation images    \n",
    "    * num_classes: number of classes you used for training\n",
    "    * eval_class_ids: classes you would like to evaluate. \\\n",
    "                    Note that current config file will evaluate only on class 1 (person in COCO dataset)\\\n",
    "                    If you remove this from config file, it will evaluate and compute the average over entire classes.\n",
    "    * batch_size\n",
    "    * workers\n",
    "* **NOTE: You need to change the model path in evaluate.yaml file based on your setting.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on tlt model\n",
    "!tao deformable_detr evaluate \\\n",
    "                    -e $SPECS_DIR/evaluate_tlt.yaml \\\n",
    "                    -k $KEY \\\n",
    "                    model_path=$RESULTS_DIR/train/dd_model.tlt \\\n",
    "                    output_dir=$RESULTS_DIR/evaluate/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Inferences <a class=\"anchor\" id=\"head-6\"></a>\n",
    "In this section, we run the `inference` tool to generate inferences on the trained models and visualize the results. The `inference` tool produces annotated image outputs and txt files that contain prediction information.\n",
    "\n",
    "We provide evaluate.yaml specification files to configure the evaluate parameters including:\n",
    "\n",
    "* exeperiment config\n",
    "    * conf_threshold: the confidence score threshold\n",
    "    * color_map: the color mapping for each class. The predicted bbox will be drawn with mapped color for each class\n",
    "* model_config: configure the model setting\n",
    "    * this config should remain same as your trained model's configuration\n",
    "* dataset_config: configure the dataset and augmentation methods\n",
    "    * infer_data_sources:\n",
    "        * image_dir: annoation file for inference data. Required to be in COCO json format\n",
    "        * json_file: the root directory for inference images\n",
    "    * num_classes: number of classes you used for training\n",
    "    * batch_size\n",
    "    * workers\n",
    "* **NOTE: You need to change the model path in evaluate.yaml file based on your setting.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao deformable_detr inference \\\n",
    "                    -e $SPECS_DIR/infer_tlt.yaml \\\n",
    "                    -k $KEY \\\n",
    "                    model_path=$RESULTS_DIR/train/dd_model.tlt \\\n",
    "                    output_dir=$RESULTS_DIR/infer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook has come to an end. You may continue by deploying this model to [DeepStream](https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_3D_Action.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
