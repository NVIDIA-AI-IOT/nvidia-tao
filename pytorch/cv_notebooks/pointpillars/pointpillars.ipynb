{"cells":[{"cell_type":"markdown","metadata":{"id":"SRNbz5hlReOz"},"source":["# 3D LIDAR Object Detection using TAO PointPillars\n","\n","Transfer learning is the process of transferring learned features from one application to another. It is a commonly used training technique where you use a model trained on one task and re-train to use it on a different task. \n","\n","Train Adapt Optimize (TAO) Toolkit  is a simple and easy-to-use Python based AI toolkit for taking purpose-built AI models and customizing them with users' own data.\n","\n","<img align=\"center\" src=\"https://developer.nvidia.com/sites/default/files/akamai/embedded-transfer-learning-toolkit-software-stack-1200x670px.png\" width=\"1080\"> "]},{"cell_type":"markdown","metadata":{"id":"3vvbQOV0ReO2"},"source":[" ## Learning Objectives\n","In this notebook, you will learn how to leverage the simplicity and convenience of TAO to:\n","\n","* Train a PointPillars model on the KITTI dataset\n","* Prune the trained model\n","* Retrain the pruned model to recover lost accuracy\n","* Run evaluation & inference on the trained model to verify the accuracy\n","* Export & deploy the model in TensorRT\n","\n"," ### Table of Contents\n","\n"," This notebook shows an example use case of PointPillars using Train Adapt Optimize (TAO) Toolkit.\n","\n"," 1. [Set up env variables and map drives](#head-1)\n"," 2. [Prepare dataset and pretrained model](#head-2)<br>\n","     2.1 [Download the dataset](#head-2-1)<br>\n","     2.2 [Verify the downloaded dataset](#head-2-2)<br>\n","     2.3 [Convert dataset to required format](#head-2-3)<br>\n"," 3. [Provide training specification](#head-3)\n"," 4. [Run TAO training](#head-4)\n"," 5. [Evaluate trained models](#head-5)\n"," 6. [Prune trained models](#head-6)\n"," 7. [Retrain pruned models](#head-7)\n"," 8. [Evaluate retrained model](#head-8)\n"," 9. [Visualize inferences](#head-9)\n","\n"," #### Note\n","1. This notebook uses KITTI dataset by default, which should be around ~35 GB. If you are limited by Google-Drive storage, we recommend to:\n","\n","    i. Download the dataset onto the local system\n","\n","    ii. Run the utility script at $COLAB_NOTEBOOKS/pytorch/util/obtain_subset.py in your local system\n","\n","    iii. This generates a subset of kitti dataset with number of sample images you wish for\n","\n","    iv. Upload this subset onto Google Drive\n","\n","1. Using the default config/spec file provided in this notebook, each weight file size of pointpillars created during training will be ~168 MB\n"]},{"cell_type":"markdown","metadata":{"id":"ntM85vWETJUG"},"source":["## Connect to a GPU Runtime\n","\n","1.   Change Runtime type to GPU by Runtime(Top Left tab)->Change Runtime Type->GPU(Hardware Accelerator)\n","2.   Then click on Connect (Top Right)\n"]},{"cell_type":"markdown","metadata":{"id":"LszwKdunTE5r"},"source":["## Mounting Google drive\n","Mount your Google drive storage to this Colab instance"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15527,"status":"ok","timestamp":1658443997787,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"FIazk70ETBDN","outputId":"45ba343c-44ce-44b7-9f26-0616cf6751fd"},"outputs":[],"source":["try:\n","    import google.colab\n","    %env GOOGLE_COLAB=1\n","    from google.colab import drive\n","    drive.mount('/content/drive', force_remount=True)\n","except:\n","    %env GOOGLE_COLAB=0\n","    print(\"Warning: Not a Colab Environment\")"]},{"cell_type":"markdown","metadata":{"id":"9hCB4MPuTNCK"},"source":["##Setup Python Environment\n","Setup the environment necessary to run the TAO Networks by running the bash script"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9YPAOXEuTMcc"},"outputs":[],"source":["import os\n","#FIXME\n","%env GENERAL_WHL_PATH=/content/drive/MyDrive/pyt/general_whl\n","#FIXME\n","%env CODEBASE_WHL_PATH=/content/drive/MyDrive/pyt/codebase_whl\n","#FIXME\n","%env COLAB_NOTEBOOKS_PATH=/content/drive/MyDrive/ColabNotebooks\n","if not os.path.exists(os.environ[\"COLAB_NOTEBOOKS_PATH\"]):\n","    raise(\"Error, enter the path of the colab notebooks repo correctly\")\n","\n","if os.path.exists(os.environ[\"GENERAL_WHL_PATH\"]) and os.path.exists(os.environ[\"GENERAL_WHL_PATH\"]):\n","    if os.environ[\"GOOGLE_COLAB\"] == \"1\":\n","        os.environ[\"bash_script\"] = \"setup_env.sh\"\n","    else:\n","        os.environ[\"bash_script\"] = \"setup_env_desktop.sh\"\n","\n","    !sed -i \"s|PATH_TO_GENERAL_WHL|$GENERAL_WHL_PATH|g\" $COLAB_NOTEBOOKS_PATH/pytorch/$bash_script\n","    !sed -i \"s|PATH_TO_CODEBASE_WHL|$CODEBASE_WHL_PATH|g\" $COLAB_NOTEBOOKS_PATH/pytorch/$bash_script\n","    !sed -i \"s|PATH_TO_COLAB_NOTEBOOKS|$COLAB_NOTEBOOKS_PATH|g\" $COLAB_NOTEBOOKS_PATH/pytorch/$bash_script\n","\n","    !sh $COLAB_NOTEBOOKS_PATH/pytorch/$bash_script\n","else:\n","    raise(\"Error, enter the whl paths correctly\")"]},{"cell_type":"markdown","metadata":{"id":"29UEiTrmReO2"},"source":[" ## 1. Set up env variables <a class=\"anchor\" id=\"head-1\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1180,"status":"ok","timestamp":1658444024332,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"e4mZmvPNReO3","outputId":"6e6bbc58-f0cc-4ff8-fd9e-3b541e864993"},"outputs":[],"source":["# Setting up env variables for cleaner command line commands.\n","import os\n","\n","%env TAO_DOCKER_DISABLE=1\n","\n","print(\"Please replace the variables with your own.\")\n","%env KEY=tlt_encode\n","%env EXPERIMENT_DIR=/content/drive/MyDrive/results/pointpillars\n","!sudo mkdir -p $EXPERIMENT_DIR && sudo chmod -R 777 $EXPERIMENT_DIR\n","\n","# defaulted to volatile colab instance memory\n","# depending on the available storage left between google drive and colab instance, choose the data_dir path\n","# %env DATA_DIR=/content/data\n","%env DATA_DIR=/content/kitti_data/\n","!sudo mkdir -p $DATA_DIR && sudo chmod -R 777 $DATA_DIR\n","\n","SPECS_DIR=f\"{os.environ['COLAB_NOTEBOOKS_PATH']}/pytorch/cv_notebooks/pointpillars/specs\"\n","%env SPECS_DIR={SPECS_DIR}\n","\n","# Showing list of specification files.\n","!ls -rlt $SPECS_DIR"]},{"cell_type":"markdown","metadata":{"id":"f6mppqLGReO6"},"source":[" ## 2. Prepare dataset and pretrained model <a class=\"anchor\" id=\"head-2\"></a>"]},{"cell_type":"markdown","metadata":{"id":"BdO9HZD5ReO6"},"source":[" We will be using the KITTI detection dataset for the tutorial. To find more details please visit\n"," http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=2d. Please download the KITTI detection images (http://www.cvlibs.net/download.php?file=data_object_image_2.zip), labels(http://www.cvlibs.net/download.php?file=data_object_label_2.zip), velodyne LIDAR pointcloud(http://www.cvlibs.net/download.php?file=data_object_velodyne.zip) and LIDAR calibration file(http://www.cvlibs.net/download.php?file=data_object_calib.zip) to $DATA_DIR.\n"," \n","The entire kitti dataset for lidar is about 40GB. If you don't have unlimited google storage or colab pro, then use the script at ColabNotebooks/pytorch/util/obtain_subset.py on your local desktop machine and upload the subset created onto your drive/colab instance storage\n","\n","The data will then be extracted to have below structure.\n","\n","```bash\n","│── ImageSets\n","│── training\n","│   ├──calib & velodyne & label_2 & image_2\n","│── testing\n","│   ├──calib & velodyne & image_2\n","```\n","\n","The `testing` directory will not be used in this notebook as it has no labels. For the `training` dataset, we will have some script to do data preprocessing and split it into `train` and `val` splits. Finally the directory seen by TAO PointPillars should look like below.\n","\n","```bash\n","│── train\n","│   ├──lidar & label\n","│── val\n","│   ├──lidar & label\n","```\n","\n","You may use this notebook with your own dataset as well. To use this example with your own dataset, please follow the same directory structure as mentioned below."]},{"cell_type":"markdown","metadata":{"id":"63F3qazUReO7"},"source":["### 2.1 Download the dataset <a class=\"anchor\" id=\"head-2-1\"></a>"]},{"cell_type":"markdown","metadata":{"id":"R17MJ940ReO7"},"source":["Once you have gotten the download links in your email, please populate them in place of the `KITTI_IMAGES_DOWNLOAD_URL`,  `KITTI_LABELS_DOWNLOAD_URL`, `KITTI_LIDAR_DOWNLOAD_DIR` and `KITTI_CALIB_DOWNLOAD_DIR`. This next cell, will download the data and place in `$DATA_DIR`"]},{"cell_type":"markdown","metadata":{"id":"wIQsJO0aReO7"},"source":["Note that images are only required for KITTI dataset in this notebook, but not required for a general dataset that follows TAO PointPillars standard format. The reason that we need images in KITTI dataset is KITTI dataset does not conform with the standard format and some pre-processing are necessary for it. The preprocessing will read each image's size and retrieve only points that are in field-of-view(FOV) of camera from the original LiDAR files. The retrieved FOV-only points will be saved to new LiDAR file for each of the original LiDAR file. This is necessary as KITTI dataset has only labels in the FOV of camera, but no labels for points outside of camera FOV."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8yRUUiyOReO7"},"outputs":[],"source":["import os\n","\n","os.environ[\"URL_IMAGES\"]=KITTI_IMAGES_DOWNLOAD_URL\n","!if [ ! -f $DATA_DIR/data_object_image_2.zip ]; then wget $URL_IMAGES -O $DATA_DIR/data_object_image_2.zip; else echo \"image archive already downloaded\"; fi \n","\n","os.environ[\"URL_LABELS\"]=KITTI_LABELS_DOWNLOAD_URL\n","!if [ ! -f $DATA_DIR/data_object_label_2.zip ]; then wget $URL_LABELS -O $DATA_DIR/data_object_label_2.zip; else echo \"label archive already downloaded\"; fi\n","\n","os.environ[\"URL_LIDAR\"]=KITTI_LIDAR_DOWNLOAD_URL\n","!if [ ! -f $DATA_DIR/data_object_velodyne.zip ]; then wget $URL_LIDAR -O $DATA_DIR/data_object_velodyne.zip; else echo \"velodyne archive already downloaded\"; fi \n","\n","os.environ[\"URL_CALIB\"]=CALIB_DOWNLOAD_URL\n","!if [ ! -f $DATA_DIR/data_object_calib.zip ]; then wget $URL_CALIB -O $DATA_DIR/data_object_calib.zip; else echo \"calib archive already downloaded\"; fi "]},{"cell_type":"markdown","metadata":{"id":"BxkN9MgpReO8"},"source":["### 2.2 Verify the downloaded dataset <a class=\"anchor\" id=\"head-2-2\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v3MkqxtkReO8"},"outputs":[],"source":["# Check the dataset is present\n","!if [ ! -f $DATA_DIR/data_object_image_2.zip ]; then echo 'Image zip file not found, please download.'; else echo 'Found Image zip file.';fi\n","!if [ ! -f $DATA_DIR/data_object_label_2.zip ]; then echo 'Label zip file not found, please download.'; else echo 'Found Labels zip file.';fi\n","!if [ ! -f $DATA_DIR/data_object_velodyne.zip ]; then echo 'Velodyne zip file not found, please download.'; else echo 'Found Velodyne zip file.';fi\n","!if [ ! -f $DATA_DIR/data_object_calib.zip ]; then echo 'Calib zip file not found, please download.'; else echo 'Found Calib zip file.';fi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bidj_R-TReO8"},"outputs":[],"source":["# unpack \n","!unzip -u $DATA_DIR/data_object_image_2.zip -d $DATA_DIR\n","!unzip -u $DATA_DIR/data_object_label_2.zip -d $DATA_DIR\n","!unzip -u $DATA_DIR/data_object_velodyne.zip -d $DATA_DIR\n","!unzip -u $DATA_DIR/data_object_calib.zip -d $DATA_DIR"]},{"cell_type":"markdown","metadata":{"id":"NWshonbeReO8"},"source":["### 2.3 Convert dataset to required format<a class=\"anchor\" id=\"head-2-3\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Generate a subset of data if you have less storage space\n","import os\n","os.environ[\"subset_data_path\"] = os.environ[\"DATA_DIR\"] + \"/subset\"\n","\n","!sudo rm -rf $subset_data_path\n","\n","!python3 $COLAB_NOTEBOOKS_PATH/pytorch/util/obtain_subset.py --source-data-dir=$DATA_DIR/training --out-data-dir=$DATA_DIR/subset/training/ --training True --num-images=1000\n","!python3 $COLAB_NOTEBOOKS_PATH/pytorch/util/obtain_subset.py --source-data-dir=$DATA_DIR/testing --out-data-dir=$DATA_DIR/subset/testing/ --num-images=1000\n","\n","!ls -rlt $DATA_DIR/subset/training\n","!ls -rlt $DATA_DIR/subset/testing\n","\n","os.environ[\"DATA_DIR\"] = os.environ[\"subset_data_path\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rTPJ0vKyReO8"},"outputs":[],"source":["# Create output directories\n","!mkdir -p $DATA_DIR/train/lidar\n","!mkdir -p $DATA_DIR/train/label\n","!mkdir -p $DATA_DIR/val/lidar\n","!mkdir -p $DATA_DIR/val/label"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1633,"status":"ok","timestamp":1658444036890,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"Rsq4puYsReO9","outputId":"e2563113-f41c-4bc3-f9f3-7d8305747420"},"outputs":[],"source":["# Retrieve FOV-only LIDAR points from 360-degree LIDAR points\n","# Since only FOV data is labelled in KITTI dataset\n","!python $SPECS_DIR/gen_lidar_points.py -p $DATA_DIR/training/velodyne \\\n","                                                           -c $DATA_DIR/training/calib    \\\n","                                                           -i $DATA_DIR/training/image_2  \\\n","                                                           -o $DATA_DIR/train/lidar"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6G3o1zPRReO9"},"outputs":[],"source":["# Convert labels from Camera coordinate system to LIDAR coordinate system, etc\n","!python $SPECS_DIR/gen_lidar_labels.py -l $DATA_DIR/training/label_2 \\\n","                                                           -c $DATA_DIR/training/calib \\\n","                                                           -o $DATA_DIR/train/label"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N_gkJ1EUReO9"},"outputs":[],"source":["# Drop DontCare class\n","!python $SPECS_DIR/drop_class.py $DATA_DIR/train/label DontCare"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NrnsipkEReO9"},"outputs":[],"source":["# train/val split\n","!python $SPECS_DIR/kitti_split.py $SPECS_DIR/val.txt \\\n","                                                      $DATA_DIR/train/lidar \\\n","                                                      $DATA_DIR/train/label \\\n","                                                      $DATA_DIR/val/lidar \\\n","                                                      $DATA_DIR/val/label"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4JHq5ti5ReO9"},"outputs":[],"source":["# Generate dataset statistics for data augmentation\n","!sed -i \"s|TAO_DATA_PATH|$DATA_DIR/|g\" $SPECS_DIR/pointpillars.yaml\n","!sed -i \"s|EXPERIMENT_DIR_PATH|$EXPERIMENT_DIR/|g\" $SPECS_DIR/pointpillars.yaml\n","!tao pointpillars dataset_convert -e $SPECS_DIR/pointpillars.yaml"]},{"cell_type":"markdown","metadata":{"id":"XC83UC_AReO-"},"source":[" ## 3. Provide training specification <a class=\"anchor\" id=\"head-3\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!cat $SPECS_DIR/pointpillars.yaml"]},{"cell_type":"markdown","metadata":{"id":"bAXbUpFSReO-"},"source":[" ## 4. Run TAO training <a class=\"anchor\" id=\"head-4\"></a>\n"," * Provide the sample spec file for training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mtRtPXomReO-","scrolled":true},"outputs":[],"source":["!tao pointpillars train -e $SPECS_DIR/pointpillars.yaml -r $EXPERIMENT_DIR -k $KEY"]},{"cell_type":"markdown","metadata":{"id":"uovo763AReO-"},"source":[" ## 5. Evaluate trained models <a class=\"anchor\" id=\"head-5\"></a>"]},{"cell_type":"markdown","metadata":{"id":"ogaqE04SReO_"},"source":["The evaluation metric in TAO PointPillars is different from that in official metric of KITTI point cloud detection. While KITTI metric considers easy/moderate/hard categories of objects and filters small objects whose sizes are smaller than a threshold, it is only meaningful for KITTI dataset. Instead, TAO PointPillars metric is a general metric that does not classify objects into easy/moderate/hard categories and does not exclude objects in calculation of metric. This makes TAO PointPillars metric a general metric that is applicable to a general dataset. The final result is average precision(AP) and mean average precision(mAP) regardless of its details in computation. Due to this, the TAO PointPillars metric is not comparable with KITTI official metric on KITTI dataset, although they should be roughly the same."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DzxDphd3ReO_"},"outputs":[],"source":["!tao pointpillars evaluate -e $SPECS_DIR/pointpillars.yaml -r $EXPERIMENT_DIR -k $KEY"]},{"cell_type":"markdown","metadata":{"id":"yQfziD8xReO_"},"source":[" ## 6. Prune trained models <a class=\"anchor\" id=\"head-6\"></a>\n"," * Specify pre-trained model\n"," * Threshold for pruning\n"," * A key to save and load the model\n"," * Output directory to store the model\n"," \n","Usually, you just need to adjust `-pth` (threshold) for accuracy and model size trade off. Higher `pth` gives you smaller model (and thus higher inference speed) but worse accuracy. The threshold to use is depend on the dataset. A `pth` value below is just a start point. If the retrain accuracy is good, you can increase this value to get smaller models. Otherwise, lower this value to get better accuracy."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LSOPnJKHRePA"},"outputs":[],"source":["!tao pointpillars prune -e $SPECS_DIR/pointpillars.yaml -r $EXPERIMENT_DIR -k $KEY \\\n","                    -m $EXPERIMENT_DIR/ckpt/checkpoint_epoch_20.tlt \\\n","                    -pth 0.1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VpfFheVARePB"},"outputs":[],"source":["!ls -lht $EXPERIMENT_DIR"]},{"cell_type":"markdown","metadata":{"id":"FX672k3_RePB"},"source":[" ## 7. Retrain pruned models <a class=\"anchor\" id=\"head-7\"></a>\n"," * Model needs to be re-trained to bring back accuracy after pruning\n"," * Specify re-training specification"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!sed -i \"s|TAO_DATA_PATH|$DATA_DIR/|g\" $SPECS_DIR/pointpillars_retrain.yaml\n","!sed -i \"s|EXPERIMENT_DIR_PATH|$EXPERIMENT_DIR/|g\" $SPECS_DIR/pointpillars_retrain.yaml"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qGj56kM2RePB"},"outputs":[],"source":["# Retraining using the pruned model as pretrained weights \n","!tao pointpillars train -e $SPECS_DIR/pointpillars_retrain.yaml -r $EXPERIMENT_DIR/retrain -k $KEY"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q2aAnJEoRePB"},"outputs":[],"source":["# Listing the newly retrained model.\n","!ls -lht $EXPERIMENT_DIR/retrain/ckpt"]},{"cell_type":"markdown","metadata":{"id":"qelG7cySRePB"},"source":[" ## 8. Evaluate retrained model <a class=\"anchor\" id=\"head-8\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PNSunOnxRePC"},"outputs":[],"source":["!tao pointpillars evaluate -e $SPECS_DIR/pointpillars_retrain.yaml -r $EXPERIMENT_DIR/retrain -k $KEY"]},{"cell_type":"markdown","metadata":{"id":"ItClpefKRePC"},"source":[" ## 9. Visualize inferences <a class=\"anchor\" id=\"head-9\"></a>\n"," In this section, we run the inference command on the trained models."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9NSFyk5sRePC"},"outputs":[],"source":["!tao pointpillars inference -e $SPECS_DIR/pointpillars_retrain.yaml -r $EXPERIMENT_DIR/retrain -k $KEY"]},{"cell_type":"markdown","metadata":{"id":"XIPaegQzRePD"},"source":["The `inference` command will produce visualization of bounding boxes of objects in and rendering of point cloud. This command can be slow due to plots. If you are not going to finish it, you can abort it and check the partial detected results(images) in `$EXPERIMENT_DIR/retrain/detected_boxes`."]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"pointpillars.ipynb","provenance":[]},"file_extension":".py","gpuClass":"standard","kernelspec":{"display_name":"Python 3.10.2 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.2"},"mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython2","version":2,"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":0}
