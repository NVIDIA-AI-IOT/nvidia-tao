{"cells":[{"cell_type":"markdown","source":["# TAO PTM Inference using TAO-Deploy\n","\n","Transfer learning is the process of transferring learned features from one application to another. It is a commonly used training technique where you use a model trained on one task and re-train to use it on a different task. \n","\n","Train Adapt Optimize (TAO) Toolkit is a simple and easy-to-use Python based AI toolkit for taking purpose-built AI models and customizing them with users' own data.\n","\n","<img align=\"center\" src=\"https://developer.nvidia.com/sites/default/files/akamai/embedded-transfer-learning-toolkit-software-stack-1200x670px.png\" width=\"1080\"> "],"metadata":{"id":"YgvdXxCl8Vk0"}},{"cell_type":"markdown","source":["## Learning Objectives\n","\n","In this notebook, you will learn how to leverage the simplicity and convenience of TAO to:\n","\n","* Download a TAO special use-case pre-trained model\n","* Export the downloaded etlt model to TensorRT engine\n","* Run inference on TRT engine file using TAO-Deploy\n","* Visualize the inferences\n","\n","### Table of Contents\n","\n","This notebook shows an example of classifying gestures using GestureNet in the Train Adapt Optimize (TAO) Toolkit.\n","\n","1. [Get the TensorRT tar file](#head-1) <br>\n","2. [Setup GPU environment](#head-2) <br>\n","    2.1 [Connect to GPU Instance](#head-2-1) <br>\n","    2.2 [Mounting Google drive](#head-2-2) <br>\n","3. [Choose PTM model](#head-3) <br>\n","4. [Untar TensorRT file if needed](#head-4) <br>\n","5. [Install dependencies](#head-5) <br>\n","6. [Download the etlt model of the ptm chosen](#head-6) <br>\n","7. [Generate TRT engine file for the ptm's etlt file](#head-7) <br>\n","8. [Run inference on TRT engine file](#head-8) <br>\n","9. [Visualize inference](#head-8) <br>"],"metadata":{"id":"ALzPuesW8Vo8"}},{"cell_type":"markdown","source":["#### FIXME\n","1. ptm_model_name - set this to one of the available PTM models\n","1. trt_tar_path - set this path of the uploaded TensorRT tar.gz file after browser download\n","1. trt_untar_folder_path - set to path of the folder where the TensoRT tar.gz file has to be untarred into\n","1. trt_version - set this to the version of TRT you have downloaded\n","1. COLAB_NOTEBOOKS_PATH - set this to the path of cloned colab notebook's github folder\n","1. GENERAL_WHL_PATH: set this to the path contain eff whl files\n","1. CODEBASE_WHL_PATH: set this to the path contining tao_deploy whl files\n","1. ptm_download_folder: set the folder path where you want to download the file into\n","1. **(Optional FIXME)** model_to_download_map(dictionary_value for chosen PTM key) - change the version tag of the PTM model if you want a version different from the below defaults\n","1. data_type - choose between FP32 and FP16\n","1. trt_out_folder - set this to the output folder for TensorRT engine file writing\n","1. inference_out_folder - set this to the output folder to write the inference results to\n","1. inference_input_images_folder - set this to the folder path containing images to run the inference on"],"metadata":{"id":"T3GthMNyebue"}},{"cell_type":"markdown","source":["## 1. Get the TensorRT tar file\n","\n","1. Visit https://developer.nvidia.com/tensorrt\n","2. Clicking `Download now` from step one directs you to https://developer.nvidia.com/nvidia-tensorrt-download where you have to Login/Join Now for Nvidia Developer Program Membership\n","3. Now, in the download page: Choose TensorRT 8 in available versions\n","4. Agree to Terms and Conditions\n","5. Click on TensorRT 8.5 GA to expand the available options\n","6. Click on 'TensorRT 8.5 GA for Linux x86_64 and CUDA 11.0, 11.1, 11.2, 11.3, 11.4, 11.5, 11.6, 11.7 and 11.8 TAR Package' to dowload the TAR file\n","7. Upload the the tar file to your Google Drive"],"metadata":{"id":"aZX-h3rFkmfL"}},{"cell_type":"markdown","source":["## 2. Setup GPU environment <a class=\"anchor\" id=\"head-2\"></a>\n","\n","### 2.1 Connect to GPU Instance <a class=\"anchor\" id=\"head-2-1\"></a>\n","\n","1. Move any data saved to the Colab Instance storage to Google Drive  \n","2. Change Runtime type to GPU by Runtime(Top Left tab)->Change Runtime Type->GPU(Hardware Accelerator)\n","3.   Then click on Connect (Top Right)\n","\n"],"metadata":{"id":"8UE4Kzb4jx8l"}},{"cell_type":"markdown","source":["### 2.2 Mounting Google drive <a class=\"anchor\" id=\"head-2-2\"></a>\n","Mount your Google drive storage to this Colab instance"],"metadata":{"id":"fAy8Hf2wj5iZ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5wfkZg7LaeGA"},"outputs":[],"source":["try:\n","    import google.colab\n","    %env GOOGLE_COLAB=1\n","    from google.colab import drive\n","    drive.mount('/content/drive', force_remount=True)\n","except:\n","    %env GOOGLE_COLAB=0\n","    print(\"Warning: Not a Colab Environment\")"]},{"cell_type":"markdown","source":["## 3. Choose PTM model <a class=\"anchor\" id=\"head-3\"></a>"],"metadata":{"id":"frHfVZXfkHhY"}},{"cell_type":"code","source":["# Define model_name\n","# Available models (#FIXME 1):\n","# 1. PeopleNet - https://ngc.nvidia.com/catalog/models/nvidia:tao:peoplenet\n","# 2. PeopleSemSegNet - https://ngc.nvidia.com/catalog/models/nvidia:tao:peoplesemsegnet\n","# 3. TrafficCamNet - https://ngc.nvidia.com/catalog/models/nvidia:tao:trafficcamnet\n","# 4. DashCamNet - https://ngc.nvidia.com/catalog/models/nvidia:tao:dashcamnet\n","# 5. FaceDetectIR - https://ngc.nvidia.com/catalog/models/nvidia:tao:facedetectir\n","# 6. FaceDetect - https://ngc.nvidia.com/catalog/models/nvidia:tao:facedetect\n","# 7. VehicleMakeNet - https://ngc.nvidia.com/catalog/models/nvidia:tao:vehiclemakenet\n","# 8. VehicleTypeNet - https://ngc.nvidia.com/catalog/models/nvidia:tao:vehicletypenet\n","# 9. LicensePlateDetection - https://ngc.nvidia.com/catalog/models/nvidia:tao:lpdnet\n","# 10. LicensePlateRecognition - https://ngc.nvidia.com/catalog/models/nvidia:tao:lprnet\n","\n","ptm_model_name = \"PeopleNet\" # FIXME1 (Add the model name from the above mentioned list)"],"metadata":{"id":"Hg0_SfJ8iKWG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4. Untar TensorRT tar file if needed <a class=\"anchor\" id=\"head-4\"></a>\n","\n","1. Set the variable `trt_tarfile_path` to the path where you uploaded the TensorRT tar file\n","1. Set the folder you want to untar into in `trt_untar_folder_path`\n","1. Set the TRT version you have downloaded in `trt_version`\n","  - If the TRT tar file's name is TensorRT-8.5.1.7.Linux.x86_64-gnu.cuda-11.8.cudnn8.6.tar.gz, then the version is 8.5.1.7"],"metadata":{"id":"DGixtxxpuX0C"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"-RkSE9yrjFh3"},"outputs":[],"source":["# FIXME 2: set this path of the uploaded TensorRT tar.gz file after browser download\n","trt_tar_path=\"/content/drive/MyDrive/TensorRT-8.5.1.7.Linux.x86_64-gnu.cuda-11.8.cudnn8.6.tar.gz\"\n","\n","import os\n","if not os.path.exists(trt_tar_path):\n","  raise Exception(\"TAR file not found in the provided path\")\n","\n","# FIXME 3: set to path of the folder where the TensoRT tar.gz file has to be untarred into\n","%env trt_untar_folder_path=/content/drive/MyDrive/trt_untar\n","# FIXME 4: set this to the version of TRT you have downloaded\n","%env trt_version=8.5.1.7\n","\n","!mkdir -p $trt_untar_folder_path\n","\n","import os\n","\n","untar = True\n","for fname in os.listdir(os.environ.get(\"trt_untar_folder_path\", None)):\n","  if fname.startswith(\"TensorRT-\"+os.environ.get(\"trt_version\")) and not fname.endswith(\".tar.gz\"):\n","    untar = False\n","\n","if untar:\n","  !tar -xzf $trt_tar_path -C $trt_untar_folder_path"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wn3FSTbd7DDJ"},"outputs":[],"source":["if not os.path.exists(f'{os.environ.get(\"trt_untar_folder_path\")}/TensorRT-{os.environ.get(\"trt_version\")}'):\n","  raise Exception(\"TensorRT not untarred properly. Please download and untar properly\")"]},{"cell_type":"markdown","source":["## 5. Install dependencies <a class=\"anchor\" id=\"head-5\"></a>"],"metadata":{"id":"dGORsFVC5Qsv"}},{"cell_type":"code","source":["#FIXME 5 - COLAB_NOTEBOOKS_PATH: set this to the path of cloned colab notebook's github folder\n","%env COLAB_NOTEBOOKS_PATH=/content/drive/MyDrive/ColabNotebooks/\n","\n","if not os.path.exists(os.environ[\"COLAB_NOTEBOOKS_PATH\"]):\n","  raise Exception(\"Error, enter Colab notebooks repo path correctly\")\n","\n","#FIXME 6 - GENERAL_WHL_PATH: set this to the path contain eff whl files\n","%env GENERAL_WHL_PATH=/content/drive/MyDrive/tao_deploy/general_whl\n","#FIXME 7 - CODEBASE_WHL_PATH: set this to the path contining tao_deploy whl files\n","%env CODEBASE_WHL_PATH=/content/drive/MyDrive/tao_deploy/codebase_whl\n","\n","if os.path.exists(os.environ[\"GENERAL_WHL_PATH\"]) and os.path.exists(os.environ[\"CODEBASE_WHL_PATH\"]):\n","\n","  !sed -i \"s|PATH_TO_TRT|$trt_untar_folder_path|g\" $COLAB_NOTEBOOKS_PATH/ptm/setup_env_colab.sh\n","  !sed -i \"s|TRT_VERSION|$trt_version|g\" $COLAB_NOTEBOOKS_PATH/ptm/setup_env_colab.sh\n","  !sed -i \"s|PATH_TO_CODEBASE_WHL|$CODEBASE_WHL_PATH|g\" $COLAB_NOTEBOOKS_PATH/ptm/setup_env_colab.sh\n","  !sed -i \"s|PATH_TO_GENERAL_WHL|$GENERAL_WHL_PATH|g\" $COLAB_NOTEBOOKS_PATH/ptm/setup_env_colab.sh\n","\n","  !sh $COLAB_NOTEBOOKS_PATH/ptm/setup_env_colab.sh\n","\n","else:\n","    raise Exception(\"Error, enter the whl paths correctly\")"],"metadata":{"id":"M_KVgR8x1nuf"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D8n0gl-97fo6"},"outputs":[],"source":["import os\n","if os.environ.get(\"LD_LIBRARY_PATH\",\"\") == \"\":\n","  os.environ[\"LD_LIBRARY_PATH\"] = \"\"\n","trt_lib_path = f':{os.environ.get(\"trt_untar_folder_path\")}/TensorRT-{os.environ.get(\"trt_version\")}/lib'\n","os.environ[\"LD_LIBRARY_PATH\"]+=trt_lib_path"]},{"cell_type":"markdown","source":["## 6. Download the etlt model of the ptm chosen <a class=\"anchor\" id=\"head-6\"></a>"],"metadata":{"id":"rERbLrF95XEB"}},{"cell_type":"code","source":["# Installing NGC CLI on the local machine.\n","## Download and install\n","%env LOCAL_PROJECT_DIR=/ngc_content/\n","%env CLI=ngccli_cat_linux.zip\n","!sudo mkdir -p $LOCAL_PROJECT_DIR/ngccli && chmod -R 777 $LOCAL_PROJECT_DIR\n","\n","# Remove any previously existing CLI installations\n","!sudo rm -rf $LOCAL_PROJECT_DIR/ngccli/*\n","!wget \"https://ngc.nvidia.com/downloads/$CLI\" -P $LOCAL_PROJECT_DIR/ngccli\n","!unzip -u -q \"$LOCAL_PROJECT_DIR/ngccli/$CLI\" -d $LOCAL_PROJECT_DIR/ngccli/\n","!rm $LOCAL_PROJECT_DIR/ngccli/*.zip \n","os.environ[\"PATH\"]=\"{}/ngccli/ngc-cli:{}\".format(os.getenv(\"LOCAL_PROJECT_DIR\", \"\"), os.getenv(\"PATH\", \"\"))\n","!cp /usr/lib/x86_64-linux-gnu/libstdc++.so.6 $LOCAL_PROJECT_DIR/ngccli/ngc-cli/libstdc++.so.6"],"metadata":{"id":"xxhfwPzut6Yd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#List the available versions of the PTM model chosen\n","\n","modelname_to_available_ptm_map = {\n","  \"PeopleNet\" : \"nvidia/tao/peoplenet:pruned_v*\",\n","  \"PeopleSegNet\" : \"nvidia/tao/peoplesegnet:deployable*\",\n","  \"PeopleSemSegNet\" : \"nvidia/tao/peoplesemsegnet:deployable*\",\n","  \"TrafficCamNet\" : \"nvidia/tao/trafficcamnet:pruned_v*\",\n","  \"DashCamNet\" : \"nvidia/tao/dashcamnet:pruned_v*\",\n","  \"VehicleMakeNet\" : \"nvidia/tao/vehiclemakenet:pruned_v*\",\n","  \"VehicleTypeNet\" : \"nvidia/tao/vehicletypenet:pruned_v*\",\n","  \"LicensePlateRecognition\" : \"nvidia/tao/lprnet:deployable*\",\n","  \"LicensePlateDetection\" : \"nvidia/tao/lpdnet:pruned_v*\",\n","  \"FaceDetect\" : \"nvidia/tao/facenet:pruned_v*\",\n","  \"FaceDetectIR\" : \"nvidia/tao/facedetectir:pruned_v*\"\n","}\n","\n","model_to_view_regex = modelname_to_available_ptm_map[ptm_model_name]\n","\n","# You can see different version of this PTM model\n","!ngc registry model list $model_to_view_regex"],"metadata":{"id":"Y3zFkzv5kjTh"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1RKSokeCwymf"},"outputs":[],"source":["# FIXME 8 - ptm_download_folder: set the folder path where you want to download the file into\n","%env ptm_download_folder=/content/drive/MyDrive/ptm_models/\n","\n","!sudo rm -rf $ptm_download_folder\n","!sudo mkdir -p $ptm_download_folder && chmod -R 777 $ptm_download_folder\n","\n","# Optional FIXME 9\n","# model_to_download_map dictionary_value for chosen PTM key - change the version tag of the PTM model if you want a version different from the below defaults\n","# Use only the versions which has a etlt file:\n","    # Go to the link of the PTM you chose\n","    # Click 'File Browser tab' to fiew the files for each version\n","    # Choose a version you want to override which contains a etlt file\n","# 1. PeopleNet - https://ngc.nvidia.com/catalog/models/nvidia:tao:peoplenet\n","# 2. PeopleSemSegNet - https://ngc.nvidia.com/catalog/models/nvidia:tao:peoplesemsegnet\n","# 3. TrafficCamNet - https://ngc.nvidia.com/catalog/models/nvidia:tao:trafficcamnet\n","# 4. DashCamNet - https://ngc.nvidia.com/catalog/models/nvidia:tao:dashcamnet\n","# 5. FaceDetectIR - https://ngc.nvidia.com/catalog/models/nvidia:tao:facedetectir\n","# 6. FaceDetect - https://ngc.nvidia.com/catalog/models/nvidia:tao:facedetect\n","# 7. VehicleMakeNet - https://ngc.nvidia.com/catalog/models/nvidia:tao:vehiclemakenet\n","# 8. VehicleTypeNet - https://ngc.nvidia.com/catalog/models/nvidia:tao:vehicletypenet\n","# 9. LicensePlateDetection - https://ngc.nvidia.com/catalog/models/nvidia:tao:lpdnet\n","# 10. LicensePlateRecognition - https://ngc.nvidia.com/catalog/models/nvidia:tao:lprnet\n","\n","\n","model_to_download_map = {\n","  \"PeopleNet\" : \"nvidia/tao/peoplenet:pruned_v2.3\",\n","  \"PeopleSemSegNet\" : \"nvidia/tao/peoplesemsegnet:deployable_vanilla_unet_v2.0.1\",\n","  \"TrafficCamNet\" : \"nvidia/tao/trafficcamnet:pruned_v1.0.2\",\n","  \"DashCamNet\" : \"nvidia/tao/dashcamnet:pruned_v1.0.2\",\n","  \"VehicleMakeNet\" : \"nvidia/tao/vehiclemakenet:pruned_v1.0.1\",\n","  \"VehicleTypeNet\" : \"nvidia/tao/vehicletypenet:pruned_v1.0.1\",\n","  \"LicensePlateRecognition\" : \"nvidia/tao/lprnet:deployable_v1.0\",\n","  \"LicensePlateDetection\" : \"nvidia/tao/lpdnet:pruned_v1.0\",\n","  \"FaceDetect\" : \"nvidia/tao/facenet:pruned_v2.0\",\n","  \"FaceDetectIR\" : \"nvidia/tao/facedetectir:pruned_v1.0.1\"\n","}\n","model_to_download = model_to_download_map[ptm_model_name]\n","os.environ[\"model_to_download\"] = model_to_download\n","\n","!ngc registry model download-version $model_to_download --dest $ptm_download_folder"]},{"cell_type":"markdown","source":["## 7. Generate TRT engine file for the ptm's etlt file <a class=\"anchor\" id=\"head-7\"></a>"],"metadata":{"id":"9tILaq555h86"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"4LvzJFeAYdGE"},"outputs":[],"source":["#FIXME 10 - data_type: choose FP32 or FP16\n","os.environ[\"data_type\"] = \"fp32\"\n","\n","#FIXME 11 - trt_out_folder: choose output folder for TensorRT engine file writing\n","trt_out_folder = \"/content/drive/MyDrive/\" + ptm_model_name\n","\n","!mkdir -p $trt_out_folder\n","\n","import glob\n","input_etlt_file_list = glob.glob(os.environ.get(\"ptm_download_folder\")+\"/**/*.etlt\", recursive=True)\n","if len(input_etlt_file_list) == 0:\n","  raise Exception(\"ETLT file was not downloaded\")\n","\n","os.environ[\"input_etlt_file\"] = input_etlt_file_list[0]\n","\n","if ptm_model_name in (\"LicensePlateRecognition\",\"LicensePlateDetection\"):\n","  # FIXME: country\n","  # us/ccpd for LicensePlateDetection - us for United States, ch for China\n","  # us/ch for LicensePlateRecongition - us for United States, ch for China\n","\n","  country = \"us\"\n","  for countrywise_ptm in input_etlt_file_list:\n","    fname = countrywise_ptm.split(\"/\")[-1]\n","    if fname.startswith(country):\n","      os.environ[\"input_etlt_file\"] = countrywise_ptm\n","\n","action = \"\"\n","if ptm_model_name in (\"PeopleNet\",\"LicensePlateDetection\",\"DashCamNet\",\"TrafficCamNet\",\"FaceDetect\",\"FaceDetectIR\"):\n","  action = \"_trt\"\n","\n","os.environ[\"KEY\"] = \"tlt_encode\"\n","\n","if ptm_model_name in (\"LicensePlateRecognition\",\"LicensePlateDetection\"):\n","  os.environ[\"KEY\"] = \"nvidia_tlt\"\n","\n","\n","os.environ[\"trt_experiment_spec\"] = f\"{os.environ.get('COLAB_NOTEBOOKS_PATH')}/ptm/specs/{ptm_model_name}/{ptm_model_name}{action}.txt\"\n","os.environ[\"trt_out_file_name\"] = f'{trt_out_folder}/{ptm_model_name}.trt.{os.environ[\"data_type\"]}'\n","\n","if ptm_model_name in (\"PeopleNet\",\"LicensePlateDetection\",\"DashCamNet\",\"TrafficCamNet\",\"FaceDetect\",\"FaceDetectIR\"):\n","  !detectnet_v2 gen_trt_engine \\\n","                    -m $input_etlt_file \\\n","                    -k $KEY  \\\n","                    -e $trt_experiment_spec \\\n","                    --data_type $data_type \\\n","                    --batch_size 1 \\\n","                    --max_batch_size 1 \\\n","                    --engine_file $trt_out_file_name\n","                \n","elif ptm_model_name in (\"VehicleMakeNet\",\"VehicleTypeNet\"):\n","  !classification_tf1 gen_trt_engine \\\n","                    -m $input_etlt_file \\\n","                    -k $KEY  \\\n","                    -e $trt_experiment_spec \\\n","                    --data_type $data_type \\\n","                    --batch_size 1 \\\n","                    --max_batch_size 1 \\\n","                    --batches 10 \\\n","                    --engine_file $trt_out_file_name\n","\n","elif ptm_model_name == \"PeopleSemSegNet\":\n","  !unet gen_trt_engine \\\n","                    -m $input_etlt_file \\\n","                    -k $KEY  \\\n","                    -e $trt_experiment_spec \\\n","                    --data_type $data_type \\\n","                    --batch_size 1 \\\n","                    --max_batch_size 3 \\\n","                    --engine_file $trt_out_file_name\n","\n","elif ptm_model_name == \"LicensePlateRecognition\":\n","  !lprnet gen_trt_engine \\\n","                    -m $input_etlt_file \\\n","                    -k $KEY  \\\n","                    --data_type $data_type \\\n","                    --min_batch_size 1 \\\n","                    --opt_batch_size 4 \\\n","                    --max_batch_size 16 \\\n","                    --engine_file $trt_out_file_name"]},{"cell_type":"markdown","source":["## 8. Run inference on TRT engine file <a class=\"anchor\" id=\"head-8\"></a>"],"metadata":{"id":"k4k3FMjm6rwT"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"61baoFhVyt9E"},"outputs":[],"source":["if ptm_model_name in (\"PeopleNet\",\"LicensePlateDetection\",\"DashCamNet\",\"TrafficCamNet\",\"FaceDetect\",\"FaceDetectIR\"):\n","  action = \"_infer\"\n","\n","os.environ[\"inference_experiment_spec\"] = f\"{os.environ.get('COLAB_NOTEBOOKS_PATH')}/ptm/specs/{ptm_model_name}/{ptm_model_name}{action}.txt\"\n","\n","# FIXME 12 - inference_out_folder: Folder path to write the inference results to\n","os.environ[\"inference_out_folder\"] = \"/content/drive/MyDrive/tao_ptm_inference/\"\n","!rm -rf $inference_out_folder\n","\n","# FIXME 13 - inference_input_images_folder: Folder path containing images to run the inference on\n","os.environ[\"inference_input_images_folder\"] = f\"/content/drive/MyDrive/tao_deploy_input_images/{ptm_model_name}\"\n","\n","print(f\"Running inference for {ptm_model_name} on {os.environ['trt_out_file_name']}\")\n","if ptm_model_name in (\"PeopleNet\",\"LicensePlateDetection\",\"DashCamNet\",\"TrafficCamNet\",\"FaceDetect\",\"FaceDetectIR\"):\n","  !detectnet_v2 inference -e $inference_experiment_spec \\\n","                                   -m $trt_out_file_name \\\n","                                   -r $inference_out_folder \\\n","                                   -i $inference_input_images_folder\n","\n","elif ptm_model_name in (\"VehicleMakeNet\",\"VehicleTypeNet\"):\n","  os.environ[\"inference_classmap\"] = f\"{os.environ.get('COLAB_NOTEBOOKS_PATH')}/ptm/specs/{ptm_model_name}/classmap.json\"\n","  !classification_tf1 inference -e $inference_experiment_spec \\\n","                                -m $trt_out_file_name \\\n","                                -r $inference_out_folder \\\n","                                -c $inference_classmap \\\n","                                -i $inference_input_images_folder\n","\n","elif ptm_model_name == \"PeopleSemSegNet\":\n","  # Write path of images to a file - it is required for PeopleSemSegNet which is based on unet \n","  with open(\"/content/PeopleSemSegNet_inference.txt\",\"w\") as file_ptr:\n","    for image_name in os.listdir(os.environ[\"inference_input_images_folder\"]):\n","      if image_name.endswith(\".jpg\") or image_name.endswith(\".png\"):\n","        file_ptr.write(os.environ[\"inference_input_images_folder\"]+\"/\"+image_name+\"\\n\")\n","        file_ptr.flush()\n","\n","  !unet inference -e $inference_experiment_spec \\\n","                  -m $trt_out_file_name \\\n","                  -r $inference_out_folder\n","      \n","elif ptm_model_name == \"LicensePlateRecognition\":\n","  character_file_link = \"https://api.ngc.nvidia.com/v2/models/nvidia/tao/lprnet/versions/trainable_v1.0/files/{}_lp_characters.txt\".format(country)\n","  !wget -q -O /content/characters.txt $character_file_link\n","\n","  !lprnet inference -i $inference_input_images_folder \\\n","                      -e $inference_experiment_spec \\\n","                      -m $trt_out_file_name"]},{"cell_type":"markdown","source":["## 9. Visualize inference <a class=\"anchor\" id=\"head-9\"></a>"],"metadata":{"id":"Dw4Vu36Q6uRl"}},{"cell_type":"code","source":["if ptm_model_name in (\"VehicleMakeNet\", \"VehicleTypeNet\"):\n","  import pandas as pd\n","  dataframe = pd.read_csv(os.environ[\"inference_out_folder\"]+ \"/result.csv\")\n","  with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.max_colwidth', None):\n","    print(dataframe)\n","\n","elif ptm_model_name != \"LicensePlateRecognition\":\n","  from IPython.display import Image, display\n","  import glob\n","\n","  subfolder = \"\"\n","  if ptm_model_name == \"PeopleSemSegNet\":\n","    subfolder = \"vis_overlay\"\n","\n","  inference_out_images_png = glob.glob(f'{os.environ[\"inference_out_folder\"]}/{subfolder}/**/*.png', recursive=True)\n","  inference_out_images_jpg = glob.glob(f'{os.environ[\"inference_out_folder\"]}/{subfolder}/**/*.jpg', recursive=True)\n","  inference_out_images = inference_out_images_png + inference_out_images_jpg\n","\n","  if len(inference_out_images) == 0:\n","    raise Exception(\"Run Inference before visualization\")\n","\n","  display(Image(inference_out_images[0]))\n"],"metadata":{"id":"_Erl19HJ5tE4"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"mount_file_id":"1cVgVgYon2RszavZhEIQ50IMukCIm7Arc","authorship_tag":"ABX9TyM0mVAxr4LlnPWsFZ2ss4Mm"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}