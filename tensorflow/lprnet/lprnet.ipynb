{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Get the TensorRT tar file before running this Notebook\n","\n","1. Visit https://developer.nvidia.com/tensorrt\n","2. Clicking `Download now` from step one directs you to https://developer.nvidia.com/nvidia-tensorrt-download where you have to Login/Join Now for Nvidia Developer Program Membership\n","3. Now, in the download page: Choose TensorRT 8 in available versions\n","4. Agree to Terms and Conditions\n","5. Click on TensorRT 8.5 GA to expand the available options\n","6. Click on 'TensorRT 8.5 GA for Linux x86_64 and CUDA 11.0, 11.1, 11.2, 11.3, 11.4, 11.5, 11.6, 11.7 and 11.8 TAR Package' to dowload the TAR file\n","7. Upload the the tar file to your Google Drive"]},{"cell_type":"markdown","metadata":{"id":"LGLBrzF8hKgS"},"source":["## Connect to GPU Instance\n","\n","1. Change Runtime type to GPU by Runtime(Top Left tab)->Change Runtime Type->GPU(Hardware Accelerator)\n","1. Then click on Connect (Top Right)"]},{"cell_type":"markdown","metadata":{"id":"SjpjyNg5c2V9"},"source":["## Mounting Google drive\n","Mount your Google drive storage to this Colab instance"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18056,"status":"ok","timestamp":1658620835786,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"EvUVkYw0hzqG","outputId":"a8f580a7-bd55-4a9c-8620-c0795752fbc9"},"outputs":[],"source":["import sys\n","if 'google.colab' in sys.modules:\n","    %env GOOGLE_COLAB=1\n","    from google.colab import drive\n","    drive.mount('/content/drive', force_remount=True)\n","else:\n","    %env GOOGLE_COLAB=0\n","    print(\"Warning: Not a Colab Environment\")"]},{"cell_type":"markdown","metadata":{"id":"4IZtVf3cPyom"},"source":["# License Plate Recognition using TAO LPRNet\n","\n","Transfer learning is the process of transferring learned features from one application to another. It is a commonly used training technique where you use a model trained on one task and re-train to use it on a different task. \n","\n","Train Adapt Optimize (TAO) Toolkit  is a simple and easy-to-use Python based AI toolkit for taking purpose-built AI models and customizing them with users' own data.\n","\n","<img align=\"center\" src=\"https://developer.nvidia.com/sites/default/files/akamai/TAO/tlt-tao-toolkit-bring-your-own-model-diagram.png\" width=\"1080\">\n"]},{"cell_type":"markdown","metadata":{"id":"YxZQEfTBPyoq"},"source":["## Learning Objectives\n","\n","In this notebook, you will learn how to leverage the simplicity and convenience of TAO to:\n","\n","* Take a pretrained baseline18 LPRNet model and train it on the OpenALPR benchmark dataset\n","* Run Inference on the trained model\n","* Export the trained model to a .etlt file for deployment to DeepStream\n","\n","## Table of Contents\n","\n","This notebook shows an example usecase of LPRNet using Train Adapt Optimize (TAO) Toolkit.\n","\n","0. [Set up env variables](#head-0)\n","1. [Prepare dataset and pre-trained model](#head-1) <br>\n","    1.1 [Download pre-trained model](#head-1-1) <br>\n","2. [Setup GPU environment](#head-2) <br>\n","    2.1 [Setup Python environment](#head-2-1) <br>\n","3. [Provide training specification](#head-3)\n","4. [Run TAO training](#head-4)\n","5. [Evaluate trained models](#head-5)\n","6. [Inferences](#head-6)"]},{"cell_type":"markdown","metadata":{"id":"y5lxlv5IPyoq"},"source":["#### Note\n","1. This notebook currently is by default set up to run training using 1 GPU. To use more GPU's please update the env variable `$NUM_GPUS` accordingly\n","2. This notebook uses OPENALPR dataset by default, which should be around ~2.2 MB.\n","3. Using the default config/spec file provided in this notebook, each weight file size of lprnet created during training will be ~111 MB\n","\n","## 0. Set up env variables and set FIXME parameters <a class=\"anchor\" id=\"head-0\"></a>\n","\n","*Note: This notebook currently is by default set up to run training using 1 GPU. To use more GPU's please update the env variable `$NUM_GPUS` accordingly*\n","\n","#### FIXME\n","1. NUM_GPUS - set this to <= number of GPU's availble on the instance\n","1. GPU_INDEX - set to to the indices of the GPU available on the instance\n","1. COLAB_NOTEBOOKS_PATH - for Google Colab environment, set this path where you want to clone the repo to; for local system environment, set this path to the already cloned repo\n","1. EXPERIMENT_DIR - set this path to a folder location where pretrained models, checkpoints and log files during different model actions will be saved\n","1. delete_existing_experiments - set to True to remove existing pretrained models, checkpoints and log files of a previous experiment\n","1. DATA_DIR - set this path to a folder location where you want to dataset to be present\n","1. delete_existing_data - set this to True to remove existing preprocessed and original data\n","1. trt_tar_path - set this path of the uploaded TensorRT tar.gz file after browser download\n","1. trt_untar_folder_path - set to path of the folder where the TensoRT tar.gz file has to be untarred into\n","1. trt_version - set this to the version of TRT you have downloaded"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dXT1Uv__Pyor"},"outputs":[],"source":["# Setting up env variables for cleaner command line commands.\n","import os\n","\n","%env TAO_DOCKER_DISABLE=1\n","\n","%env KEY=nvidia_tlt\n","#FIXME1\n","%env NUM_GPUS=1\n","#FIXME2\n","%env GPU_INDEX=0\n","\n","#FIXME3\n","%env COLAB_NOTEBOOKS_PATH=/content/drive/MyDrive/nvidia-tao\n","if os.environ[\"GOOGLE_COLAB\"] == \"1\":\n","    if not os.path.exists(os.path.join(os.environ[\"COLAB_NOTEBOOKS_PATH\"])):\n","\n","      !git clone https://github.com/NVIDIA-AI-IOT/nvidia-tao.git $COLAB_NOTEBOOKS_PATH\n","else:\n","    if not os.path.exists(os.environ[\"COLAB_NOTEBOOKS_PATH\"]):\n","        raise Exception(\"Error, enter the path of the colab notebooks repo correctly\")\n","\n","#FIXME4\n","%env EXPERIMENT_DIR=/content/drive/MyDrive/results/lprnet\n","#FIXME5\n","delete_existing_experiments = True\n","#FIXME6\n","%env DATA_DIR=/content/drive/MyDrive/lprnet_data/\n","#FIXME7\n","delete_existing_data = False\n","\n","if delete_existing_experiments:\n","    !sudo rm -rf $EXPERIMENT_DIR\n","if delete_existing_data:\n","    !sudo rm -rf $DATA_DIR\n","\n","SPECS_DIR=f\"{os.environ['COLAB_NOTEBOOKS_PATH']}/tensorflow/lprnet/specs\"\n","%env SPECS_DIR={SPECS_DIR}\n","# Showing list of specification files.\n","!ls -rlt $SPECS_DIR\n","\n","!sudo mkdir -p $DATA_DIR && sudo chmod -R 777 $DATA_DIR\n","!sudo mkdir -p $EXPERIMENT_DIR && sudo chmod -R 777 $EXPERIMENT_DIR"]},{"cell_type":"markdown","metadata":{"id":"WI_8N9_IPyov"},"source":["## 1. Prepare dataset and pre-trained model <a class=\"anchor\" id=\"head-1\"></a>"]},{"cell_type":"markdown","metadata":{"id":"HvQbbU5wPyov"},"source":[" We will be using the OpenALPR benchmark dataset for the tutorial. The following script will download the dataset automatically and convert it to the format used by TAO. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kBI24bHSPyov"},"outputs":[],"source":["!bash $COLAB_NOTEBOOKS_PATH/tensorflow/lprnet/download_and_prepare_data.sh $DATA_DIR"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Oi9ACFA0Pyov"},"outputs":[],"source":["# verify\n","!echo $DATA_DIR\n","!ls -l $DATA_DIR/\n","!ls -l $DATA_DIR/train\n","!ls -l $DATA_DIR/train/image\n","!ls -l $DATA_DIR/train/label\n"]},{"cell_type":"markdown","metadata":{"id":"p2B28ZwpPyox"},"source":["### 1.1 Download pretrained model from NGC"]},{"cell_type":"markdown","metadata":{"id":"IU1c75iPPyox"},"source":["We will use NGC CLI to get the pre-trained models. For more details, go to https://ngc.nvidia.com and click the SETUP on the navigation bar."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"85pHGpdOPyoy"},"outputs":[],"source":["# Installing NGC CLI on the local machine.\n","## Download and install\n","%env LOCAL_PROJECT_DIR=/ngc_content/\n","%env CLI=ngccli_cat_linux.zip\n","!sudo mkdir -p $LOCAL_PROJECT_DIR/ngccli && sudo chmod -R 777 $LOCAL_PROJECT_DIR\n","\n","# Remove any previously existing CLI installations\n","!sudo rm -rf $LOCAL_PROJECT_DIR/ngccli/*\n","!wget \"https://ngc.nvidia.com/downloads/$CLI\" -P $LOCAL_PROJECT_DIR/ngccli\n","!unzip -u -q \"$LOCAL_PROJECT_DIR/ngccli/$CLI\" -d $LOCAL_PROJECT_DIR/ngccli/\n","!rm $LOCAL_PROJECT_DIR/ngccli/*.zip \n","os.environ[\"PATH\"]=\"{}/ngccli/ngc-cli:{}\".format(os.getenv(\"LOCAL_PROJECT_DIR\", \"\"), os.getenv(\"PATH\", \"\"))\n","!cp /usr/lib/x86_64-linux-gnu/libstdc++.so.6 $LOCAL_PROJECT_DIR/ngccli/ngc-cli/libstdc++.so.6"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IcsQ_FWqPyoy"},"outputs":[],"source":["!ngc registry model list nvidia/tao/lprnet:*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dAx-bsIDPyoy"},"outputs":[],"source":["!mkdir -p $EXPERIMENT_DIR/pretrained_lprnet_baseline18/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8EieO4uYPyoy"},"outputs":[],"source":["# Pull pretrained model from NGC\n","!ngc registry model download-version nvidia/tao/lprnet:trainable_v1.0 --dest $EXPERIMENT_DIR/pretrained_lprnet_baseline18"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m0ANlm3XPyoz"},"outputs":[],"source":["print(\"Check that model is downloaded into dir.\")\n","!ls -l $EXPERIMENT_DIR/pretrained_lprnet_baseline18/lprnet_vtrainable_v1.0"]},{"cell_type":"markdown","metadata":{"id":"_26rCobXcri1"},"source":["## 2. Setup GPU environment <a class=\"anchor\" id=\"head-2\"></a>\n"]},{"cell_type":"markdown","metadata":{"id":"MBV_YWiTc_KM"},"source":["### 2.1 Setup Python environment <a class=\"anchor\" id=\"head-2-1\"></a>\n","Setup the environment necessary to run the TAO Networks by running the bash script"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s2Xygw-y8fjm"},"outputs":[],"source":["# FIXME 8: set this path of the uploaded TensorRT tar.gz file after browser download\n","trt_tar_path=\"/content/drive/MyDrive/TensorRT-8.5.1.7.Linux.x86_64-gnu.cuda-11.8.cudnn8.6.tar.gz\"\n","\n","import os\n","if not os.path.exists(trt_tar_path):\n","  raise Exception(\"TAR file not found in the provided path\")\n","\n","# FIXME 9: set to path of the folder where the TensoRT tar.gz file has to be untarred into\n","%env trt_untar_folder_path=/content/trt_untar\n","# FIXME 10: set this to the version of TRT you have downloaded\n","%env trt_version=8.5.1.7\n","\n","!mkdir -p $trt_untar_folder_path\n","\n","import os\n","\n","untar = True\n","for fname in os.listdir(os.environ.get(\"trt_untar_folder_path\", None)):\n","  if fname.startswith(\"TensorRT-\"+os.environ.get(\"trt_version\")) and not fname.endswith(\".tar.gz\"):\n","    untar = False\n","\n","if untar:\n","  !tar -xzf $trt_tar_path -C /content/trt_untar"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","if os.environ[\"GOOGLE_COLAB\"] == \"1\":\n","    os.environ[\"bash_script\"] = \"setup_env.sh\"\n","else:\n","    os.environ[\"bash_script\"] = \"setup_env_desktop.sh\"\n","\n","!sed -i \"s|PATH_TO_TRT|$trt_untar_folder_path|g\"$COLAB_NOTEBOOKS_PATH/tensorflow/$bash_script\n","!sed -i \"s|TRT_VERSION|$trt_version|g\" $COLAB_NOTEBOOKS_PATH/tensorflow/$bash_script\n","!sed -i \"s|PATH_TO_COLAB_NOTEBOOKS|$COLAB_NOTEBOOKS_PATH|g\" $COLAB_NOTEBOOKS_PATH/tensorflow/$bash_script\n","\n","!sh $COLAB_NOTEBOOKS_PATH/tensorflow/$bash_script"]},{"cell_type":"markdown","metadata":{"id":"7N3YWk16Pyoz"},"source":["## 3. Provide training specification <a class=\"anchor\" id=\"head-3\"></a>\n","\n","* Note the spec $SPEC_DIR/default_sepc.txt is for training on US license plates:\n","    * the max license plate length is 8;\n","        * You can change `max_label_length` in `lpr_config` to satisfy your own dataset.\n","    * the characters of US license plates are: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, A, B, C, D, E, F, G, H, I, J, K, L, M, N, P, Q, R, S, T, U, V, W, X, Y, Z \n","        * You can change `characters_list_file` in `dataset_config` to set your own characters.\n","        * `characters_list_file` should contain all the characters in dataset. And one character takes one line. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v3fLUuoaPyo0","scrolled":true},"outputs":[],"source":["!sed -i \"s|TAO_DATA_PATH|$DATA_DIR/|g\" $SPECS_DIR/tutorial_spec.txt\n","!sed -i \"s|TAO_SPEC_DIR|$SPECS_DIR/|g\" $SPECS_DIR/tutorial_spec.txt\n","!cat $SPECS_DIR/tutorial_spec.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yb9OAcd7Pyo0"},"outputs":[],"source":["!cat $SPECS_DIR/us_lp_characters.txt"]},{"cell_type":"markdown","metadata":{"id":"-XifdFs5Pyo0"},"source":["## 4. Run TAO training <a class=\"anchor\" id=\"head-4\"></a>\n","* Provide the sample spec file and the output directory location for models\n","* WARNING: training will take several hours or one day to complete"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OnhmABuWPyo0"},"outputs":[],"source":["!mkdir -p $EXPERIMENT_DIR/experiment_dir_unpruned"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c8wKuOHNPyo1","scrolled":true},"outputs":[],"source":["print(\"For multi-GPU, change --gpus based on your machine.\")\n","!tao model lprnet train --gpus=1 --gpu_index=$GPU_INDEX \\\n","                  -e $SPECS_DIR/tutorial_spec.txt \\\n","                  -r $EXPERIMENT_DIR/experiment_dir_unpruned \\\n","                  -k $KEY \\\n","                  -m $EXPERIMENT_DIR/pretrained_lprnet_baseline18/lprnet_vtrainable_v1.0/us_lprnet_baseline18_trainable.tlt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R7I87ZIdPyo1"},"outputs":[],"source":["print(\"To resume training from a checkpoint, set the -m option to be the .tlt you want to resume from and --initial_epochs to be the epoch index of the resumed checkpoint\")\n","# !tao model lprnet train --gpu_index=$GPU_INDEX \\\n","#                   -e $SPECS_DIR/tutorial_spec.txt \\\n","#                   -r $EXPERIMENT_DIR/experiment_dir_unpruned \\\n","#                   -k $KEY \\\n","#                   -m $EXPERIMENT_DIR/experiment_dir_unpruned/weights/lprnet_epoch-01.tlt\n","#                   --initial_epoch 2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WDkWKQQkPyo1"},"outputs":[],"source":["print('Model for each epoch:')\n","print('---------------------')\n","!ls -ltrh $EXPERIMENT_DIR/experiment_dir_unpruned/weights/"]},{"cell_type":"markdown","metadata":{"id":"dCYg-llnPyo1"},"source":["## 5. Evaluate trained models <a class=\"anchor\" id=\"head-5\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ELV8uJr-Pyo2","scrolled":true},"outputs":[],"source":["!tao model lprnet evaluate --gpu_index=$GPU_INDEX -e $SPECS_DIR/tutorial_spec.txt \\\n","                     -m $EXPERIMENT_DIR/experiment_dir_unpruned/weights/lprnet_epoch-24.tlt \\\n","                     -k $KEY"]},{"cell_type":"markdown","metadata":{"id":"cvZE4H8KPyo2"},"source":["## 6. Inferences <a class=\"anchor\" id=\"head-6\"></a>\n","In this section, we run the lprnet inference tool to generate inferences on the trained models and print the results. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YlqyTE7MPyo3"},"outputs":[],"source":["# Running inference for detection on n images\n","!tao model lprnet inference --gpu_index=$GPU_INDEX -i $DATA_DIR/val/image \\\n","                      -e $SPECS_DIR/tutorial_spec.txt \\\n","                      -m $EXPERIMENT_DIR/experiment_dir_unpruned/weights/lprnet_epoch-24.tlt \\\n","                      -k $KEY "]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"lprnet.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.10.2 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.2"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":0}
