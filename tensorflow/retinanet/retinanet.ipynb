{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Get the TensorRT tar file before running this Notebook\n","\n","1. Visit https://developer.nvidia.com/tensorrt\n","2. Clicking `Download now` from step one directs you to https://developer.nvidia.com/nvidia-tensorrt-download where you have to Login/Join Now for Nvidia Developer Program Membership\n","3. Now, in the download page: Choose TensorRT 8 in available versions\n","4. Agree to Terms and Conditions\n","5. Click on TensorRT 8.6 GA to expand the available options\n","6. Click on 'TensorRT 8.6 GA for Linux x86_64 and CUDA 12.0 and 12.1 TAR Package' to dowload the TAR file\n","7. Upload the the tar file to your Google Drive"]},{"cell_type":"markdown","metadata":{"id":"LGLBrzF8hKgS"},"source":["## Connect to GPU Instance\n","\n","1. Change Runtime type to GPU by Runtime(Top Left tab)->Change Runtime Type->GPU(Hardware Accelerator)\n","1. Then click on Connect (Top Right)"]},{"cell_type":"markdown","metadata":{"id":"SjpjyNg5c2V9"},"source":["## Mounting Google drive\n","Mount your Google drive storage to this Colab instance"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18056,"status":"ok","timestamp":1658620835786,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"EvUVkYw0hzqG","outputId":"a8f580a7-bd55-4a9c-8620-c0795752fbc9"},"outputs":[],"source":["import sys\n","if 'google.colab' in sys.modules:\n","    %env GOOGLE_COLAB=1\n","    from google.colab import drive\n","    drive.mount('/content/drive', force_remount=True)\n","else:\n","    %env GOOGLE_COLAB=0\n","    print(\"Warning: Not a Colab Environment\")"]},{"cell_type":"markdown","metadata":{"id":"kbFo_8A0ZFQP"},"source":["# Object Detection using TAO RetinaNet\n","\n","Transfer learning is the process of transferring learned features from one application to another. It is a commonly used training technique where you use a model trained on one task and re-train to use it on a different task. \n","\n","Train Adapt Optimize (TAO) Toolkit  is a simple and easy-to-use Python based AI toolkit for taking purpose-built AI models and customizing them with users' own data.\n","\n","<img align=\"center\" src=\"https://developer.nvidia.com/sites/default/files/akamai/TAO/tlt-tao-toolkit-bring-your-own-model-diagram.png\" width=\"1080\">"]},{"cell_type":"markdown","metadata":{"id":"x1bS82Q4ZFQR"},"source":["## Learning Objectives\n","In this notebook, you will learn how to leverage the simplicity and convenience of TAO to:\n","\n","* Take a pretrained resnet18 model and train a ResNet-18 RetinaNet model on the KITTI dataset\n","* Prune the trained retinanet model\n","* Retrain the pruned model to recover lost accuracy\n","* Export the pruned model\n","* Quantize the pruned model using QAT\n","* Run Inference on the trained model\n","* Export the pruned, quantized and retrained model to a .etlt file for deployment to DeepStream\n","\n","## Table of Contents\n","\n","This notebook shows an example usecase of RetinaNet object detection using Train Adapt Optimize (TAO) Toolkit.\n","\n","0. [Set up env variables](#head-0)\n","1. [Prepare dataset and pre-trained model](#head-1) <br>\n","    1.1 [Download the dataset](#head-1-1) <br>\n","    1.2 [Validate the downloaded dataset](#head-1-2) <br>\n","    1.3 [Generate tfrecords from kitti format dataset](#head-1-3) <br>\n","    1.4 [Download pre-trained model](#head-1-4) <br>\n","2. [Setup GPU environment](#head-2) <br>\n","    2.1 [Setup Python environment](#head-2-1) <br>\n","3. [Generate tfrecords from kitti format dataset](#head-3)\n","4. [Provide training specification](#head-4)\n","5. [Run TAO training](#head-5)\n","6. [Evaluate trained models](#head-6)\n","7. [Prune trained models](#head-7)\n","8. [Retrain pruned models](#head-8)\n","9. [Evaluate retrained model](#head-9)\n","10. [Visualize inferences](#head-10)"]},{"cell_type":"markdown","metadata":{"id":"qDGUJzH9ZFQS"},"source":["#### Note\n","1. This notebook currently is by default set up to run training using 1 GPU. To use more GPU's please update the env variable `$NUM_GPUS` accordingly\n","2. This notebook uses KITTI dataset by default, which should be around ~12 GB. If you are limited by Google-Drive storage, we recommend to:\n","\n","    i. Download the dataset onto the local system\n","\n","    ii. Run the utility script at $COLAB_NOTEBOOKS/tensorflow/utils/generate_kitti_subset.py in your local system\n","\n","    iii. This generates a subset of kitti dataset with number of sample images you wish for\n","\n","    iv. Upload this subset onto Google Drive\n","\n","3. Using the default config/spec file provided in this notebook, each weight file size of retinanet created during training will be ~141 MB\n","\n","## 0. Set up env variables and set FIXME parameters <a class=\"anchor\" id=\"head-0\"></a>\n","\n","*Note: This notebook currently is by default set up to run training using 1 GPU. To use more GPU's please update the env variable `$NUM_GPUS` accordingly*\n","\n","#### FIXME\n","1. NUM_GPUS - set this to <= number of GPU's availble on the instance\n","1. COLAB_NOTEBOOKS_PATH - for Google Colab environment, set this path where you want to clone the repo to; for local system environment, set this path to the already cloned repo\n","1. EXPERIMENT_DIR - set this path to a folder location where pretrained models, checkpoints and log files during different model actions will be saved\n","1. delete_existing_experiments - set to True to remove existing pretrained models, checkpoints and log files of a previous experiment\n","1. DATA_DIR - set this path to a folder location where you want to dataset to be present\n","1. delete_existing_data - set this to True to remove existing preprocessed and original data\n","1. trt_tar_path - set this path of the uploaded TensorRT tar.gz file after browser download\n","1. trt_untar_folder_path - set to path of the folder where the TensoRT tar.gz file has to be untarred into\n","1. trt_version - set this to the version of TRT you have downloaded"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1657348927326,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"5vxt7N61ZFQS","outputId":"e8ae168e-bff5-431e-968e-bba7a8e6c5fb"},"outputs":[],"source":["# Setting up env variables for cleaner command line commands.\n","import os\n","\n","%env TAO_DOCKER_DISABLE=1\n","\n","%env KEY=nvidia_tlt\n","#FIXME1\n","%env NUM_GPUS=1\n","\n","#FIXME2\n","%env COLAB_NOTEBOOKS_PATH=/content/drive/MyDrive/nvidia-tao\n","if os.environ[\"GOOGLE_COLAB\"] == \"1\":\n","    if not os.path.exists(os.path.join(os.environ[\"COLAB_NOTEBOOKS_PATH\"])):\n","\n","      !git clone https://github.com/NVIDIA-AI-IOT/nvidia-tao.git $COLAB_NOTEBOOKS_PATH\n","else:\n","    if not os.path.exists(os.environ[\"COLAB_NOTEBOOKS_PATH\"]):\n","        raise Exception(\"Error, enter the path of the colab notebooks repo correctly\")\n","\n","#FIXME3\n","%env EXPERIMENT_DIR=/content/drive/MyDrive/results/retinanet\n","#FIXME4\n","delete_existing_experiments = True\n","#FIXME5\n","%env DATA_DIR=/content/drive/MyDrive/kitti_data/\n","#FIXME6\n","delete_existing_data = False\n","\n","if delete_existing_experiments:\n","    !sudo rm -rf $EXPERIMENT_DIR\n","if delete_existing_data:\n","    !sudo rm -rf $DATA_DIR\n","\n","SPECS_DIR=f\"{os.environ['COLAB_NOTEBOOKS_PATH']}/tensorflow/retinanet/specs\"\n","%env SPECS_DIR={SPECS_DIR}\n","# Showing list of specification files.\n","!ls -rlt $SPECS_DIR\n","\n","!sudo mkdir -p $DATA_DIR && sudo chmod -R 777 $DATA_DIR\n","!sudo mkdir -p $EXPERIMENT_DIR && sudo chmod -R 777 $EXPERIMENT_DIR"]},{"cell_type":"markdown","metadata":{"id":"74w4Nt9fZFQV"},"source":["## 1. Prepare dataset and pre-trained model <a class=\"anchor\" id=\"head-2\"></a>"]},{"cell_type":"markdown","metadata":{},"source":["We will be using NVIDIA created Synthetic Object detection data based on KITTI dataset format in this notebook. To find more details about kitti format, please visit [here](https://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=2d).\n","\n","**If using custom dataset; it should follow this dataset structure**\n","```\n","$DATA_DIR/training\n","├── images\n","│   ├── image_name_1.jpg\n","│   ├── image_name_2.jpg\n","|   ├── ...\n","└── labels\n","    ├── image_name_1.txt\n","    ├── image_name_2.txt\n","    ├── ...\n","$DATA_DIR/val\n","├── images\n","│   ├── image_name_5.jpg\n","│   ├── image_name_6.jpg\n","|   ├── ...\n","└── labels\n","    ├── image_name_5.txt\n","    ├── image_name_6.txt\n","    ├── ...\n","```\n","The file name should be same for images and labels folders"]},{"cell_type":"markdown","metadata":{},"source":["### 1.1 Download the dataset <a class=\"anchor\" id=\"head-1-1\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!python3 -m pip install awscli\n","!aws s3 cp --no-sign-request s3://tao-object-detection-synthetic-dataset/tao_od_synthetic_train.tar.gz $DATA_DIR/\n","!aws s3 cp --no-sign-request s3://tao-object-detection-synthetic-dataset/tao_od_synthetic_val.tar.gz $DATA_DIR/\n","\n","!mkdir -p $DATA_DIR/train/ && rm -rf $DATA_DIR/train/*\n","!mkdir -p $DATA_DIR/val/ && rm -rf $DATA_DIR/val/*\n","\n","!tar -xzf $DATA_DIR/tao_od_synthetic_train.tar.gz -C $DATA_DIR/train/\n","!tar -xzf $DATA_DIR/tao_od_synthetic_val.tar.gz -C $DATA_DIR/val/"]},{"cell_type":"markdown","metadata":{"id":"_uK6GEBjZFQZ"},"source":["### 1.4 Download pre-trained model <a class=\"anchor\" id=\"head-1-4\"></a>"]},{"cell_type":"markdown","metadata":{"id":"rV5f0EfWZFQZ"},"source":["We will use NGC CLI to get the pre-trained models. For more details, go to [ngc.nvidia.com](ngc.nvidia.com) and click the SETUP on the navigation bar."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2240,"status":"ok","timestamp":1657348699959,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"J7Hn8AY0ZFQZ","outputId":"84196c5f-cb1d-4c0d-d766-63fdc37be2e0"},"outputs":[],"source":["# Installing NGC CLI on the local machine.\n","## Download and install\n","%env LOCAL_PROJECT_DIR=/ngc_content/\n","%env CLI=ngccli_cat_linux.zip\n","!sudo mkdir -p $LOCAL_PROJECT_DIR/ngccli && sudo chmod -R 777 $LOCAL_PROJECT_DIR\n","\n","# Remove any previously existing CLI installations\n","!sudo rm -rf $LOCAL_PROJECT_DIR/ngccli/*\n","!wget --content-disposition 'https://api.ngc.nvidia.com/v2/resources/nvidia/ngc-apps/ngc_cli/versions/3.23.0/files/ngccli_linux.zip' -P $LOCAL_PROJECT_DIR/ngccli -O $LOCAL_PROJECT_DIR/ngccli/$CLI\n","!unzip -u -q \"$LOCAL_PROJECT_DIR/ngccli/$CLI\" -d $LOCAL_PROJECT_DIR/ngccli/\n","!rm $LOCAL_PROJECT_DIR/ngccli/*.zip\n","os.environ[\"PATH\"]=\"{}/ngccli/ngc-cli:{}\".format(os.getenv(\"LOCAL_PROJECT_DIR\", \"\"), os.getenv(\"PATH\", \"\"))\n","!cp /usr/lib/x86_64-linux-gnu/libstdc++.so.6 $LOCAL_PROJECT_DIR/ngccli/ngc-cli/libstdc++.so.6"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2077,"status":"ok","timestamp":1657348702029,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"dTK7WYffZFQZ","outputId":"a9e71750-e7e8-446a-bb36-8dcb990eda20"},"outputs":[],"source":["!ngc registry model list nvidia/tao/pretrained_object_detection:*"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1657349000769,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"BowwCKypZFQa"},"outputs":[],"source":["!mkdir -p $EXPERIMENT_DIR/pretrained_resnet18/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7819,"status":"ok","timestamp":1657349009909,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"3aebMmpwZFQa","outputId":"e07d1e6c-460c-4eb6-f040-b67e67d94002"},"outputs":[],"source":["# Pull pretrained model from NGC\n","!ngc registry model download-version nvidia/tao/pretrained_object_detection:resnet18 \\\n","                    --dest $EXPERIMENT_DIR/pretrained_resnet18"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1657349009909,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"YFJEIyXRZFQa","outputId":"d363436a-672d-425c-8b3c-b49c3b9df1ce"},"outputs":[],"source":["print(\"Check that model is downloaded into dir.\")\n","!ls -l $EXPERIMENT_DIR/pretrained_resnet18/pretrained_object_detection_vresnet18"]},{"cell_type":"markdown","metadata":{"id":"_26rCobXcri1"},"source":["## 2. Setup GPU environment <a class=\"anchor\" id=\"head-2\"></a>\n"]},{"cell_type":"markdown","metadata":{"id":"MBV_YWiTc_KM"},"source":["### 2.1 Setup Python environment <a class=\"anchor\" id=\"head-2-1\"></a>\n","Setup the environment necessary to run the TAO Networks by running the bash script"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s2Xygw-y8fjm"},"outputs":[],"source":["# FIXME 7: set this path of the uploaded TensorRT tar.gz file after browser download\n","trt_tar_path=\"/content/drive/MyDrive/TensorRT-8.6.1.6.Linux.x86_64-gnu.cuda-12.0.tar.gz\"\n","\n","import os\n","if not os.path.exists(trt_tar_path):\n","  raise Exception(\"TAR file not found in the provided path\")\n","\n","# FIXME 8: set to path of the folder where the TensoRT tar.gz file has to be untarred into\n","%env trt_untar_folder_path=/content/trt_untar\n","# FIXME 9: set this to the version of TRT you have downloaded\n","%env trt_version=8.6.1.6\n","\n","!sudo mkdir -p $trt_untar_folder_path && sudo chmod -R 777 $trt_untar_folder_path/\n","\n","import os\n","\n","untar = True\n","for fname in os.listdir(os.environ.get(\"trt_untar_folder_path\", None)):\n","  if fname.startswith(\"TensorRT-\"+os.environ.get(\"trt_version\")) and not fname.endswith(\".tar.gz\"):\n","    untar = False\n","\n","if untar:\n","  !tar -xzf $trt_tar_path -C /content/trt_untar\n","\n","if os.environ.get(\"LD_LIBRARY_PATH\",\"\") == \"\":\n","  os.environ[\"LD_LIBRARY_PATH\"] = \"\"\n","trt_lib_path = f':{os.environ.get(\"trt_untar_folder_path\")}/TensorRT-{os.environ.get(\"trt_version\")}/lib'\n","os.environ[\"LD_LIBRARY_PATH\"]+=trt_lib_path"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","if os.environ[\"GOOGLE_COLAB\"] == \"1\":\n","    os.environ[\"bash_script\"] = \"setup_env.sh\"\n","else:\n","    os.environ[\"bash_script\"] = \"setup_env_desktop.sh\"\n","\n","!sed -i \"s|PATH_TO_TRT|$trt_untar_folder_path|g\" $COLAB_NOTEBOOKS_PATH/tensorflow/$bash_script\n","!sed -i \"s|TRT_VERSION|$trt_version|g\" $COLAB_NOTEBOOKS_PATH/tensorflow/$bash_script\n","!sed -i \"s|PATH_TO_COLAB_NOTEBOOKS|$COLAB_NOTEBOOKS_PATH|g\" $COLAB_NOTEBOOKS_PATH/tensorflow/$bash_script\n","\n","!sh $COLAB_NOTEBOOKS_PATH/tensorflow/$bash_script"]},{"cell_type":"markdown","metadata":{"id":"GzE2l3CzZFQY"},"source":["## 3 Generate tfrecords from kitti format dataset <a class=\"anchor\" id=\"head-3\"></a>\n","\n","- Update the tfrecords spec file to take in your kitti format dataset\n","- Create the tfrecords using the `dataset_convert`\n","\n","*Note: TFRecords only need to be generated for the training set once.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"04NxnccfZFQY"},"outputs":[],"source":["print(\"TFRecords conversion spec file:\")\n","!sed -i \"s|TAO_DATA_PATH|$DATA_DIR/|g\" $SPECS_DIR/retinanet_tfrecords_kitti_train.txt\n","!cat $SPECS_DIR/retinanet_tfrecords_kitti_train.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":67097,"status":"ok","timestamp":1657348697400,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"mNWsk10EZFQZ","outputId":"000a23f8-fe0c-474f-9a3b-a649c6055464"},"outputs":[],"source":["# Creating a new directory for the output tfrecords dump.\n","print(\"Converting the training set to TFRecords.\")\n","!mkdir -p $DATA_DIR/tfrecords && sudo rm -rf $DATA_DIR/tfrecords/*\n","!tao model retinanet dataset_convert \\\n","               -d $SPECS_DIR/retinanet_tfrecords_kitti_train.txt \\\n","               -o $DATA_DIR/tfrecords/kitti_train"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1657348697400,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"zB9bE0BkZFQZ","outputId":"888e856a-0b82-4b0c-ac49-8786caaa595e"},"outputs":[],"source":["!ls -rlt $DATA_DIR/tfrecords/"]},{"cell_type":"markdown","metadata":{"id":"QcEjLrl6ZFQb"},"source":["## 4. Provide training specification <a class=\"anchor\" id=\"head-4\"></a>\n","* Pre-trained models\n","* Augmentation parameters for on the fly data augmentation\n","* Other training (hyper-)parameters such as batch size, number of epochs, learning rate etc.\n","* *Note* that in the provided spec file, anchor boxes are set to have 3 aspect ratios (`aspect_ratios_global`) and 3 anchor sizes(`n_anchor_levels`) per feature map cell.\n","* *Note* that the provided spec file uses `batch_size_per_gpu: 24`, which assumes at least 16G GPU memory. If you need to adjust batch size, please adjust the learning rate accordingly."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":561,"status":"ok","timestamp":1657348819522,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"MtamzJ3vZFQb","outputId":"2b596c7b-130f-49f8-f749-bc56bee84d55","scrolled":true},"outputs":[],"source":["!sed -i \"s|TAO_DATA_PATH|$DATA_DIR/|g\" $SPECS_DIR/retinanet_train_resnet18_kitti.txt\n","!sed -i \"s|EXPERIMENT_DIR_PATH|$EXPERIMENT_DIR/|g\" $SPECS_DIR/retinanet_train_resnet18_kitti.txt\n","!cat $SPECS_DIR/retinanet_train_resnet18_kitti.txt"]},{"cell_type":"markdown","metadata":{"id":"Q5LeMKBcZFQc"},"source":["## 5. Run TAO training <a class=\"anchor\" id=\"head-5\"></a>\n","* Provide the sample spec file and the output directory location for models\n","* WARNING: training will take several hours or one day to complete"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":179,"status":"ok","timestamp":1657348824871,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"A-2Fkg5qZFQd"},"outputs":[],"source":["!mkdir -p $EXPERIMENT_DIR/experiment_dir_unpruned"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":618106,"status":"ok","timestamp":1657349686633,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"SoxGOyy9ZFQd","outputId":"caf77ef5-be8b-4084-aa92-76025be333d6","scrolled":true},"outputs":[],"source":["!sed -i \"s|YOUR_PRETRAINED_MODEL|$EXPERIMENT_DIR/pretrained_resnet18/pretrained_object_detection_vresnet18/resnet_18.hdf5|g\" $SPECS_DIR/retinanet_train_resnet18_kitti.txt\n","print(\"To run with multigpu, please change --gpus based on the number of available GPUs in your machine.\")\n","!tao model retinanet train -e $SPECS_DIR/retinanet_train_resnet18_kitti.txt \\\n","                     -r $EXPERIMENT_DIR/experiment_dir_unpruned \\\n","                     -k $KEY \\\n","                     --gpus 1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":316609,"status":"ok","timestamp":1657350003236,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"QC90aD0wZFQd","outputId":"ee27307d-c427-4ff1-d6ae-594505eb7ee8"},"outputs":[],"source":["!mkdir -p $EXPERIMENT_DIR/experiment_dir_unpruned_qat\n","print(\"To run with QAT enabled, please uncomment and run the following command.\")\n","# !sed -i \"s/enable_qat: False/enable_qat: True/g\" $SPECS_DIR/retinanet_train_resnet18_kitti.txt\n","# !tao model retinanet train -e $SPECS_DIR/retinanet_train_resnet18_kitti.txt \\\n","#                      -r $EXPERIMENT_DIR/experiment_dir_unpruned_qat \\\n","#                      -k $KEY \\\n","#                      --gpus 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eJ3qaHYfZFQd"},"outputs":[],"source":["print(\"To resume training from a checkpoint, you need to update the spec file.\")\n","print(\"use resume_model_path instead of pretrain_model_path with the checkpoint path you wish to resume from.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xv_8TGTGZFQd"},"outputs":[],"source":["print('Model for each epoch:')\n","print('---------------------')\n","!ls -ltrh $EXPERIMENT_DIR/experiment_dir_unpruned/weights"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":336,"status":"ok","timestamp":1657350130552,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"HT5Hld98ZFQe","outputId":"3cd94185-f702-4b53-9829-8a68f3a08d44"},"outputs":[],"source":["# You can check the evaluation stats in the csv file and pick the model with highest val accuracy.\n","!cat $EXPERIMENT_DIR/experiment_dir_unpruned/retinanet_training_log_resnet18.csv\n","%env EPOCH=010"]},{"cell_type":"markdown","metadata":{"id":"JwANlQILZFQe"},"source":["## 6. Evaluate trained models <a class=\"anchor\" id=\"head-6\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":95686,"status":"ok","timestamp":1657350285221,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"kaUgoDoNZFQe","outputId":"14b6aa19-410a-4c11-bd41-fcf509500df0","scrolled":true},"outputs":[],"source":["!tao model retinanet evaluate -e $SPECS_DIR/retinanet_train_resnet18_kitti.txt \\\n","                        -m $EXPERIMENT_DIR/experiment_dir_unpruned/weights/retinanet_resnet18_epoch_$EPOCH.hdf5 \\\n","                        -k $KEY"]},{"cell_type":"markdown","metadata":{"id":"NzJ-2OsVZFQf"},"source":["## 7. Prune trained models <a class=\"anchor\" id=\"head-7\"></a>\n","* Specify pre-trained model\n","* Equalization criterion (`Only for resnets as they have element wise operations or MobileNets.`)\n","* Threshold for pruning.\n","* A key to save and load the model\n","* Output directory to store the model\n","\n","Usually, you just need to adjust `-pth` (threshold) for accuracy and model size trade off. Higher `pth` gives you smaller model (and thus higher inference speed) but worse accuracy. The threshold value depends on the dataset and the model. `0.4` in the block below is just a start point. If the retrain accuracy is good, you can increase this value to get smaller models. Otherwise, lower this value to get better accuracy."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":231,"status":"ok","timestamp":1657350285431,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"PEfSmpa1ZFQf"},"outputs":[],"source":["# Create an output directory to save the pruned model.\n","!mkdir -p $EXPERIMENT_DIR/experiment_dir_pruned"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":44506,"status":"ok","timestamp":1657350329933,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"mK5vGksEZFQf","outputId":"e9d79778-cce2-430e-9251-aa4f14ec444e","scrolled":true},"outputs":[],"source":["!tao model retinanet prune -m $EXPERIMENT_DIR/experiment_dir_unpruned/weights/retinanet_resnet18_epoch_$EPOCH.hdf5 \\\n","               -o $EXPERIMENT_DIR/experiment_dir_pruned/retinanet_resnet18_pruned.hdf5 \\\n","               -pth 0.4 \\\n","               -k $KEY"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yqAZEuOkZFQf"},"outputs":[],"source":["!mkdir -p $EXPERIMENT_DIR/experiment_dir_pruned_qat\n","print(\"To prune a QAT model:\")\n","# !tao model retinanet prune -m $EXPERIMENT_DIR/experiment_dir_unpruned_qat/weights/retinanet_resnet18_epoch_$EPOCH.hdf5 \\\n","#                -o $EXPERIMENT_DIR/experiment_dir_pruned_qat/retinanet_resnet18_pruned.hdf5 \\\n","#                -pth 0.4 \\\n","#                -k $KEY"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1657350329933,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"XpW_LkNMZFQf","outputId":"88711434-d188-475c-c2a6-18208f16ea1d"},"outputs":[],"source":["!ls -rlt $EXPERIMENT_DIR/experiment_dir_pruned/"]},{"cell_type":"markdown","metadata":{"id":"3h6sgGbaZFQf"},"source":["## 8. Retrain pruned models <a class=\"anchor\" id=\"head-8\"></a>\n","* Model needs to be re-trained to bring back accuracy after pruning\n","* Specify re-training specification\n","* WARNING: training will take several hours or one day to complete"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C0aep2C9ZFQf","scrolled":true},"outputs":[],"source":["# Printing the retrain spec file. \n","# Here we have updated the spec file to include the newly pruned model as a pretrained weights.\n","!sed -i \"s|TAO_DATA_PATH|$DATA_DIR/|g\" $SPECS_DIR/retinanet_retrain_resnet18_kitti.txt\n","!sed -i \"s|EXPERIMENT_DIR_PATH|$EXPERIMENT_DIR/|g\" $SPECS_DIR/retinanet_retrain_resnet18_kitti.txt\n","!cat $SPECS_DIR/retinanet_retrain_resnet18_kitti.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":410,"status":"ok","timestamp":1657350330339,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"Bby9m5OAZFQg"},"outputs":[],"source":["!mkdir -p $EXPERIMENT_DIR/experiment_dir_retrain"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":258489,"status":"ok","timestamp":1657350863502,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"BhPIpK2oZFQg","outputId":"245b11b6-309e-46ef-a635-bcec2b470c05","scrolled":true},"outputs":[],"source":["# Retraining using the pruned model as pretrained weights.\n","print(\"To run with multigpu, please change --gpus based on the number of available GPUs in your machine.\")\n","!tao model retinanet train --gpus 1 \\\n","                     -e $SPECS_DIR/retinanet_retrain_resnet18_kitti.txt \\\n","                     -r $EXPERIMENT_DIR/experiment_dir_retrain \\\n","                     -k $KEY"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wlK1KGX2ZFQg"},"outputs":[],"source":["!mkdir -p $EXPERIMENT_DIR/experiment_dir_retrain_qat\n","print(\"To run with QAT enabled, please uncomment and run the following command.\")\n","# !sed -i \"s/enable_qat: False/enable_qat: True/g\" $SPECS_DIR/retinanet_retrain_resnet18_kitti.txt\n","# !tao model retinanet train --gpus 1 \\\n","#                      -e $SPECS_DIR/retinanet_retrain_resnet18_kitti.txt \\\n","#                      -r $EXPERIMENT_DIR/experiment_dir_retrain_qat \\\n","#                      -k $KEY"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Na-xHgNgZFQg"},"outputs":[],"source":["# Listing the newly retrained model.\n","!ls -rlt $EXPERIMENT_DIR/experiment_dir_retrain/weights\n","# !ls -rlt $EXPERIMENT_DIR/experiment_dir_retrain_qat/weights"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":214,"status":"ok","timestamp":1657350919481,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"OWwJSTPmZFQg","outputId":"5c3f3a69-af99-4b45-ad22-df43d8cf1626"},"outputs":[],"source":["# You can check the evaluation stats in the csv file and pick the model with highest val accuracy.\n","!cat $EXPERIMENT_DIR/experiment_dir_retrain/retinanet_training_log_resnet18.csv\n","%env EPOCH=010"]},{"cell_type":"markdown","metadata":{"id":"eQAG58FZZFQg"},"source":["## 9. Evaluate retrained model <a class=\"anchor\" id=\"head-9\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":88647,"status":"ok","timestamp":1657351009273,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"GqA6pUBAZFQg","outputId":"f0c51388-ba6a-4b88-e0c7-6d8cc78807f7","scrolled":true},"outputs":[],"source":["!tao model retinanet evaluate -e $SPECS_DIR/retinanet_retrain_resnet18_kitti.txt \\\n","                        -m $EXPERIMENT_DIR/experiment_dir_retrain/weights/retinanet_resnet18_epoch_$EPOCH.hdf5 \\\n","                        -k $KEY"]},{"cell_type":"markdown","metadata":{"id":"XwUu6mxkZFQg"},"source":["## 10. Visualize inferences <a class=\"anchor\" id=\"head-10\"></a>\n","In this section, we run the tlt-infer tool to generate inferences on the trained models and visualize the results."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":116098,"status":"ok","timestamp":1657351125358,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"-VDXzHiMZFQh","outputId":"3c3f8d86-0fb8-42cf-fe6c-6a88b3f7f9b0"},"outputs":[],"source":["# Running inference for detection on n images\n","!tao model retinanet inference -i $DATA_DIR/val/images \\\n","                         -r $EXPERIMENT_DIR/retinanet_infer_images \\\n","                         -e $SPECS_DIR/retinanet_retrain_resnet18_kitti.txt \\\n","                         -m $EXPERIMENT_DIR/experiment_dir_retrain/weights/retinanet_resnet18_epoch_$EPOCH.hdf5 \\\n","                         -k $KEY"]},{"cell_type":"markdown","metadata":{"id":"E4mj120GZFQh"},"source":["The `inference` tool produces two outputs. \n","1. Overlain images in `$EXPERIMENT_DIR/retinanet_annotated_images`\n","2. Frame by frame bbox labels in kitti format located in `$EXPERIMENT_DIR/retinanet_infer_labels`"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":497},"executionInfo":{"elapsed":12899,"status":"ok","timestamp":1657351138241,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"znLaYclKZFQh","outputId":"c71519f2-8600-4d69-c717-2e64fe47af5a"},"outputs":[],"source":["# Simple grid visualizer\n","import matplotlib.pyplot as plt\n","import os\n","from math import ceil\n","valid_image_ext = ['.jpg', '.png', '.jpeg', '.ppm']\n","\n","def visualize_images(image_dir, num_cols=1, num_images=1):\n","    output_path = os.path.join(os.environ['EXPERIMENT_DIR'], image_dir)\n","    num_rows = int(ceil(float(num_images) / float(num_cols)))\n","    f, axarr = plt.subplots(num_rows, num_cols, figsize=[80,30])\n","    f.tight_layout()\n","    a = [os.path.join(output_path, image) for image in os.listdir(output_path) \n","         if os.path.splitext(image)[1].lower() in valid_image_ext]\n","    for idx, img_path in enumerate(a[:num_images]):\n","        col_id = idx % num_cols\n","        row_id = idx // num_cols\n","        img = plt.imread(img_path)\n","        axarr[row_id, col_id].imshow(img) "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":291,"output_embedded_package_id":"1rqOxwb09k2ORTZc_N3UFaSdkDzAphjMU"},"executionInfo":{"elapsed":29197,"status":"ok","timestamp":1657351167972,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"Xy_450agZFQh","outputId":"6e96a889-1909-44e0-b734-a4e807a62f52","scrolled":true},"outputs":[],"source":["# Visualizing the sample images.\n","OUTPUT_PATH = 'retinanet_infer_images/images_annotated' # relative path from $EXPERIMENT_DIR.\n","COLS = 2 # number of columns in the visualizer grid.\n","IMAGES = 4 # number of images to visualize.\n","\n","visualize_images(OUTPUT_PATH, num_cols=COLS, num_images=IMAGES)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"retinanet.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.10.2 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.18"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":0}
