{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15700,"status":"ok","timestamp":1657346453957,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"4t-q4duYZJpX","outputId":"1752c7f0-f484-43a5-b373-dd216fd96bb6"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"markdown","metadata":{"id":"kbFo_8A0ZFQP"},"source":["# Object Detection using TAO RetinaNet\n","\n","Transfer learning is the process of transferring learned features from one application to another. It is a commonly used training technique where you use a model trained on one task and re-train to use it on a different task. \n","\n","Train Adapt Optimize (TAO) Toolkit  is a simple and easy-to-use Python based AI toolkit for taking purpose-built AI models and customizing them with users' own data.\n","\n","<img align=\"center\" src=\"https://developer.nvidia.com/sites/default/files/akamai/embedded-transfer-learning-toolkit-software-stack-1200x670px.png\" width=\"1080\">"]},{"cell_type":"markdown","metadata":{"id":"x1bS82Q4ZFQR"},"source":["## Learning Objectives\n","In this notebook, you will learn how to leverage the simplicity and convenience of TAO to:\n","\n","* Take a pretrained resnet18 model and train a ResNet-18 RetinaNet model on the KITTI dataset\n","* Prune the trained retinanet model\n","* Retrain the pruned model to recover lost accuracy\n","* Export the pruned model\n","* Quantize the pruned model using QAT\n","* Run Inference on the trained model\n","* Export the pruned, quantized and retrained model to a .etlt file for deployment to DeepStream\n","* Run inference on the exported. etlt model to verify deployment using TensorRT\n","\n","## Table of Contents\n","\n","This notebook shows an example usecase of RetinaNet object detection using Train Adapt Optimize (TAO) Toolkit.\n","\n","0. [Set up env variables](#head-0)\n","1. [Prepare dataset and pre-trained model](#head-1) <br>\n","    1.1 [Download the dataset](#head-1-1) <br>\n","    1.2 [Validate the downloaded dataset](#head-1-2) <br>\n","    1.3 [Generate tfrecords from kitti format dataset](#head-1-3) <br>\n","    1.4 [Download pre-trained model](#head-1-4) <br>\n","2. [Setup GPU environment](#head-2) <br>\n","    2.1 [Connect to GPU Instance](#head-2-1) <br>\n","    2.2 [Mounting Google drive](#head-2-2) <br>\n","    2.3 [Setup Python environment](#head-2-3) <br>\n","    2.4 [Reset env variables](#head-2-4) <br>\n","3. [Provide training specification](#head-3)\n","4. [Run TAO training](#head-4)\n","5. [Evaluate trained models](#head-5)\n","6. [Prune trained models](#head-6)\n","7. [Retrain pruned models](#head-7)\n","8. [Evaluate retrained model](#head-8)\n","9. [Visualize inferences](#head-9)"]},{"cell_type":"markdown","metadata":{"id":"qDGUJzH9ZFQS"},"source":["## 0. Set up env variables <a class=\"anchor\" id=\"head-0\"></a>\n","\n","When using the purpose-built pretrained models from NGC, please make sure to set the `$KEY` environment variable to the key as mentioned in the model overview. Failing to do so, can lead to errors when trying to load them as pretrained models.\n","\n","*Note: Please make sure to remove any stray artifacts/files from the `$USER_EXPERIMENT_DIR` or `$DATA_DOWNLOAD_DIR` paths as mentioned below, that may have been generated from previous experiments. Having checkpoint files etc may interfere with creating a training graph for a new experiment.*"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1657348927326,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"5vxt7N61ZFQS","outputId":"e8ae168e-bff5-431e-968e-bba7a8e6c5fb"},"outputs":[],"source":["# Setting up env variables for cleaner command line commands.\n","import os\n","\n","print(\"Please replace the variable with your key.\")\n","%env KEY=YOUR_KEY\n","%env EXPERIMENT_DIR=/results/retinanet\n","%env DATA_DIR=/content/drive/MyDrive/pointpillars_data\n","%env SPECS_DIR=/content/drive/MyDrive/ColabNotebooks/tensorflow/retinanet/specs\n","\n","# Showing list of specification files.\n","!ls -rlt $SPECS_DIR"]},{"cell_type":"markdown","metadata":{"id":"74w4Nt9fZFQV"},"source":["## 1. Prepare dataset and pre-trained model <a class=\"anchor\" id=\"head-2\"></a>"]},{"cell_type":"markdown","metadata":{"id":"8tqYq6fiZFQW"},"source":[" We will be using the KITTI detection dataset for the tutorial. To find more details please visit\n"," http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=2d. Please download the KITTI detection images (http://www.cvlibs.net/download.php?file=data_object_image_2.zip) and labels (http://www.cvlibs.net/download.php?file=data_object_label_2.zip) to $DATA_DOWNLOAD_DIR."]},{"cell_type":"markdown","metadata":{"id":"_vi4nEFaZFQW"},"source":["### 1.1. Download the dataset <a class=\"anchor\" id=\"head-1-1\"></a>\n","Once you have gotten the download links in your email, please populate them in place of the `KITTI_IMAGES_DOWNLOAD_URL` and the `KITTI_LABELS_DOWNLOAD_URL`. This next cell, will download the data and place in `$DATA_DIR`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xjCaix7qZFQX"},"outputs":[],"source":["# Create local dir\n","!mkdir -p $DATA_DIR\n","!mkdir -p $EXPERIMENT_DIR"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vwiJYBRbZFQX"},"outputs":[],"source":["import os\n","os.environ[\"URL_IMAGES\"]=KITTI_IMAGES_DOWNLOAD_URL\n","!if [ ! -f $DATA_DIR/data_object_image_2.zip ]; then wget $URL_IMAGES -O $DATA_DIR/data_object_image_2.zip; else echo \"image archive already downloaded\"; fi \n","os.environ[\"URL_LABELS\"]=KITTI_LABELS_DOWNLOAD_URL\n","!if [ ! -f $DATA_DIR/data_object_label_2.zip ]; then wget $URL_LABELS -O $DATA_DIR/data_object_label_2.zip; else echo \"label archive already downloaded\"; fi "]},{"cell_type":"markdown","metadata":{"id":"2v5GwbuHZFQX"},"source":["### 1.2. Validate the downloaded dataset <a class=\"anchor\" id=\"head-1-2\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4IGWcwJRZFQX"},"outputs":[],"source":["# Check the dataset is present\n","!if [ ! -f $DATA_DIR/data_object_image_2.zip ]; then echo 'Image zip file not found, please download.'; else echo 'Found Image zip file.';fi\n","!if [ ! -f $DATA_DIR/data_object_label_2.zip ]; then echo 'Label zip file not found, please download.'; else echo 'Found Labels zip file.';fi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ph64d7KVZFQX"},"outputs":[],"source":["# This may take a while: verify integrity of zip files \n","!sha256sum $DATA_DIR/data_object_image_2.zip | cut -d ' ' -f 1 | grep -xq '^351c5a2aa0cd9238b50174a3a62b846bc5855da256b82a196431d60ff8d43617$' ; \\\n","if test $? -eq 0; then echo \"images OK\"; else echo \"images corrupt, redownload!\" && rm -f $DATA_DIR/data_object_image_2.zip; fi \n","!sha256sum $DATA_DIR/data_object_label_2.zip | cut -d ' ' -f 1 | grep -xq '^4efc76220d867e1c31bb980bbf8cbc02599f02a9cb4350effa98dbb04aaed880$' ; \\\n","if test $? -eq 0; then echo \"labels OK\"; else echo \"labels corrupt, redownload!\" && rm -f $DATA_DIR/data_object_label_2.zip; fi "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TmDP6HzOZFQY"},"outputs":[],"source":["# unpack \n","!unzip -u $DATA_DIR/data_object_image_2.zip -d $DATA_DIR\n","!unzip -u $DATA_DIR/data_object_label_2.zip -d $DATA_DIR"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZRFoJLNlZFQY"},"outputs":[],"source":["# verify\n","!ls -l $DATA_DIR"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N5TYybJNZFQY"},"outputs":[],"source":["# Generate val dataset out of training dataset\n","%cd /content/drive/MyDrive/ColabNotebooks/tensorflow/retinanet/specs\n","!python3 generate_val_dataset.py --input_image_dir=$DATA_DIR/training/image_2 \\\n","                                 --input_label_dir=$DATA_DIR/training/label_2 \\\n","                                 --output_dir=$DATA_DIR/val"]},{"cell_type":"markdown","metadata":{"id":"GzE2l3CzZFQY"},"source":["### 1.3 Generate tfrecords from kitti format dataset <a class=\"anchor\" id=\"head-1-3\"></a>\n","\n","- Update the tfrecords spec file to take in your kitti format dataset\n","- Create the tfrecords using the `dataset_convert`\n","\n","*Note: TFRecords only need to be generated for the training set once.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"04NxnccfZFQY"},"outputs":[],"source":["print(\"TFRecords conversion spec file:\")\n","!cat $SPECS_DIR/retinanet_tfrecords_kitti_train.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":67097,"status":"ok","timestamp":1657348697400,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"mNWsk10EZFQZ","outputId":"000a23f8-fe0c-474f-9a3b-a649c6055464"},"outputs":[],"source":["# Creating a new directory for the output tfrecords dump.\n","print(\"Converting the training set to TFRecords.\")\n","!mkdir -p $DATA_DIR/tfrecords && rm -rf $DATA_DIR/tfrecords/*\n","!tao retinanet dataset_convert \\\n","               -d $SPECS_DIR/retinanet_tfrecords_kitti_train.txt \\\n","               -o $DATA_DIR/tfrecords/kitti_train"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1657348697400,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"zB9bE0BkZFQZ","outputId":"888e856a-0b82-4b0c-ac49-8786caaa595e"},"outputs":[],"source":["!ls -rlt $DATA_DIR/tfrecords/"]},{"cell_type":"markdown","metadata":{"id":"_uK6GEBjZFQZ"},"source":["### 1.4 Download pre-trained model <a class=\"anchor\" id=\"head-1-4\"></a>"]},{"cell_type":"markdown","metadata":{"id":"rV5f0EfWZFQZ"},"source":["We will use NGC CLI to get the pre-trained models. For more details, go to [ngc.nvidia.com](ngc.nvidia.com) and click the SETUP on the navigation bar."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2240,"status":"ok","timestamp":1657348699959,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"J7Hn8AY0ZFQZ","outputId":"84196c5f-cb1d-4c0d-d766-63fdc37be2e0"},"outputs":[],"source":["# Installing NGC CLI on the local machine.\n","## Download and install\n","%env LOCAL_PROJECT_DIR=/content/\n","%env CLI=ngccli_cat_linux.zip\n","!mkdir -p $LOCAL_PROJECT_DIR/ngccli\n","\n","# Remove any previously existing CLI installations\n","!rm -rf $LOCAL_PROJECT_DIR/ngccli/*\n","!wget \"https://ngc.nvidia.com/downloads/$CLI\" -P $LOCAL_PROJECT_DIR/ngccli\n","!unzip -u -q \"$LOCAL_PROJECT_DIR/ngccli/$CLI\" -d $LOCAL_PROJECT_DIR/ngccli/\n","!rm $LOCAL_PROJECT_DIR/ngccli/*.zip \n","os.environ[\"PATH\"]=\"{}/ngccli/ngc-cli:{}\".format(os.getenv(\"LOCAL_PROJECT_DIR\", \"\"), os.getenv(\"PATH\", \"\"))\n","!cp /usr/lib/x86_64-linux-gnu/libstdc++.so.6 /content/ngccli/ngc-cli/libstdc++.so.6"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2077,"status":"ok","timestamp":1657348702029,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"dTK7WYffZFQZ","outputId":"a9e71750-e7e8-446a-bb36-8dcb990eda20"},"outputs":[],"source":["!ngc registry model list nvidia/tao/pretrained_object_detection:*"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1657349000769,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"BowwCKypZFQa"},"outputs":[],"source":["!mkdir -p $EXPERIMENT_DIR/pretrained_resnet18/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7819,"status":"ok","timestamp":1657349009909,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"3aebMmpwZFQa","outputId":"e07d1e6c-460c-4eb6-f040-b67e67d94002"},"outputs":[],"source":["# Pull pretrained model from NGC\n","!ngc registry model download-version nvidia/tao/pretrained_object_detection:resnet18 \\\n","                    --dest $EXPERIMENT_DIR/pretrained_resnet18"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1657349009909,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"YFJEIyXRZFQa","outputId":"d363436a-672d-425c-8b3c-b49c3b9df1ce"},"outputs":[],"source":["print(\"Check that model is downloaded into dir.\")\n","!ls -l $EXPERIMENT_DIR/pretrained_resnet18/pretrained_object_detection_vresnet18"]},{"cell_type":"markdown","metadata":{"id":"_26rCobXcri1"},"source":["## 2. Setup GPU environment <a class=\"anchor\" id=\"head-2\"></a>\n"]},{"cell_type":"markdown","metadata":{"id":"k7Cx1_lMded7"},"source":["### 2.1 Connect to GPU Instance <a class=\"anchor\" id=\"head-2-1\"></a>\n","\n","1. Move any data saved to the Colab Instance storage to Google Drive  \n","2. Change Runtime type to GPU by Runtime(Top Left tab)->Change Runtime Type->GPU(Hardware Accelerator)\n","3.   Then click on Connect (Top Right)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"yl8BoM0Jhzh9"},"source":["### 2.2 Mounting Google drive <a class=\"anchor\" id=\"head-2-2\"></a>\n","Mount your Google drive storage to this Colab instance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vk2m-N4Nh0Sd"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"MBV_YWiTc_KM"},"source":["### 2.3 Setup Python environment <a class=\"anchor\" id=\"head-2-3\"></a>\n","Setup the environment necessary to run the TAO Networks by running the bash script"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s2Xygw-y8fjm"},"outputs":[],"source":["!sh /content/drive/MyDrive/ColabNotebooks/tensorflow/setup_env.sh"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","os.environ[\"PYTHONPATH\"]+=\":/opt/nvidia/\"\n","os.environ[\"PYTHONPATH\"]+=\":/usr/local/lib/python3.6/dist-packages/third_party/nvml\""]},{"cell_type":"markdown","metadata":{"id":"Fl8fSfXseED3"},"source":["### 2.4 Reset env variables <a class=\"anchor\" id=\"head-2-4\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l_T2vBdzeIcO"},"outputs":[],"source":["# Setting up env variables for cleaner command line commands.\n","import os\n","\n","%env KEY=nvidia_tlt\n","%env NUM_GPUS=1\n","%env EXPERIMENT_DIR=/results/classification\n","%env DATA_DIR=/content/drive/MyDrive/tf_data/classification_data/\n","\n","# Set this path if you don't run the notebook from the samples directory.\n","# %env NOTEBOOK_ROOT=~/tao-samples/classification\n","\n","%env SPECS_DIR=/content/drive/MyDrive/ColabNotebooks/tensorflow/classification/specs\n","\n","# Showing list of specification files.\n","!ls -rlt $SPECS_DIR"]},{"cell_type":"markdown","metadata":{"id":"QcEjLrl6ZFQb"},"source":["## 3. Provide training specification <a class=\"anchor\" id=\"head-3\"></a>\n","* Pre-trained models\n","* Augmentation parameters for on the fly data augmentation\n","* Other training (hyper-)parameters such as batch size, number of epochs, learning rate etc.\n","* *Note* that in the provided spec file, anchor boxes are set to have 3 aspect ratios (`aspect_ratios_global`) and 3 anchor sizes(`n_anchor_levels`) per feature map cell.\n","* *Note* that the provided spec file uses `batch_size_per_gpu: 24`, which assumes at least 16G GPU memory. If you need to adjust batch size, please adjust the learning rate accordingly."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":561,"status":"ok","timestamp":1657348819522,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"MtamzJ3vZFQb","outputId":"2b596c7b-130f-49f8-f749-bc56bee84d55","scrolled":true},"outputs":[],"source":["!cat $SPECS_DIR/retinanet_train_resnet18_kitti.txt"]},{"cell_type":"markdown","metadata":{"id":"Q5LeMKBcZFQc"},"source":["## 4. Run TAO training <a class=\"anchor\" id=\"head-4\"></a>\n","* Provide the sample spec file and the output directory location for models\n","* WARNING: training will take several hours or one day to complete"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":179,"status":"ok","timestamp":1657348824871,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"A-2Fkg5qZFQd"},"outputs":[],"source":["!mkdir -p $EXPERIMENT_DIR/experiment_dir_unpruned"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":618106,"status":"ok","timestamp":1657349686633,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"SoxGOyy9ZFQd","outputId":"caf77ef5-be8b-4084-aa92-76025be333d6","scrolled":true},"outputs":[],"source":["!sed -i \"s|YOUR_PRETRAINED_MODEL|$USER_EXPERIMENT_DIR/pretrained_resnet18/pretrained_object_detection_vresnet18/resnet_18.hdf5|g\" $SPECS_DIR/retinanet_train_resnet18_kitti.txt\n","print(\"To run with multigpu, please change --gpus based on the number of available GPUs in your machine.\")\n","!tao retinanet train -e $SPECS_DIR/retinanet_train_resnet18_kitti.txt \\\n","                     -r $USER_EXPERIMENT_DIR/experiment_dir_unpruned \\\n","                     -k $KEY \\\n","                     --gpus 1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":316609,"status":"ok","timestamp":1657350003236,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"QC90aD0wZFQd","outputId":"ee27307d-c427-4ff1-d6ae-594505eb7ee8"},"outputs":[],"source":["!mkdir -p $EXPERIMENT_DIR/experiment_dir_unpruned_qat\n","print(\"To run with QAT enabled, please uncomment and run the following command.\")\n","# !sed -i \"s/enable_qat: False/enable_qat: True/g\" $SPECS_DIR/retinanet_train_resnet18_kitti.txt\n","!tao retinanet train -e $SPECS_DIR/retinanet_train_resnet18_kitti.txt \\\n","                     -r $USER_EXPERIMENT_DIR/experiment_dir_unpruned_qat \\\n","                     -k $KEY \\\n","                     --gpus 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eJ3qaHYfZFQd"},"outputs":[],"source":["print(\"To resume training from a checkpoint, you need to update the spec file.\")\n","print(\"use resume_model_path instead of pretrain_model_path with the checkpoint path you wish to resume from.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xv_8TGTGZFQd"},"outputs":[],"source":["print('Model for each epoch:')\n","print('---------------------')\n","!ls -ltrh $EXPERIMENT_DIR/experiment_dir_unpruned/weights"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":336,"status":"ok","timestamp":1657350130552,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"HT5Hld98ZFQe","outputId":"3cd94185-f702-4b53-9829-8a68f3a08d44"},"outputs":[],"source":["# You can check the evaluation stats in the csv file and pick the model with highest val accuracy.\n","!cat $EXPERIMENT_DIR/experiment_dir_unpruned/retinanet_training_log_resnet18.csv\n","%set_env EPOCH=100"]},{"cell_type":"markdown","metadata":{"id":"JwANlQILZFQe"},"source":["## 5. Evaluate trained models <a class=\"anchor\" id=\"head-5\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":95686,"status":"ok","timestamp":1657350285221,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"kaUgoDoNZFQe","outputId":"14b6aa19-410a-4c11-bd41-fcf509500df0","scrolled":true},"outputs":[],"source":["!tao retinanet evaluate -e $SPECS_DIR/retinanet_train_resnet18_kitti.txt \\\n","                        -m $EXPERIMENT_DIR/experiment_dir_unpruned/weights/retinanet_resnet18_epoch_$EPOCH.tlt \\\n","                        -k $KEY"]},{"cell_type":"markdown","metadata":{"id":"NzJ-2OsVZFQf"},"source":["## 6. Prune trained models <a class=\"anchor\" id=\"head-6\"></a>\n","* Specify pre-trained model\n","* Equalization criterion (`Only for resnets as they have element wise operations or MobileNets.`)\n","* Threshold for pruning.\n","* A key to save and load the model\n","* Output directory to store the model\n","\n","Usually, you just need to adjust `-pth` (threshold) for accuracy and model size trade off. Higher `pth` gives you smaller model (and thus higher inference speed) but worse accuracy. The threshold value depends on the dataset and the model. `0.4` in the block below is just a start point. If the retrain accuracy is good, you can increase this value to get smaller models. Otherwise, lower this value to get better accuracy."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":231,"status":"ok","timestamp":1657350285431,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"PEfSmpa1ZFQf"},"outputs":[],"source":["# Create an output directory to save the pruned model.\n","!mkdir -p $EXPERIMENT_DIR/experiment_dir_pruned"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":44506,"status":"ok","timestamp":1657350329933,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"mK5vGksEZFQf","outputId":"e9d79778-cce2-430e-9251-aa4f14ec444e","scrolled":true},"outputs":[],"source":["!tao retinanet prune -m $EXPERIMENT_DIR/experiment_dir_unpruned/weights/retinanet_resnet18_epoch_$EPOCH.tlt \\\n","               -o $EXPERIMENT_DIR/experiment_dir_pruned/retinanet_resnet18_pruned.tlt \\\n","               -pth 0.4 \\\n","               -k $KEY"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yqAZEuOkZFQf"},"outputs":[],"source":["!mkdir -p $EXPERIMENT_DIR/experiment_dir_pruned_qat\n","print(\"To prune a QAT model:\")\n","# !tao retinanet prune -m $USER_EXPERIMENT_DIR/experiment_dir_unpruned_qat/weights/retinanet_resnet18_epoch_$EPOCH.tlt \\\n","#                -o $USER_EXPERIMENT_DIR/experiment_dir_pruned_qat/retinanet_resnet18_pruned.tlt \\\n","#                -pth 0.4 \\\n","#                -k $KEY"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1657350329933,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"XpW_LkNMZFQf","outputId":"88711434-d188-475c-c2a6-18208f16ea1d"},"outputs":[],"source":["!ls -rlt $EXPERIMENT_DIR/experiment_dir_pruned/"]},{"cell_type":"markdown","metadata":{"id":"3h6sgGbaZFQf"},"source":["## 7. Retrain pruned models <a class=\"anchor\" id=\"head-7\"></a>\n","* Model needs to be re-trained to bring back accuracy after pruning\n","* Specify re-training specification\n","* WARNING: training will take several hours or one day to complete"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C0aep2C9ZFQf","scrolled":true},"outputs":[],"source":["# Printing the retrain spec file. \n","# Here we have updated the spec file to include the newly pruned model as a pretrained weights.\n","!sed -i \"s|YOUR_PRETRAINED_MODEL|$USER_EXPERIMENT_DIR/experiment_dir_pruned/retinanet_resnet18_pruned.tlt|g\" $SPECS_DIR/retinanet_retrain_resnet18_kitti.txt\n","!cat $SPECS_DIR/retinanet_retrain_resnet18_kitti.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":410,"status":"ok","timestamp":1657350330339,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"Bby9m5OAZFQg"},"outputs":[],"source":["!mkdir -p $EXPERIMENT_DIR/experiment_dir_retrain"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":258489,"status":"ok","timestamp":1657350863502,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"BhPIpK2oZFQg","outputId":"245b11b6-309e-46ef-a635-bcec2b470c05","scrolled":true},"outputs":[],"source":["# Retraining using the pruned model as pretrained weights.\n","print(\"To run with multigpu, please change --gpus based on the number of available GPUs in your machine.\")\n","!tao retinanet train --gpus 1 \\\n","                     -e $SPECS_DIR/retinanet_retrain_resnet18_kitti.txt \\\n","                     -r $EXPERIMENT_DIR/experiment_dir_retrain \\\n","                     -k $KEY"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wlK1KGX2ZFQg"},"outputs":[],"source":["!mkdir -p $EXPERIMENT_DIR/experiment_dir_retrain_qat\n","print(\"To run with QAT enabled, please uncomment and run the following command.\")\n","# !sed -i \"s/enable_qat: False/enable_qat: True/g\" $SPECS_DIR/retinanet_retrain_resnet18_kitti.txt\n","# !tao retinanet train --gpus 1 \\\n","#                      -e $SPECS_DIR/retinanet_retrain_resnet18_kitti.txt \\\n","#                      -r $USER_EXPERIMENT_DIR/experiment_dir_retrain_qat \\\n","#                      -k $KEY"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Na-xHgNgZFQg"},"outputs":[],"source":["# Listing the newly retrained model.\n","!ls -rlt $EXPERIMENT_DIR/experiment_dir_retrain/weights\n","# !ls -rlt $EXPERIMENT_DIR/experiment_dir_retrain_qat/weights"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":214,"status":"ok","timestamp":1657350919481,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"OWwJSTPmZFQg","outputId":"5c3f3a69-af99-4b45-ad22-df43d8cf1626"},"outputs":[],"source":["# You can check the evaluation stats in the csv file and pick the model with highest val accuracy.\n","!cat $EXPERIMENT_DIR/experiment_dir_retrain/retinanet_training_log_resnet18.csv\n","%set_env EPOCH=010"]},{"cell_type":"markdown","metadata":{"id":"eQAG58FZZFQg"},"source":["## 8. Evaluate retrained model <a class=\"anchor\" id=\"head-8\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":88647,"status":"ok","timestamp":1657351009273,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"GqA6pUBAZFQg","outputId":"f0c51388-ba6a-4b88-e0c7-6d8cc78807f7","scrolled":true},"outputs":[],"source":["!tao retinanet evaluate -e $SPECS_DIR/retinanet_retrain_resnet18_kitti.txt \\\n","                        -m $EXPERIMENT_DIR/experiment_dir_retrain/weights/retinanet_resnet18_epoch_$EPOCH.tlt \\\n","                        -k $KEY"]},{"cell_type":"markdown","metadata":{"id":"XwUu6mxkZFQg"},"source":["## 9. Visualize inferences <a class=\"anchor\" id=\"head-9\"></a>\n","In this section, we run the tlt-infer tool to generate inferences on the trained models and visualize the results."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":116098,"status":"ok","timestamp":1657351125358,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"-VDXzHiMZFQh","outputId":"3c3f8d86-0fb8-42cf-fe6c-6a88b3f7f9b0"},"outputs":[],"source":["# Running inference for detection on n images\n","!tao retinanet inference -i $DATA_DIR/testing/image_2 \\\n","                         -o $EXPERIMENT_DIR/retinanet_infer_images \\\n","                         -e $SPECS_DIR/retinanet_retrain_resnet18_kitti.txt \\\n","                         -m $EXPERIMENT_DIR/experiment_dir_retrain/weights/retinanet_resnet18_epoch_$EPOCH.tlt \\\n","                         -l $EXPERIMENT_DIR/retinanet_infer_labels \\\n","                         -k $KEY"]},{"cell_type":"markdown","metadata":{"id":"E4mj120GZFQh"},"source":["The `inference` tool produces two outputs. \n","1. Overlain images in `$EXPERIMENT_DIR/retinanet_annotated_images`\n","2. Frame by frame bbox labels in kitti format located in `$EXPERIMENT_DIR/retinanet_infer_labels`"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":497},"executionInfo":{"elapsed":12899,"status":"ok","timestamp":1657351138241,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"znLaYclKZFQh","outputId":"c71519f2-8600-4d69-c717-2e64fe47af5a"},"outputs":[],"source":["# Simple grid visualizer\n","!pip3 install matplotlib==3.3.3\n","import matplotlib.pyplot as plt\n","import os\n","from math import ceil\n","valid_image_ext = ['.jpg', '.png', '.jpeg', '.ppm']\n","\n","def visualize_images(image_dir, num_cols=1, num_images=1):\n","    output_path = os.path.join(os.environ['EXPERIMENT_DIR'], image_dir)\n","    num_rows = int(ceil(float(num_images) / float(num_cols)))\n","    f, axarr = plt.subplots(num_rows, num_cols, figsize=[80,30])\n","    f.tight_layout()\n","    a = [os.path.join(output_path, image) for image in os.listdir(output_path) \n","         if os.path.splitext(image)[1].lower() in valid_image_ext]\n","    for idx, img_path in enumerate(a[:num_images]):\n","        col_id = idx % num_cols\n","        row_id = idx // num_cols\n","        img = plt.imread(img_path)\n","        axarr[row_id, col_id].imshow(img) "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":291,"output_embedded_package_id":"1rqOxwb09k2ORTZc_N3UFaSdkDzAphjMU"},"executionInfo":{"elapsed":29197,"status":"ok","timestamp":1657351167972,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"Xy_450agZFQh","outputId":"6e96a889-1909-44e0-b734-a4e807a62f52","scrolled":true},"outputs":[],"source":["# Visualizing the sample images.\n","OUTPUT_PATH = 'retinanet_infer_images' # relative path from $EXPERIMENT_DIR.\n","COLS = 2 # number of columns in the visualizer grid.\n","IMAGES = 4 # number of images to visualize.\n","\n","visualize_images(OUTPUT_PATH, num_cols=COLS, num_images=IMAGES)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"retinanet.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.10.2 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.2"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":0}
