{"cells":[{"cell_type":"markdown","metadata":{"id":"LGLBrzF8hKgS"},"source":["## Switch to CPU Instance (Advisable only for Non Colab-Pro instance)\n","\n","1. Switch to CPU Instance for until Step 2 for non GPU dependent tasks\n","2. This increases your time available for the GPU dependent tasks on a Colab instance\n","2. Change Runtime type to CPU by Runtime(Top Left tab)->Change Runtime Type->None(Hardware Accelerator)\n","3.   Then click on Connect (Top Right)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SjpjyNg5c2V9"},"source":["## Mounting Google drive\n","Mount your Google drive storage to this Colab instance"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18056,"status":"ok","timestamp":1658620835786,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"EvUVkYw0hzqG","outputId":"a8f580a7-bd55-4a9c-8620-c0795752fbc9"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"markdown","metadata":{"id":"fXMv1eS1dMRn"},"source":["# Fiducial Points Estimation using TAO FPENet\n","\n","Transfer learning is the process of transferring learned features from one application to another. It is a commonly used training technique where you use a model trained on one task and re-train to use it on a different task. \n","\n","Train Adapt Optimize (TAO) Toolkit is a simple and easy-to-use Python based AI toolkit for taking purpose-built AI models and customizing them with users' own data.\n","\n","<img align=\"center\" src=\"https://developer.nvidia.com/sites/default/files/akamai/embedded-transfer-learning-toolkit-software-stack-1200x670px.png\" width=\"1080\"> "]},{"cell_type":"markdown","metadata":{"id":"PIIOID4XdMRp"},"source":["## Learning Objectives\n","In this notebook, you will learn how to leverage the simplicity and convenience of TAO to:\n","\n","* Take a pretrained model and train a FPENet model on AFW dataset\n","* Run Inference on the trained model\n","* Export the retrained model to a .etlt file for deployment to DeepStream SDK\n","\n","### Table of Contents\n","\n","This notebook shows an example of Fiducial Points Estimation using Train Adapt Optimize (TAO) Toolkit.\n","\n","0. [Set up env variables](#head-0)\n","1. [Prepare dataset and pre-trained model](#head-1) <br>\n","    1.1 [Verify downloaded dataset](#head-1-1) <br>\n","    1.2 [Download pre-trained model](#head-1-2) <br>\n","2. [Setup GPU environment](#head-2) <br>\n","    2.1 [Connect to GPU Instance](#head-2-1) <br>\n","    2.2 [Mounting Google drive](#head-2-2) <br>\n","    2.3 [Setup Python environment](#head-2-3) <br>\n","    2.4 [Reset env variables](#head-2-4) <br>\n","3. [Generate tfrecords from labels in json format](#head-3)\n","4. [Provide training specification](#head-4)\n","5. [Run TAO training](#head-5)\n","6. [Evaluate trained models](#head-6)\n","7. [Run inference for a set of images](#head-7)\n","8. [Deploy](#head-8)"]},{"cell_type":"markdown","metadata":{"id":"-kc7-X2NdMRq"},"source":["## 0. Set up env variables <a class=\"anchor\" id=\"head-0\"></a>\n","When using the purpose-built pretrained models from NGC, please make sure to set the `$KEY` environment variable to the key as mentioned in the model overview. Failing to do so, can lead to errors when trying to load them as pretrained models.\n","\n","*Note: This notebook currently is by default set up to run training using 1 GPU. To use more GPU's please update the env variable `$NUM_GPUS` accordingly*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CE5DDDPYdMRq"},"outputs":[],"source":["# Setting up env variables for cleaner command-line commands.\n","# Set this path if you don't run the notebook from the samples directory.\n","# %env NOTEBOOK_ROOT=~/tao-samples/fpenet\n","\n","%env KEY=nvidia_tlt\n","%env NUM_GPUS=1\n","%env EXPERIMENT_DIR=/results/fpenet\n","%env DATA_DIR=/content/drive/MyDrive/fpenet_data\n","# $SAMPLES_DIR is the path to the sample notebook folder and the dependency folder\n","# $SAMPLES_DIR/deps should exist for dependency installation\n","%env SPECS_DIR=/content/drive/MyDrive/ColabNotebooks/tensorflow/fpenet/specs\n","\n","# Showing list of specification files.\n","!ls -rlt $SPECS_DIR"]},{"cell_type":"markdown","metadata":{"id":"7Fxd3ut-dMRt"},"source":["## 1. Prepare dataset and pre-trained model <a class=\"anchor\" id=\"head-1\"></a>"]},{"cell_type":"markdown","metadata":{"id":"ef-qT_vQdMRt"},"source":["Download public dataset.\n","\n","Please download and unzip the AFW dataset to `$LOCAL_EXPERIMENT_DIR` directory.\n","\n","https://ibug.doc.ic.ac.uk/download/annotations/afw.zip/"]},{"cell_type":"markdown","metadata":{"id":"SZikZTiAdMRt"},"source":["### A. Download and Verify dataset <a class=\"anchor\" id=\"head-1-1\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zb1GbDbOdMRt"},"outputs":[],"source":["# Check the dataset is present\n","!if [ ! -d $DATA_DIR/afw ]; then echo 'Data folder not found, please download.'; else echo 'Found Data folder.';fi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pzwGvsezdMRt"},"outputs":[],"source":["# convert datset to required format\n","%cd /content/drive/MyDrive/ColabNotebooks/tensorflow/fpenet/\n","import os\n","from data_utils import convert_dataset\n","afw_data_path = os.path.join(os.environ[\"DATA_DIR\"], 'afw')\n","afw_image_save_path = os.path.join(os.environ[\"EXPERIMENT_DIR\"], 'afw')\n","\n","convert_dataset(afw_data_path, os.path.join(os.environ['DATA_DIR'], 'afw/afw.json'), afw_image_save_path)\n","# Note that we are using dummy labels for keypoints 69 to 80.\n","\n","print('Dataset conversion finished.')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RR7HJ0xfdMRu"},"outputs":[],"source":["# Check the dataset is generated\n","!if [ ! -f $DATA_DIR/afw/afw.json ]; then echo 'Labels not found, please regenerate.'; else echo 'Found Labels.';fi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IpKFqn6XdMRu"},"outputs":[],"source":["# Sample json label.\n","!sed -n 1,201p $DATA_DIR/afw/afw.json"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"389f5c_DdMRu"},"outputs":[],"source":["# Sample image.\n","import os\n","from IPython.display import Image\n","Image(filename=os.path.join(afw_data_path, '134212_1.png'))"]},{"cell_type":"markdown","metadata":{"id":"nNd5hotwdMRu"},"source":["### B. Obtain pre-trained model <a class=\"anchor\" id=\"head-1-2\"></a>\n","\n","Please follow the instructions in the following to download and verify the pretrain model for fpenet.\n","\n","For FpeNet pre-trained model please download model: `nvidia/tao/fpenet:trainable_v1.0`.\n","\n","After obtaining the pre-trained model, please place the model in $LOCAL_EXPERIMENT_DIR\n","\n","You will then have the following path-\n","\n","* pre-trained model in `$LOCAL_EXPERIMENT_DIR/pretrained_models/fpenet_vtrainable_v1.0/model.tlt`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-P79y9qbdMRu"},"outputs":[],"source":["# Installing NGC CLI on the local machine.\n","## Download and install\n","%env LOCAL_PROJECT_DIR=/content/\n","%env CLI=ngccli_cat_linux.zip\n","!mkdir -p $LOCAL_PROJECT_DIR/ngccli\n","\n","# Remove any previously existing CLI installations\n","!rm -rf $LOCAL_PROJECT_DIR/ngccli/*\n","!wget \"https://ngc.nvidia.com/downloads/$CLI\" -P $LOCAL_PROJECT_DIR/ngccli\n","!unzip -u -q \"$LOCAL_PROJECT_DIR/ngccli/$CLI\" -d $LOCAL_PROJECT_DIR/ngccli/\n","!rm $LOCAL_PROJECT_DIR/ngccli/*.zip \n","os.environ[\"PATH\"]=\"{}/ngccli/ngc-cli:{}\".format(os.getenv(\"LOCAL_PROJECT_DIR\", \"\"), os.getenv(\"PATH\", \"\"))\n","!cp /usr/lib/x86_64-linux-gnu/libstdc++.so.6 /content/ngccli/ngc-cli/libstdc++.so.6"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TBmZ1FDydMRv"},"outputs":[],"source":["# List models available in the model registry.\n","!ngc registry model list nvidia/tao/fpenet:*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ugy1m1hMdMRv"},"outputs":[],"source":["# Create the target destination to download the model.\n","!mkdir -p $EXPERIMENT_DIR/pretrained_models/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MBKQeOU8dMRv"},"outputs":[],"source":["# Download the pretrained model from NGC\n","!ngc registry model download-version nvidia/tao/fpenet:trainable_v1.0 \\\n","    --dest $EXPERIMENT_DIR/pretrained_models/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CaJxqprhdMRv"},"outputs":[],"source":["!ls -rlt $EXPERIMENT_DIR/pretrained_models/fpenet_vtrainable_v1.0 "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P7Dnl_r9dMRv"},"outputs":[],"source":["# Check the model is present\n","!if [ ! -f $EXPERIMENT_DIR/pretrained_models/fpenet_vtrainable_v1.0/model.tlt ]; then echo 'Pretrained model file not found, please download.'; else echo 'Found Pretrain model file.';fi"]},{"cell_type":"markdown","metadata":{"id":"_26rCobXcri1"},"source":["## 2. Setup GPU environment <a class=\"anchor\" id=\"head-2\"></a>\n"]},{"cell_type":"markdown","metadata":{"id":"k7Cx1_lMded7"},"source":["### 2.1 Connect to GPU Instance <a class=\"anchor\" id=\"head-2-1\"></a>\n","\n","1. Move any data saved to the Colab Instance storage to Google Drive  \n","2. Change Runtime type to GPU by Runtime(Top Left tab)->Change Runtime Type->GPU(Hardware Accelerator)\n","3.   Then click on Connect (Top Right)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"yl8BoM0Jhzh9"},"source":["### 2.2 Mounting Google drive <a class=\"anchor\" id=\"head-2-2\"></a>\n","Mount your Google drive storage to this Colab instance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vk2m-N4Nh0Sd"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"MBV_YWiTc_KM"},"source":["### 2.3 Setup Python environment <a class=\"anchor\" id=\"head-2-3\"></a>\n","Setup the environment necessary to run the TAO Networks by running the bash script"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s2Xygw-y8fjm"},"outputs":[],"source":["!sh /content/drive/MyDrive/ColabNotebooks/tensorflow/setup_env.sh"]},{"cell_type":"markdown","metadata":{"id":"Fl8fSfXseED3"},"source":["### 2.4 Reset env variables <a class=\"anchor\" id=\"head-2-4\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l_T2vBdzeIcO"},"outputs":[],"source":["# Setting up env variables for cleaner command line commands.\n","import os\n","\n","%env KEY=nvidia_tlt\n","%env NUM_GPUS=1\n","%env EXPERIMENT_DIR=/results/classification\n","%env DATA_DIR=/content/drive/MyDrive/tf_data/classification_data/\n","\n","# Set this path if you don't run the notebook from the samples directory.\n","# %env NOTEBOOK_ROOT=~/tao-samples/classification\n","\n","%env SPECS_DIR=/content/drive/MyDrive/ColabNotebooks/tensorflow/classification/specs\n","\n","# Showing list of specification files.\n","!ls -rlt $LOCAL_SPECS_DIR"]},{"cell_type":"markdown","metadata":{"id":"0bz6mpFudMRv"},"source":["## 3. Generate tfrecords from labels in json format <a class=\"anchor\" id=\"head-3\"></a>\n","* Create the tfrecords using the dataset_convert command\n","* Input is ground truth landmarks and output is tfrecord files"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Y6bqYZsdMRv"},"outputs":[],"source":["# Modify dataset_config for data preparation\n","# verify all paths\n","!cat $SPECS_DIR/dataset_config.yaml"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hRs-Uu5DdMRw"},"outputs":[],"source":["!ls $DATA_DIR/afw"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cUcXT-XSAXIu"},"outputs":[],"source":["!python3.6 -m pip install uff"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1657304566443,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"K-H1ceJyCLdC","outputId":"4ffc4ba0-0a3e-4a0b-ec86-2a65d791e68a"},"outputs":[],"source":["#!cp -r /content/drive/MyDrive/fpenet_data/afw /results/fpenet/\n","!ls /content/fpenet_data/data/tfrecords/afw/FpeTfRecords"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28323,"status":"ok","timestamp":1657304391419,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"Exp6JgAvdMRw","outputId":"ab807057-86b7-44d7-f2a7-28928b147a8d"},"outputs":[],"source":["!fpenet dataset_convert -e $SPECS_DIR/dataset_config.yaml"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":317,"status":"ok","timestamp":1657304512586,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"5ON-TGpldMRw","outputId":"f452d36d-c7c6-41c6-9393-7287dc59c12f"},"outputs":[],"source":["# check the tfrecords are generated\n","!if [ ! -d $EXPERIMENT_DIR/data/tfrecords/afw/FpeTfRecords ]; then echo 'Tfrecords folder not found, please generate.'; else echo 'Found Tfrecords folder.';fi"]},{"cell_type":"markdown","metadata":{"id":"4KRFC6P4dMRw"},"source":["## 4. Provide training specification <a class=\"anchor\" id=\"head-4\"></a>\n","* Tfrecords for the train datasets\n","    * In order to use the newly generated tfrecords for training, update the 'tfrecords_directory_path' and 'tfrecord_folder_name' parameters of 'dataset_info' section in the spec file at `$SPECS_DIR/experiment_spec.yaml`\n","* Pre-trained model path\n","    * Update \"pretrained_model_path\" in the spec file at `$SPECS_DIR/experiment_spec.yaml`\n","    * If you want to train from random weights with your own data, you can enter \"null\" for \"pretrained_model_path\" section\n","* Augmentation parameters for on the fly data augmentation\n","* Other training (hyper-)parameters such as batch size, number of epochs, learning rate etc."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"575vWwFLdMRx"},"outputs":[],"source":["!cat $SPECS_DIR/experiment_spec.yaml"]},{"cell_type":"markdown","metadata":{"id":"YXyru4PNdMRx"},"source":["## 5. Run TAO training <a class=\"anchor\" id=\"head-5\"></a>\n","* Provide the sample spec file and the output directory location for models\n","\n","*Note: The training may take hours to complete. Also, the remaining notebook, assumes that the training was done in single-GPU mode. \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23442,"status":"ok","timestamp":1657304846661,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"fqO5YUX7dMRy","outputId":"6e3f0c44-3de3-42e7-ab9f-686709475aca"},"outputs":[],"source":["!fpenet train -e $SPECS_DIR/experiment_spec.yaml \\\n","                  -r $EXPERIMENT_DIR/models/exp1 \\\n","                  -k $KEY"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8_6ouWM2dMRy"},"outputs":[],"source":["# check the training folder for generated files\n","!ls -lh $LOCAL_EXPERIMENT_DIR/models/exp1"]},{"cell_type":"markdown","metadata":{"id":"yxlD3oLWdMRy"},"source":["## 6. Evaluate the trained model <a class=\"anchor\" id=\"head-6\"></a>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O5XhVkk3dMRz"},"outputs":[],"source":["!tao fpenet evaluate  -m $USER_EXPERIMENT_DIR/models/exp1 \\\n","                      -k $KEY"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6-OlEfBIdMRz"},"outputs":[],"source":["# check the kpi predictions file is generated\n","!if [ ! -f $LOCAL_EXPERIMENT_DIR/models/exp1/kpi_testing_error_per_region.csv ]; then echo 'KPI results file not found!'; else cat $LOCAL_EXPERIMENT_DIR/models/exp1/kpi_testing_error_per_region.csv;fi\n","# Since keypoints 69 to 80 are dummy labels, error for pupil and ears would be high."]},{"cell_type":"markdown","metadata":{"id":"duNbUBL-dMRz"},"source":["## 7. Run inference on testing set <a class=\"anchor\" id=\"head-7\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7KNo6N4pdMRz"},"outputs":[],"source":["!tao fpenet inference -e $SPECS_DIR/experiment_spec.yaml \\\n","                      -i $SPECS_DIR/inference_sample.json \\\n","                      -r $LOCAL_PROJECT_DIR \\\n","                      -m $USER_EXPERIMENT_DIR/models/exp1/model.tlt \\\n","                      -o $USER_EXPERIMENT_DIR/models/exp1 \\\n","                      -k $KEY"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5WU7WLTldMRz"},"outputs":[],"source":["# check the results file is generated\n","!if [ ! -f $LOCAL_EXPERIMENT_DIR/models/exp1/result.txt ]; then echo 'Results file not found!'; else cat $LOCAL_EXPERIMENT_DIR/models/exp1/result.txt;fi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eqWB-6gSdMR0"},"outputs":[],"source":["import os\n","import cv2\n","import IPython.display\n","import PIL.Image\n","%matplotlib inline\n","# read results\n","results_file = os.path.join(os.environ['LOCAL_EXPERIMENT_DIR'], 'models/exp1/result.txt')\n","results = open(results_file, 'r').readlines()[0] # display one image as an example\n","\n","pred_part = results.strip().split(' ')\n","# get image path (append root path, if present)\n","image_path = pred_part[0].replace(os.environ[\"USER_EXPERIMENT_DIR\"], os.environ[\"LOCAL_EXPERIMENT_DIR\"])\n","# get predictions\n","fl_res = [float(x) for x in pred_part[1:]]\n","# read image\n","img = cv2.imread(image_path)\n","img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","# set color for landmarks\n","fl_color=(0,255,0)\n","# loop through keypoints and draw on image\n","for q in range(76): # not drawing ear points\n","    row_pred_x = fl_res[2*q]\n","    col_pred_y = fl_res[(2*q)+1]\n","    img_rgb = cv2.circle(img_rgb,(int(row_pred_x), int(col_pred_y)), 1, fl_color, 1)\n","# display image\n","IPython.display.display(PIL.Image.fromarray(img_rgb))\n","# Note that the accuracy is not gauranteed for this visualization example."]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["Xw2BvEkndMR0","xrBfleDQdMR1","8PuJBA-gdMR1","0Z3wDI1fdMR1"],"name":"fpenet.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.10.2 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.2"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":0}
