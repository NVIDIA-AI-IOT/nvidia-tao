{"cells":[{"cell_type":"markdown","metadata":{"id":"LGLBrzF8hKgS"},"source":["## Switch to CPU Instance (Advisable only for Non Colab-Pro instance)\n","\n","1. Switch to CPU Instance for until Step 2 for non GPU dependent tasks\n","2. This increases your time available for the GPU dependent tasks on a Colab instance\n","2. Change Runtime type to CPU by Runtime(Top Left tab)->Change Runtime Type->None(Hardware Accelerator)\n","3.   Then click on Connect (Top Right)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SjpjyNg5c2V9"},"source":["## Mounting Google drive\n","Mount your Google drive storage to this Colab instance"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23648,"status":"ok","timestamp":1657314733379,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"JdYQfEbUpZuM","outputId":"265bd42d-cf89-476f-99f4-43d156c46a92"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"markdown","metadata":{"id":"d8aFis_6kGs3"},"source":["# Gesture Classification using TAO GestureNet\n","\n","Transfer learning is the process of transferring learned features from one application to another. It is a commonly used training technique where you use a model trained on one task and re-train to use it on a different task. \n","\n","Train Adapt Optimize (TAO) Toolkit is a simple and easy-to-use Python based AI toolkit for taking purpose-built AI models and customizing them with users' own data.\n","\n","<img align=\"center\" src=\"https://developer.nvidia.com/sites/default/files/akamai/embedded-transfer-learning-toolkit-software-stack-1200x670px.png\" width=\"1080\"> "]},{"cell_type":"markdown","metadata":{"id":"6X_ABJVpkGs5"},"source":["## Learning Objectives\n","\n","In this notebook, you will learn how to leverage the simplicity and convenience of TAO to:\n","\n","* Take a pretrained model and train a GestureNet model on HGR dataset\n","* Run Inference on the trained model\n","* Export the retrained model to a .etlt file for deployment to DeepStream SDK\n","\n","### Table of Contents\n","\n","This notebook shows an example of classifying gestures using GestureNet in the Train Adapt Optimize (TAO) Toolkit.\n","\n","0. [Set up env variables](#head-0)\n","1. [Prepare dataset and pre-trained model](#head-1) <br>\n","    A. [Verify and prepare dataset](#head-1-1) <br>\n","    B. [Generate hand crops and dataset json](#head-1-2) <br>\n","    C. [Download pre-trained model](#head-1-3) <br>\n","2. [Setup GPU environment](#head-2) <br>\n","    2.1 [Connect to GPU Instance](#head-2-1) <br>\n","    2.2 [Mounting Google drive](#head-2-2) <br>\n","    2.3 [Setup Python environment](#head-2-3) <br>\n","    2.4 [Reset env variables](#head-2-4) <br>\n","3. [Provide training specification](#head-3) <br>\n","4. [Run TAO training](#head-4) <br>\n","5. [Evaluate the trained model](#head-5) <br>\n","6. [Export](#head-6) <br>\n","7. [Inference](#head-7) <br>"]},{"cell_type":"markdown","metadata":{"id":"CSSZS-JrkGs5"},"source":["## 0. Set up env variables <a class=\"anchor\" id=\"head-0\"></a>\n","When using the purpose-built pretrained models from NGC, please make sure to set the `$KEY` environment variable to the key as mentioned in the model overview. Failing to do so, can lead to errors when trying to load them as pretrained models.\n","\n","*Note: This notebook currently is by default set up to run training using 1 GPU. To use more GPU's please update the env variable `$NUM_GPUS` accordingly*"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1890,"status":"ok","timestamp":1657318493153,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"tvYoHB6qkGs6","outputId":"8f503429-c065-4799-d9e1-dd13865c4573"},"outputs":[],"source":["# Setting up env variables for cleaner command-line commands.\n","import os\n","\n","%env KEY=nvidia_tlt\n","%env NUM_GPUS=1\n","\n","# Change the paths according to your directory structure\n","%env EXPERIMENT_DIR=/results/gesturenet\n","%env DATA_DIR=/content/drive/MyDrive/gesturenet_data\n","%env SPECS_DIR=/content/drive/MyDrive/ColabNotebooks/tensorflow/gesturenet/specs\n","\n","# Showing list of specification files.\n","!ls -rlt $SPECS_DIR"]},{"cell_type":"markdown","metadata":{"id":"_f8VVY4QkGs-"},"source":["## 1. Prepare dataset and pre-trained model <a class=\"anchor\" id=\"head-1\"></a>"]},{"cell_type":"markdown","metadata":{"id":"ucAKz9f2kGs-"},"source":["We will be using the database for hand gesture recognition (HGR) for the tutorial. To find more details, please visit http://sun.aei.polsl.pl/~mkawulok/gestures/. Please download the HGR1 [images](http://sun.aei.polsl.pl/~mkawulok/gestures/hgr1_images.zip), [feature points](http://sun.aei.polsl.pl/~mkawulok/gestures/hgr1_feature_pts.zip) and HGR2B [images](http://sun.aei.polsl.pl/~mkawulok/gestures/hgr2b_images.zip), [feature points](http://sun.aei.polsl.pl/~mkawulok/gestures/hgr2b_feature_pts.zip) and place the zip files in `$DATA_DIR`. "]},{"cell_type":"markdown","metadata":{"id":"Ycu0T0PokGs-"},"source":["### A. Verify and prepare dataset <a class=\"anchor\" id=\"head-1-1\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":340,"status":"ok","timestamp":1657318780524,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"Tyg80GlskGs-","outputId":"6523dc5e-965e-415f-9095-83b9cb3b9db0"},"outputs":[],"source":["# Check the zip files are present.\n","!if [ ! -f $DATA_DIR/hgr1_images.zip ]; then echo 'hgr1_images zip file not found, please download.'; else echo 'Found hgr1_images zip file.';fi\n","!if [ ! -f $DATA_DIR/hgr1_feature_pts.zip ]; then echo 'hgr1_feature_pts zip file not found, please download.'; else echo 'Found hgr1_feature_pts zip file.';fi\n","!if [ ! -f $DATA_DIR/hgr2b_images.zip ]; then echo 'hgr2b_images zip file not found, please download.'; else echo 'Found hgr2b_images zip file.';fi\n","!if [ ! -f $DATA_DIR/hgr2b_feature_pts.zip ]; then echo 'hgr2b_feature_pts zip file not found, please download.'; else echo 'Found hgr2b_feature_pts zip file.';fi"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30007,"status":"ok","timestamp":1657318810528,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"XDQ0U8KAkGs_","outputId":"142cea52-8191-42d5-92f2-a3871c0ac58f"},"outputs":[],"source":["# unpack downloaded datasets to $DATA_DOWNLOAD_DIR.\n","# The images will be under $DATA_DOWNLOAD_DIR/original_images and $DATA_DOWNLOAD_DIR/feature_points\n","!unzip -u ${DATA_DIR}/hgr1_images.zip -d ${DATA_DIR}\n","!unzip -u ${DATA_DIR}/hgr1_feature_pts.zip -d ${DATA_DIR}\n","!unzip -u ${DATA_DIR}/hgr2b_images.zip -d ${DATA_DIR}\n","!unzip -u ${DATA_DIR}/hgr2b_feature_pts.zip -d ${DATA_DIR}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rS9TBfT05czk"},"outputs":[],"source":["!rm -rf /content/drive/MyDrive/gesturenet_data/*.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4909,"status":"ok","timestamp":1657318835537,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"ZiFNchzZkGs_","outputId":"a193b637-8135-4aff-809b-c8e30d8ceab6"},"outputs":[],"source":["# Convert dataset to required format for gesturenet dataset_convert\n","%cd /content/drive/MyDrive/ColabNotebooks/tensorflow/gesturenet\n","!python3.6 convert_hgr_to_tlt_data.py --input_image_dir=$DATA_DIR/original_images \\\n","                                      --input_label_file=$DATA_DIR/feature_points \\\n","                                      --output_dir=$EXPERIMENT_DIR"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1657318874970,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"FVRv9Aj2kGs_","outputId":"1f452797-e3fc-4fc6-dd8e-864bf445b2ec"},"outputs":[],"source":["# verify\n","import os\n","\n","EXPERIMENT_DIR = os.environ.get('EXPERIMENT_DIR')\n","num_labels = len(os.listdir(os.path.join(EXPERIMENT_DIR, \"original/data/annotation\")))\n","print(\"Number of labels in the dataset. {}\".format(num_labels))"]},{"cell_type":"markdown","metadata":{"id":"kOJMAQw8kGtA"},"source":["### B. Generate hand crops and dataset json <a class=\"anchor\" id=\"head-1-2\"></a>\n","\n","* Update the `dataset_config.json` and `dataset_experiment_config.json` spec files\n","* Create the crop and json using the gesturenet dataset_convert \n","\n","*Note: Crops and dataset json only need to be generated once.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TksyG5X8kGtA"},"outputs":[],"source":["print(\"Hand crop generation spec file\")\n","!cat $SPECS_DIR/dataset_config.json"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sKm5b3CTkGtA"},"outputs":[],"source":["print(\"Dataset experiment spec file\")\n","!cat $SPECS_DIR/dataset_experiment_config.json"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15798,"status":"ok","timestamp":1657320864345,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"nMudS405kGtA","outputId":"3920cc1f-a085-4969-eff6-0e4a0149d973"},"outputs":[],"source":["!tao gesturenet dataset_convert --dataset_spec $SPECS_DIR/dataset_config.json \\\n","                                --k_folds 0 \\\n","                                --experiment_spec $SPECS_DIR/dataset_experiment_config.json \\\n","                                --output_filename $EXPERIMENT_DIR/data.json \\\n","                                --experiment_name v1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1657319550897,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"PfYW5IaGkGtA","outputId":"e4ed7c8d-ed7f-41ba-c590-a3f0a6f97341"},"outputs":[],"source":["# Check to see if proper json file is generated.\n","!if [ ! -f $EXPERIMENT_DIR/data.json ]; then echo \"Json file was not generated properly.\"; else echo \"Json was generated properly.\"; fi"]},{"cell_type":"markdown","metadata":{"id":"bg6AC5cEkGtB"},"source":["### C. Download pre-trained model <a class=\"anchor\" id=\"head-1-3\"></a>\n","\n","Please follow the instructions in the following to download and verify the pretrained model for gesturenet.\n","\n","For FpeNet pretrained model please download model: `nvidia/tao/gesturenet:trainable_v1.0`.\n","\n","After obtaining the pre-trained model, please place the model in $EXPERIMENT_DIR\n","\n","You will have the following path-\n","\n","* pretrained model in `$EXPERIMENT_DIR/pretrained_models/gesturenet_vtrainable_v1.0/model.tlt`"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2643,"status":"ok","timestamp":1657319570079,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"UnhyzNO-kGtB","outputId":"f5b78671-8c66-4607-fad6-ee2d9b6df3c4"},"outputs":[],"source":["# Installing NGC CLI on the local machine.\n","## Download and install\n","%env LOCAL_PROJECT_DIR=/content/\n","%env CLI=ngccli_cat_linux.zip\n","!mkdir -p $LOCAL_PROJECT_DIR/ngccli\n","\n","# Remove any previously existing CLI installations\n","!rm -rf $LOCAL_PROJECT_DIR/ngccli/*\n","!wget \"https://ngc.nvidia.com/downloads/$CLI\" -P $LOCAL_PROJECT_DIR/ngccli\n","!unzip -u -q \"$LOCAL_PROJECT_DIR/ngccli/$CLI\" -d $LOCAL_PROJECT_DIR/ngccli/\n","!rm $LOCAL_PROJECT_DIR/ngccli/*.zip \n","os.environ[\"PATH\"]=\"{}/ngccli/ngc-cli:{}\".format(os.getenv(\"LOCAL_PROJECT_DIR\", \"\"), os.getenv(\"PATH\", \"\"))\n","!cp /usr/lib/x86_64-linux-gnu/libstdc++.so.6 $LOCAL_PROJECT_DIR/ngccli/ngc-cli/libstdc++.so.6"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5340,"status":"ok","timestamp":1657319577601,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"V3xF0wQRkGtB","outputId":"7e348538-67a0-433b-e074-57f359e3cf72"},"outputs":[],"source":["# List models available in the model registry.\n","!ngc registry model list nvidia/tao/gesturenet:*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0zaXOGk7kGtB"},"outputs":[],"source":["# Create the target destination to download the model.\n","!mkdir -p $EXPERIMENT_DIR/pretrained_models/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16942,"status":"ok","timestamp":1657319598002,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"85j0_Z9VkGtC","outputId":"46025628-a435-40a4-b30c-83c68be0305e"},"outputs":[],"source":["# Download the pretrained model from NGC\n","!ngc registry model download-version nvidia/tao/gesturenet:trainable_v1.0 \\\n","    --dest $EXPERIMENT_DIR/pretrained_models/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":38,"status":"ok","timestamp":1657319598003,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"aL4Ae3cGkGtC","outputId":"bceafbb2-da63-4e90-c632-8aa895c8b8ed"},"outputs":[],"source":["!ls -rlt $EXPERIMENT_DIR/pretrained_models/gesturenet_vtrainable_v1.0 "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1657319598003,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"tm2PZr3ukGtC","outputId":"2a4a42fd-b5d0-447e-fc06-68bc250fd4ee"},"outputs":[],"source":["# Check the model is present\n","!if [ ! -f $EXPERIMENT_DIR/pretrained_models/gesturenet_vtrainable_v1.0/model.tlt ]; then echo 'Pretrained model file not found, please download.'; else echo 'Found Pretrain model file.';fi"]},{"cell_type":"markdown","metadata":{"id":"_26rCobXcri1"},"source":["## 2. Setup GPU environment <a class=\"anchor\" id=\"head-2\"></a>\n"]},{"cell_type":"markdown","metadata":{"id":"k7Cx1_lMded7"},"source":["### 2.1 Connect to GPU Instance <a class=\"anchor\" id=\"head-2-1\"></a>\n","\n","1. Move any data saved to the Colab Instance storage to Google Drive  \n","2. Change Runtime type to GPU by Runtime(Top Left tab)->Change Runtime Type->GPU(Hardware Accelerator)\n","3.   Then click on Connect (Top Right)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"yl8BoM0Jhzh9"},"source":["### 2.2 Mounting Google drive <a class=\"anchor\" id=\"head-2-2\"></a>\n","Mount your Google drive storage to this Colab instance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vk2m-N4Nh0Sd"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"MBV_YWiTc_KM"},"source":["### 2.3 Setup Python environment <a class=\"anchor\" id=\"head-2-3\"></a>\n","Setup the environment necessary to run the TAO Networks by running the bash script"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s2Xygw-y8fjm"},"outputs":[],"source":["!sh /content/drive/MyDrive/ColabNotebooks/tensorflow/setup_env.sh"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","os.environ[\"PYTHONPATH\"]+=\":/opt/nvidia/\"\n","os.environ[\"PYTHONPATH\"]+=\":/usr/local/lib/python3.6/dist-packages/third_party/nvml\""]},{"cell_type":"markdown","metadata":{"id":"Fl8fSfXseED3"},"source":["### 2.4 Reset env variables <a class=\"anchor\" id=\"head-2-4\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l_T2vBdzeIcO"},"outputs":[],"source":["# Setting up env variables for cleaner command-line commands.\n","import os\n","\n","%env KEY=nvidia_tlt\n","%env NUM_GPUS=1\n","\n","# Change the paths according to your directory structure\n","%env EXPERIMENT_DIR=/results/gesturenet\n","%env DATA_DIR=/content/drive/MyDrive/gesturenet_data\n","%env SPECS_DIR=/content/drive/MyDrive/ColabNotebooks/tensorflow/gesturenet/specs\n","\n","# Showing list of specification files.\n","!ls -rlt $SPECS_DIR"]},{"cell_type":"markdown","metadata":{"id":"YhZtE_bPkGtC"},"source":["## 3. Provide training specification <a class=\"anchor\" id=\"head-3\"></a>"]},{"cell_type":"markdown","metadata":{"id":"JMi1PwNWkGtC"},"source":["* Dataset configuration\n","    * In order to load the data properly, you will need to change the `dataset:data_path` to the generated `json` (folder and file) file generated in part B above. By default it is located at `$SPECS_DIR/data.json`\n","    * Update number of classes and class number to name map\n","* Pre-trained models. There is an optional parameter to load head of model. Only set `add_new_head: false` if you want to finetune on dataset with same gestures as pretrained model. Please ensure the gesture class to index map matches pretrained model.\n","* Augmentation parameters for on the fly data augmentation\n","* Other training (hyper-)parameters such as batch size, number of epochs, learning rate etc.|"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"961gu_EDkGtE"},"outputs":[],"source":["!cat $SPECS_DIR/train_spec.json"]},{"cell_type":"markdown","metadata":{"id":"m86CBsM8kGtE"},"source":["## 4. Run TAO training <a class=\"anchor\" id=\"head-4\"></a>\n","* Provide the sample spec file and the encryption key"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":164867,"status":"ok","timestamp":1657321088940,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"DUO52ukVkGtF","outputId":"9f88ea72-78bf-4333-dfb7-f5521d9df313"},"outputs":[],"source":["!tao gesturenet train -e $SPECS_DIR/train_spec.json \\\n","                      -k $KEY"]},{"cell_type":"markdown","metadata":{"id":"c60hDsGpkGtF"},"source":["## 5. Evaluate the trained model <a class=\"anchor\" id=\"head-5\"></a>\n","\n","* Please update model path to location trained model is saved at"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17353,"status":"ok","timestamp":1657320375342,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"xXESDjIBkGtG","outputId":"3c4b0ca6-2104-4266-ae4c-f0ebbd05dc37"},"outputs":[],"source":["!tao gesturenet evaluate -e $EXPERIMENT_DIR/model/train_spec.json \\\n","                         -m $EXPERIMENT_DIR/model/model.tlt \\\n","                         -k $KEY"]},{"cell_type":"markdown","metadata":{"id":"hjuGqbTQkGtG"},"source":["## 6. Inference <a class=\"anchor\" id=\"head-7\"></a>\n","In this section, we run the `gesturenet inference` tool to generate inferences on the trained models. Please ensure the spec file `inference.json` is configured correctly. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16521,"status":"ok","timestamp":1657320433571,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"SSwNHtK3kGtG","outputId":"dc0f8a4f-eda7-4989-8491-8f6eed267b85"},"outputs":[],"source":["!tao gesturenet inference -e $EXPERIMENT_DIR/model/train_spec.json \\\n","                          -m $EXPERIMENT_DIR/model/model.tlt \\\n","                          -k $KEY \\\n","                          --image_root_path $EXPERIMENT_DIR \\\n","                          --data_json $EXPERIMENT_DIR/data.json \\\n","                          --data_type kpi_set \\\n","                          --results_dir $EXPERIMENT_DIR/model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":511},"executionInfo":{"elapsed":363,"status":"error","timestamp":1657320433926,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"aGFcPzPykGtG","outputId":"b5171fd6-6dc5-4957-e250-e515c55a3bf6"},"outputs":[],"source":["import os\n","import cv2\n","import IPython.display\n","import json\n","import PIL.Image\n","\n","json_spec_path = os.path.join(os.environ.get('EXPERIMENT_DIR'), 'data.json')\n","data_type = \"kpi_set\"\n","result_file = os.path.join(os.environ.get('EXPERIMENT_DIR'), 'model/results.txt')\n","model_spec_path = os.path.join(os.environ.get('EXPERIMENT_DIR'), 'model/train_spec.json')\n","\n","# Read in json spec.\n","with open(json_spec_path, 'r') as file:\n","    full_spec = json.load(file)\n","spec = full_spec[data_type]\n","\n","# Read in model spec.\n","with open(model_spec_path, 'r') as file:\n","    model_spec = json.load(file)\n","\n","class_labels = model_spec['dataset']['classes']\n","\n","results = open(result_file, 'r')\n","\n","images = spec['images']\n","\n","for image_dict in images:\n","\n","    image_path = os.path.join(os.environ.get('EXPERIMENT_DIR'), image_dict['full_image_path'])\n","    bbox = image_dict['bbox_coordinates']\n","    image = cv2.imread(image_path)\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","    # Get corners of rectangle.\n","    upper_left = tuple(bbox[0])\n","    bottom_right = tuple(bbox[3])\n","    # draw rectangle onto image.\n","    cv2.rectangle(image, upper_left, bottom_right, (0, 255, 0), 2)\n","\n","    if image is None:\n","        results.readline()\n","        continue\n","    image_result = results.readline()\n","    prediction = image_result.split(' ')[1]\n","    # Get class label.\n","    label = list(class_labels.keys())[list(class_labels.values()).index(int(prediction))]\n","    # Get bottom right corner.\n","    x = 0\n","    y = image.shape[0]-5\n","    # Display Image.\n","    image = cv2.putText(image, label, (x, y), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (255, 24, 8))\n","    IPython.display.display(PIL.Image.fromarray(image))"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"gesturenet.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.10.2 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.2"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":0}
