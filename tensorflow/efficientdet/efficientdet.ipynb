{"cells":[{"cell_type":"markdown","metadata":{"id":"LGLBrzF8hKgS"},"source":["## Switch to CPU Instance (Advisable only for Non Colab-Pro instance)\n","\n","1. Switch to CPU Instance for until Step 2 for non GPU dependent tasks\n","2. This increases your time available for the GPU dependent tasks on a Colab instance\n","2. Change Runtime type to CPU by Runtime(Top Left tab)->Change Runtime Type->None(Hardware Accelerator)\n","3.   Then click on Connect (Top Right)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SjpjyNg5c2V9"},"source":["## Mounting Google drive\n","Mount your Google drive storage to this Colab instance"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18056,"status":"ok","timestamp":1658620835786,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"EvUVkYw0hzqG","outputId":"a8f580a7-bd55-4a9c-8620-c0795752fbc9"},"outputs":[],"source":["try:\n","    import google.colab\n","    %env GOOGLE_COLAB=1\n","    from google.colab import drive\n","    drive.mount('/content/drive', force_remount=True)\n","except:\n","    %env GOOGLE_COLAB=0\n","    print(\"Warning: Not a Colab Environment\")"]},{"cell_type":"markdown","metadata":{"id":"No669Jw33h_m"},"source":["# Object Detection using TAO EfficientDet\n","\n","Transfer learning is the process of transferring learned features from one application to another. It is a commonly used training technique where you use a model trained on one task and re-train to use it on a different task. \n","\n","Train Adapt Optimize (TAO) Toolkit  is a simple and easy-to-use Python based AI toolkit for taking purpose-built AI models and customizing them with users' own data.\n","\n","<img align=\"center\" src=\"https://developer.nvidia.com/sites/default/files/akamai/embedded-transfer-learning-toolkit-software-stack-1200x670px.png\" width=\"1080\"> "]},{"cell_type":"markdown","metadata":{"id":"-WChP8tS3h_o"},"source":["## Learning Objectives\n","In this notebook, you will learn how to leverage the simplicity and convenience of TAO to:\n","\n","* Take a pretrained model and train an EfficientDet-D0 model on COCO dataset\n","* Evaluate the trained model\n","* Run pruning and finetuning with the trained model\n","* Run inference with the trained model and visualize the result\n","* Export the trained model to a .etlt file for deployment to DeepStream\n","\n","### Table of Contents\n","This notebook shows an example use case for instance segmentation using the Train Adapt Optimize (TAO) Toolkit.\n","\n","0. [Set up env variables](#head-0)\n","1. [Prepare dataset and pre-trained model](#head-1)\n","2. [Setup GPU environment](#head-2) <br>\n","    2.1 [Connect to GPU Instance](#head-2-1) <br>\n","    2.2 [Mounting Google drive](#head-2-2) <br>\n","    2.3 [Setup Python environment](#head-2-3) <br>\n","    2.4 [Reset env variables](#head-2-4) <br>\n","3. [Generate tfrecords](#head-3)\n","3. [Provide training specification](#head-4)\n","4. [Run TAO training](#head-5)\n","5. [Evaluate trained models](#head-6)\n","6. [Prune trained model](#head-7)\n","7. [Retrain pruned models](#head-8)\n","8. [Evaluate retrained model](#head-9)\n","9. [Visualize inferences](#head-10)"]},{"cell_type":"markdown","metadata":{"id":"ovUusYmU3h_p"},"source":["## 0. Set up env variables and set FIXME parameters <a class=\"anchor\" id=\"head-0\"></a>\n","\n","*Note: This notebook currently is by default set up to run training using 1 GPU. To use more GPU's please update the env variable `$NUM_GPUS` accordingly*\n","\n","#### FIXME\n","1. NUM_GPUS - set this to <= number of GPU's availble on the instance\n","1. EXPERIMENT_DIR - set this path to a folder location where pretrained models, checkpoints and log files during different model actions will be saved\n","1. delete_existing_experiments - set to True to remove existing pretrained models, checkpoints and log files of a previous experiment\n","1. DATA_DIR - set this path to a folder location where you want to dataset to be present\n","1. delete_existing_data - set this to True to remove existing preprocessed and original data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":833,"status":"ok","timestamp":1657304634635,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"zyDaFvJm3h_p","outputId":"292f5e53-833c-46dd-9318-6c2f42a4e779"},"outputs":[],"source":["# Setting up env variables for cleaner command line commands.\n","import os\n","\n","%env TAO_DOCKER_DISABLE=1\n","\n","%env KEY=nvidia_tlt\n","#FIXME1\n","%env NUM_GPUS=1\n","\n","# Change the paths according to your directory structure, these are just examples\n","%env COLAB_NOTEBOOKS_PATH=/content/drive/MyDrive/ColabNotebooks\n","if not os.path.exists(os.environ[\"COLAB_NOTEBOOKS_PATH\"]):\n","    raise(\"Error, enter the path of the colab notebooks repo correctly\")\n","\n","#FIXME2\n","%env EXPERIMENT_DIR=/content/drive/MyDrive/results/efficientdet\n","#FIXME3\n","delete_existing_experiments = True\n","#FIXME4\n","%env DATA_DIR=/content/drive/MyDrive/coco_data/\n","#FIXME5\n","delete_existing_data = False\n","\n","if delete_existing_experiments:\n","    !sudo rm -rf $EXPERIMENT_DIR\n","if delete_existing_data:\n","    !sudo rm -rf $DATA_DIR\n","\n","SPECS_DIR=f\"{os.environ['COLAB_NOTEBOOKS_PATH']}/tensorflow/efficientdet/specs\"\n","%env SPECS_DIR={SPECS_DIR}\n","# Showing list of specification files.\n","!ls -rlt $SPECS_DIR\n","\n","!sudo mkdir -p $DATA_DIR && sudo chmod -R 777 $DATA_DIR\n","!sudo mkdir -p $EXPERIMENT_DIR && sudo chmod -R 777 $EXPERIMENT_DIR"]},{"cell_type":"markdown","metadata":{"id":"yejbCUAF3h_s"},"source":["## 1. Prepare dataset and pre-trained model <a class=\"anchor\" id=\"head-1\"></a>"]},{"cell_type":"markdown","metadata":{"id":"jH1LV9cJ3h_s"},"source":[" We will be using the COCO dataset for the tutorial. The following script will download COCO dataset automatically and convert it to TFRecords. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TRmPIwhJ3h_t","scrolled":true},"outputs":[],"source":["# Download and preprocess data\n","!bash $SPECS_DIR/download_coco.sh $DATA_DIR"]},{"cell_type":"markdown","metadata":{"id":"njTeq1pO3h_t"},"source":["Note that the dataset conversion scripts provided in `specs` are intended for the standard COCO dataset. If your data doesn't have `caption` groundtruth or test set, you can modify `download_and_preprocess_coco.sh` and `create_coco_tf_record.py` by commenting out corresponding variables."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A6d2FIkJ3h_t"},"outputs":[],"source":["# verify\n","!ls -l $DATA_DIR"]},{"cell_type":"markdown","metadata":{"id":"XkeE2ptP3h_t"},"source":["### Download pretrained model from NGC"]},{"cell_type":"markdown","metadata":{"id":"wfMlnYMX3h_t"},"source":[" We will use NGC CLI to get the pre-trained models. For more details, go to ngc.nvidia.com and click the SETUP on the navigation bar."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2391,"status":"ok","timestamp":1657304645567,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"p5Y7liwi3h_u","outputId":"47b33efd-0c6e-40a0-de85-fcccda15a701"},"outputs":[],"source":["# Installing NGC CLI on the local machine.\n","## Download and install\n","%env LOCAL_PROJECT_DIR=/ngc_content/\n","%env CLI=ngccli_cat_linux.zip\n","!sudo mkdir -p $LOCAL_PROJECT_DIR/ngccli && sudo chmod -R 777 $LOCAL_PROJECT_DIR\n","\n","# Remove any previously existing CLI installations\n","!sudo rm -rf $LOCAL_PROJECT_DIR/ngccli/*\n","!wget \"https://ngc.nvidia.com/downloads/$CLI\" -P $LOCAL_PROJECT_DIR/ngccli\n","!unzip -u -q \"$LOCAL_PROJECT_DIR/ngccli/$CLI\" -d $LOCAL_PROJECT_DIR/ngccli/\n","!rm $LOCAL_PROJECT_DIR/ngccli/*.zip \n","os.environ[\"PATH\"]=\"{}/ngccli/ngc-cli:{}\".format(os.getenv(\"LOCAL_PROJECT_DIR\", \"\"), os.getenv(\"PATH\", \"\"))\n","!cp /usr/lib/x86_64-linux-gnu/libstdc++.so.6 $LOCAL_PROJECT_DIR/ngccli/ngc-cli/libstdc++.so.6"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5216,"status":"ok","timestamp":1657304658105,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"csMoOQ0R3h_u","outputId":"2c0603bd-a0d8-4148-c930-41a5cd1381b2"},"outputs":[],"source":["!ngc registry model list nvidia/tao/pretrained_efficientdet:efficientnet_b0*"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11370,"status":"ok","timestamp":1657304669468,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"7o9r3bwk3h_u","outputId":"5e612650-f1fe-4196-a540-ab343d511a9e"},"outputs":[],"source":["# Pull pretrained model from NGC\n","!ngc registry model download-version nvidia/tao/pretrained_efficientdet:efficientnet_b0 --dest $EXPERIMENT_DIR"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1657304669468,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"4BNnwm063h_u","outputId":"f648361e-0984-40c7-a3ec-24a1ffb829e2"},"outputs":[],"source":["print(\"Check that model is downloaded into dir.\")\n","!ls -l $EXPERIMENT_DIR/pretrained_efficientdet_vefficientnet_b0"]},{"cell_type":"markdown","metadata":{"id":"_26rCobXcri1"},"source":["## 2. Setup GPU environment <a class=\"anchor\" id=\"head-2\"></a>\n"]},{"cell_type":"markdown","metadata":{"id":"k7Cx1_lMded7"},"source":["### 2.1 Connect to GPU Instance <a class=\"anchor\" id=\"head-2-1\"></a>\n","\n","1. Move any data saved to the Colab Instance storage to Google Drive  \n","2. Change Runtime type to GPU by Runtime(Top Left tab)->Change Runtime Type->GPU(Hardware Accelerator)\n","3.   Then click on Connect (Top Right)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"yl8BoM0Jhzh9"},"source":["### 2.2 Mounting Google drive <a class=\"anchor\" id=\"head-2-2\"></a>\n","Mount your Google drive storage to this Colab instance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vk2m-N4Nh0Sd"},"outputs":[],"source":["try:\n","    import google.colab\n","    %env GOOGLE_COLAB=1\n","    from google.colab import drive\n","    drive.mount('/content/drive', force_remount=True)\n","except:\n","    %env GOOGLE_COLAB=0\n","    print(\"Warning: Not a Colab Environment\")"]},{"cell_type":"markdown","metadata":{"id":"MBV_YWiTc_KM"},"source":["### 2.3 Setup Python environment <a class=\"anchor\" id=\"head-2-3\"></a>\n","Setup the environment necessary to run the TAO Networks by running the bash script"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s2Xygw-y8fjm"},"outputs":[],"source":["#FIXME\n","%env GENERAL_WHL_PATH=/content/drive/MyDrive/tf/general_whl\n","#FIXME\n","%env CODEBASE_WHL_PATH=/content/drive/MyDrive/tf/codebase_whl\n","\n","import os\n","if os.path.exists(os.environ[\"GENERAL_WHL_PATH\"]) and os.path.exists(os.environ[\"GENERAL_WHL_PATH\"]):\n","    if os.environ[\"GOOGLE_COLAB\"] == \"1\":\n","        os.environ[\"bash_script\"] = \"setup_env.sh\"\n","    else:\n","        os.environ[\"bash_script\"] = \"setup_env_desktop.sh\"\n","\n","    !sed -i \"s|PATH_TO_GENERAL_WHL|$GENERAL_WHL_PATH|g\" $COLAB_NOTEBOOKS_PATH/tensorflow/$bash_script\n","    !sed -i \"s|PATH_TO_CODEBASE_WHL|$CODEBASE_WHL_PATH|g\" $COLAB_NOTEBOOKS_PATH/tensorflow/$bash_script\n","    !sed -i \"s|PATH_TO_COLAB_NOTEBOOKS|$COLAB_NOTEBOOKS_PATH|g\" $COLAB_NOTEBOOKS_PATH/tensorflow/$bash_script\n","\n","    !sh $COLAB_NOTEBOOKS_PATH/tensorflow/$bash_script\n","else:\n","    raise(\"Error, enter the whl paths correctly\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if os.environ.get(\"PYTHONPATH\",\"\") == \"\":\n","    os.environ[\"PYTHONPATH\"] = \"\"\n","os.environ[\"PYTHONPATH\"]+=\":/opt/nvidia/\"\n","if os.environ[\"GOOGLE_COLAB\"] == \"1\":\n","    os.environ[\"PYTHONPATH\"]+=\":/usr/local/lib/python3.6/dist-packages/third_party/nvml\"\n","else:\n","    os.environ[\"PYTHONPATH\"]+=\":/home_duplicate/rarunachalam/miniconda3/envs/tf_py_36/lib/python3.6/site-packages/third_party/nvml\" # FIX MINICONDA PATH"]},{"cell_type":"markdown","metadata":{"id":"Fl8fSfXseED3"},"source":["### 2.4 Reset env variables <a class=\"anchor\" id=\"head-2-4\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l_T2vBdzeIcO"},"outputs":[],"source":["# Setting up env variables for cleaner command line commands.\n","import os\n","\n","%env TAO_DOCKER_DISABLE=1\n","\n","%env KEY=nvidia_tlt\n","%env NUM_GPUS=1\n","\n","# Change the paths according to your directory structure, these are just examples\n","%env COLAB_NOTEBOOKS_PATH=/content/drive/MyDrive/ColabNotebooks\n","if not os.path.exists(os.environ[\"COLAB_NOTEBOOKS_PATH\"]):\n","    raise(\"Error, enter the path of the colab notebooks repo correctly\")\n","%env EXPERIMENT_DIR=/content/drive/MyDrive/results/efficientdet\n","%env DATA_DIR=/content/drive/MyDrive/coco_data/\n","\n","SPECS_DIR=f\"{os.environ['COLAB_NOTEBOOKS_PATH']}/tensorflow/efficientdet/specs\"\n","%env SPECS_DIR={SPECS_DIR}\n","# Showing list of specification files.\n","!ls -rlt $SPECS_DIR"]},{"cell_type":"markdown","metadata":{},"source":["## 3. Generate tfrecords <a class=\"anchor\" id=\"head-3\"></a>\n","* Create the tfrecords on the dataset split"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":72471,"status":"ok","timestamp":1657216689146,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"egGmwc9eioR0","outputId":"4e1c002b-d82b-4fcf-f42e-dff6f61a2e05"},"outputs":[],"source":["# convert training data to TFRecords\n","!tao efficientdet_tf1 dataset_convert -i $DATA_DIR/raw-data/train2017 \\\n","                                  -a $DATA_DIR/raw-data/annotations/instances_train2017.json \\\n","                                  -o $DATA_DIR --include_masks -t train -s 256"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[" # convert validation data to TFRecords\n"," !tao efficientdet_tf1 dataset_convert -i $DATA_DIR/raw-data/val2017 \\\n","                                  -a $DATA_DIR/raw-data/annotations/instances_val2017.json \\\n","                                  -o $DATA_DIR --include_masks -t val -s 32"]},{"cell_type":"markdown","metadata":{"id":"tMntctgD3h_u"},"source":["## 4. Provide training specification <a class=\"anchor\" id=\"head-4\"></a>\n","* Tfrecords for the train datasets\n","    * In order to use the newly generated tfrecords, update the dataset_config parameter in the spec file at `$SPECS_DIR/efficientdet_d0_train.txt` \n","Note that the learning rate in the spec file is set for 4 GPU training. If you have N gpus, you should divide LR by 4/N.\n","* Pre-trained models\n","* Augmentation parameters for on the fly data augmentation\n","* Other training (hyper-)parameters such as batch size, number of epochs, learning rate etc.\n","* **Note that the sample spec is not meant to produce SOTA accuracy on COCO. To reproduce SOTA, you might want to use TAO to train an ImageNet model first and change the total_steps to 100K or above.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q_KX9mJE3h_v","scrolled":true},"outputs":[],"source":["!sed -i \"s|TAO_DATA_PATH|$DATA_DIR/|g\" $SPECS_DIR/efficientdet_d0_train.txt\n","!sed -i \"s|EXPERIMENT_DIR_PATH|$EXPERIMENT_DIR/|g\" $SPECS_DIR/efficientdet_d0_train.txt\n","!cat $SPECS_DIR/efficientdet_d0_train.txt"]},{"cell_type":"markdown","metadata":{"id":"T7hPpgYi3h_v"},"source":["## 5. Train an Efficientdet model <a class=\"anchor\" id=\"head-5\"></a>\n","* Provide the sample spec file and the output directory location for models\n","* Evaluation uses COCO metrics. For more info, please refer to: https://cocodataset.org/#detection-eval\n","* WARNING: training will take several hours or one day to complete"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GcU7WdPT3h_v"},"outputs":[],"source":["!mkdir -p $EXPERIMENT_DIR/experiment_dir_unpruned"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1241726,"status":"ok","timestamp":1657305917711,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"ZND899vg3h_v","outputId":"e4ec092d-fb4b-472a-83cf-939ed5b67128","scrolled":true},"outputs":[],"source":["print(\"For multi-GPU, change --gpus based on your machine.\")\n","!tao efficientdet_tf1 train -e $SPECS_DIR/efficientdet_d0_train.txt \\\n","                        -d $EXPERIMENT_DIR/experiment_dir_unpruned\\\n","                        -k $KEY \\\n","                        --gpus $NUM_GPUS"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":160908,"status":"ok","timestamp":1657301853922,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"oqo-pwDm3h_v","outputId":"3f0df852-03bc-4193-b7f3-5192239864cc"},"outputs":[],"source":["print(\"To resume training from a checkpoint, simply run the same training script. It will pick up from where it's left.\")\n","# !tao efficientdet_tf1 train -e $SPECS_DIR/efficientdet_d0_train.txt \\\n","#                         -d $EXPERIMENT_DIR/experiment_dir_unpruned\\\n","#                         -k $KEY \\\n","#                         --gpus $NUM_GPUS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_AEGWaLo3h_v"},"outputs":[],"source":["print('Model for each epoch:')\n","print('---------------------')\n","!ls -ltrh $EXPERIMENT_DIR/experiment_dir_unpruned/"]},{"cell_type":"markdown","metadata":{"id":"D1gT4ODr3h_v"},"source":["## 6. Evaluate trained models <a class=\"anchor\" id=\"head-6\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1657306404829,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"vpiOxXPq3h_v","outputId":"ffda84d1-cae3-45b6-f196-05b2f6563ef4"},"outputs":[],"source":["%env NUM_STEP=1250"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15918,"status":"ok","timestamp":1657306432389,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"BZaK8-wl3h_w","outputId":"2026008b-955f-4570-90a3-a83fa5a13c64","scrolled":true},"outputs":[],"source":["!tao efficientdet_tf1 evaluate -e $SPECS_DIR/efficientdet_d0_train.txt \\\n","                           -m $EXPERIMENT_DIR/experiment_dir_unpruned/model.step-$NUM_STEP.tlt \\\n","                           -k $KEY"]},{"cell_type":"markdown","metadata":{"id":"c64JQ0oD3h_w"},"source":["## 7. Prune <a class=\"anchor\" id=\"head-7\"></a>"]},{"cell_type":"markdown","metadata":{"id":"7iyVm5lJ3h_w"},"source":["- Specify pre-trained model\n","- Equalization criterion\n","- Threshold for pruning.\n","- A key to save and load the model\n","- Output directory to store the model\n","\n","Usually, you just need to adjust -pth (threshold) for accuracy and model size trade off. Higher pth gives you smaller model (and thus higher inference speed) but worse accuracy. The threshold value depends on the dataset and the model. 0.4 in the block below is just a start point. If the retrain accuracy is good, you can increase this value to get smaller models. Otherwise, lower this value to get better accuracy."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iiNg1-wZ3h_x"},"outputs":[],"source":["# Create an output directory to save the pruned model.\n","!mkdir -p $EXPERIMENT_DIR/experiment_dir_pruned"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VnE-2nYw3h_x"},"outputs":[],"source":["!tao efficientdet_tf1 prune -m $EXPERIMENT_DIR/experiment_dir_unpruned/model.step-$NUM_STEP.tlt \\\n","                        -o $EXPERIMENT_DIR/experiment_dir_pruned \\\n","                        -pth 0.7 \\\n","                        -k $KEY"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_c1fUKFx3h_x"},"outputs":[],"source":["!ls -l $EXPERIMENT_DIR/experiment_dir_pruned"]},{"cell_type":"markdown","metadata":{"id":"XZ7DTkGK3h_x"},"source":["**Note** that you should retrain the pruned model first, as it cannot be directly used for evaluation or inference. "]},{"cell_type":"markdown","metadata":{"id":"zIURB_Fx3h_y"},"source":["## 8. Retrain pruned models <a class=\"anchor\" id=\"head-8\"></a>"]},{"cell_type":"markdown","metadata":{"id":"KEJn68CI3h_y"},"source":["- Model needs to be re-trained to bring back accuracy after pruning\n","- Specify re-training specification\n","- WARNING: training will take several hours or one day to complete"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JNQeMeEm3h_y"},"outputs":[],"source":["!sed -i \"s|TAO_DATA_PATH|$DATA_DIR/|g\" $SPECS_DIR/efficientdet_d0_retrain.txt\n","!sed -i \"s|EXPERIMENT_DIR_PATH|$EXPERIMENT_DIR/|g\" $SPECS_DIR/efficientdet_d0_retrain.txt\n","!cat $SPECS_DIR/efficientdet_d0_retrain.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YzW-p4HD3h_y"},"outputs":[],"source":["!mkdir -p $EXPERIMENT_DIR/experiment_dir_retrain"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yPiw-ux-3h_y"},"outputs":[],"source":["!tao efficientdet_tf1 train -e $SPECS_DIR/efficientdet_d0_retrain.txt \\\n","                        -d $EXPERIMENT_DIR/experiment_dir_retrain\\\n","                        -k $KEY \\\n","                        --gpus $NUM_GPUS"]},{"cell_type":"markdown","metadata":{"id":"eNQkd7-b3h_z"},"source":["## 9. Evaluate retrained model <a class=\"anchor\" id=\"head-9\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oxXk1OE93h_z"},"outputs":[],"source":["%env NUM_STEP=1250"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H16PXNho3h_z"},"outputs":[],"source":["!tao efficientdet_tf1 evaluate -e $SPECS_DIR/efficientdet_d0_retrain.txt \\\n","                           -m $EXPERIMENT_DIR/experiment_dir_retrain/model.step-$NUM_STEP.tlt \\\n","                           -k $KEY"]},{"cell_type":"markdown","metadata":{"id":"JMrvxt6t3h_z"},"source":["## 10. Visualize inferences <a class=\"anchor\" id=\"head-10\"></a>\n","In this section, we run the `infer` tool to generate inferences on the trained models and visualize the results. The `infer` tool produces annotated image outputs. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qT4Bdn573h_z","scrolled":true},"outputs":[],"source":["# Running inference for detection on n images\n","!tao efficientdet_tf1 inference -i $DATA_DIR/raw-data/test2017 \\\n","                            -o $EXPERIMENT_DIR/annotated_images \\\n","                            -e $SPECS_DIR/efficientdet_d0_train.txt \\\n","                            -m $EXPERIMENT_DIR/experiment_dir_unpruned/model.step-$NUM_STEP.tlt \\\n","                            --label_map $SPECS_DIR/coco_labels.txt \\\n","                            -k $KEY"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dllTxv5r3h_z"},"outputs":[],"source":["# Simple grid visualizer\n","import matplotlib.pyplot as plt\n","import os\n","from math import ceil\n","valid_image_ext = ['.jpg']\n","\n","def visualize_images(image_dir, num_cols=4, num_images=10):\n","    output_path = os.path.join(os.environ['EXPERIMENT_DIR'], image_dir)\n","    num_rows = int(ceil(float(num_images) / float(num_cols)))\n","    f, axarr = plt.subplots(num_rows, num_cols, figsize=[80,30])\n","    f.tight_layout()\n","    a = [os.path.join(output_path, image) for image in os.listdir(output_path) \n","         if os.path.splitext(image)[1].lower() in valid_image_ext]\n","    for idx, img_path in enumerate(a[:num_images]):\n","        col_id = idx % num_cols\n","        row_id = idx // num_cols\n","        img = plt.imread(img_path)\n","        axarr[row_id, col_id].imshow(img) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MLz5kDSi3h_0","scrolled":true},"outputs":[],"source":["# Visualizing the sample images.\n","OUTPUT_PATH = 'annotated_images' # relative path from $EXPERIMENT_DIR.\n","COLS = 2 # number of columns in the visualizer grid.\n","IMAGES = 4 # number of images to visualize.\n","\n","visualize_images(OUTPUT_PATH, num_cols=COLS, num_images=IMAGES)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["eNQkd7-b3h_z"],"machine_shape":"hm","name":"efficientdet.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.10.2 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.2"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":0}
