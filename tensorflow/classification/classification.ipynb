{"cells":[{"cell_type":"markdown","metadata":{"id":"LGLBrzF8hKgS"},"source":["## Switch to CPU Instance (Advisable only for Non Colab-Pro instance)\n","\n","1. Switch to CPU Instance for until Step 2 for non GPU dependent tasks\n","2. This increases your time available for the GPU dependent tasks on a Colab instance\n","2. Change Runtime type to CPU by Runtime(Top Left tab)->Change Runtime Type->None(Hardware Accelerator)\n","3.   Then click on Connect (Top Right)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SjpjyNg5c2V9"},"source":["## Mounting Google drive\n","Mount your Google drive storage to this Colab instance"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18056,"status":"ok","timestamp":1658620835786,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"EvUVkYw0hzqG","outputId":"a8f580a7-bd55-4a9c-8620-c0795752fbc9"},"outputs":[],"source":["try:\n","    import google.colab\n","    %env GOOGLE_COLAB=1\n","    from google.colab import drive\n","    drive.mount('/content/drive', force_remount=True)\n","except:\n","    %env GOOGLE_COLAB=0\n","    print(\"Warning: Not a Colab Environment\")"]},{"cell_type":"markdown","metadata":{"id":"YKV53RnEhxu7"},"source":["# TAO Image Classification \n","\n","Transfer learning is the process of transferring learned features from one application to another. It is a commonly used training technique where you use a model trained on one task and re-train to use it on a different task. \n","\n","Train Adapt Optimize (TAO) Toolkit  is a simple and easy-to-use Python based AI toolkit for taking purpose-built AI models and customizing them with users' own data.\n","\n","<img align=\"center\" src=\"https://developer.nvidia.com/sites/default/files/akamai/TAO/tlt-tao-toolkit-bring-your-own-model-diagram.png\" width=\"1080\"> "]},{"cell_type":"markdown","metadata":{"id":"_sgNEt9Mhxu-"},"source":["## Learning Objectives\n","In this notebook, you will learn how to leverage the simplicity and convenience of TAO to:\n","\n","* Take a pretrained resnet18 model and finetune on a sample dataset converted from PascalVOC\n","* Prune the finetuned model\n","* Retrain the pruned model to recover lost accuracy\n","* Export the pruned model\n","* Run Inference on the trained model\n","* Export the pruned and retrained model to a .etlt file for deployment to DeepStream\n","\n","### Table of Contents\n","This notebook shows an example use case for classification using the Train Adapt Optimize (TAO) Toolkit.\n","\n","0. [Set up env variables](#head-0)\n","1. [Prepare dataset and pretrained model](#head-1)\n","    1. [Split the dataset into train/test/val](#head-1-1)\n","    2. [Download pre-trained model](#head-1-2)\n","2. [Setup GPU environment](#head-2) <br>\n","    2.1 [Connect to GPU Instance](#head-2-1) <br>\n","    2.2 [Mounting Google drive](#head-2-2) <br>\n","    2.3 [Setup Python environment](#head-2-3) <br>\n","    2.4 [Reset env variables](#head-2-4) <br>\n","3. [Provide training specification](#head-3)\n","4. [Run TAO training](#head-4)\n","5. [Evaluate trained models](#head-5)\n","6. [Prune trained models](#head-6)\n","7. [Retrain pruned models](#head-7)\n","8. [Testing the model](#head-8)\n","9. [Visualize inferences](#head-9)\n"]},{"cell_type":"markdown","metadata":{"id":"tBi_JDq3hxu-"},"source":["#### Note\n","1. This notebook currently is by default set up to run training using 1 GPU. To use more GPU's please update the env variable `$NUM_GPUS` accordingly\n","2. This notebook uses VOC dataset by default, which should be around ~3 GB.\n","3. Using the default config/spec file provided in this notebook, each weight file size of classification created during training will be ~88 MB\n","\n","## 0. Set up env variables and set FIXME parameters <a class=\"anchor\" id=\"head-0\"></a>\n","\n","*Note: This notebook currently is by default set up to run training using 1 GPU. To use more GPU's please update the env variable `$NUM_GPUS` accordingly*\n","\n","#### FIXME\n","1. NUM_GPUS - set this to <= number of GPU's availble on the instance\n","1. COLAB_NOTEBOOKS_PATH - for Google Colab environment, set this path where you want to clone the repo to; for local system environment, set this path to the already cloned repo\n","1. EXPERIMENT_DIR - set this path to a folder location where pretrained models, checkpoints and log files during different model actions will be saved\n","1. delete_existing_experiments - set to True to remove existing pretrained models, checkpoints and log files of a previous experiment\n","1. DATA_DIR - set this path to a folder location where you want to dataset to be present\n","1. delete_existing_data - set this to True to remove existing preprocessed and original data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":353,"status":"ok","timestamp":1658620849523,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"vw5T4INKhxu_","outputId":"226ac190-4e1f-4032-bc0d-315598903bf0"},"outputs":[],"source":["# Setting up env variables for cleaner command line commands.\n","import os\n","\n","%env TAO_DOCKER_DISABLE=1\n","\n","%env KEY=nvidia_tlt\n","#FIXME1\n","%env NUM_GPUS=1\n","\n","#FIXME2\n","%env COLAB_NOTEBOOKS_PATH=/content/drive/MyDrive\n","if os.environ[\"GOOGLE_COLAB\"] == \"1\":\n","    if not os.path.exists(os.path.join(os.environ[\"COLAB_NOTEBOOKS_PATH\"],\"nvidia-tao\")):\n","        !git clone https://github.com/NVIDIA-AI-IOT/nvidia-tao.git $COLAB_NOTEBOOKS_PATH\n","else:\n","    if not os.path.exists(os.environ[\"COLAB_NOTEBOOKS_PATH\"]):\n","        raise Exception(\"Error, enter the path of the colab notebooks repo correctly\")\n","\n","#FIXME3\n","%env EXPERIMENT_DIR=/content/drive/MyDrive/results/classification\n","#FIXME4\n","delete_existing_experiments = True\n","#FIXME5\n","%env DATA_DIR=/content/drive/MyDrive/tf_data/classification_data/\n","#FIXME6\n","delete_existing_data = False\n","\n","if delete_existing_experiments:\n","    !sudo rm -rf $EXPERIMENT_DIR\n","if delete_existing_data:\n","    !sudo rm -rf $DATA_DIR\n","\n","SPECS_DIR=f\"{os.environ['COLAB_NOTEBOOKS_PATH']}/tensorflow/classification/specs\"\n","%env SPECS_DIR={SPECS_DIR}\n","# Showing list of specification files.\n","!ls -rlt $SPECS_DIR\n","\n","!sudo mkdir -p $DATA_DIR && sudo chmod -R 777 $DATA_DIR\n","!sudo mkdir -p $EXPERIMENT_DIR && sudo chmod -R 777 $EXPERIMENT_DIR"]},{"cell_type":"markdown","metadata":{"id":"mguwyH6dhxvC"},"source":["## 1. Prepare datasets and pre-trained model <a class=\"anchor\" id=\"head-2\"></a>"]},{"cell_type":"markdown","metadata":{"id":"oAvz0tgVhxvC"},"source":["We will be using the pascal VOC dataset for the tutorial. To find more details please visit \n","http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html#devkit. Please download the dataset present at http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar to $DATA_DIR."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YvLSpMachxvC"},"outputs":[],"source":["# Check that file is present\n","import os\n","DATA_DIR = os.environ.get('DATA_DIR')\n","if not os.path.isfile(os.path.join(DATA_DIR , 'VOCtrainval_11-May-2012.tar')):\n","    print('tar file for dataset not found. Please download.')\n","else:\n","    print('Found dataset.')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Udd0_37KhxvC"},"outputs":[],"source":["# unpack \n","!tar -xvf $DATA_DIR/VOCtrainval_11-May-2012.tar -C $DATA_DIR "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ojwdhEEmhxvC"},"outputs":[],"source":["# verify\n","!ls $DATA_DIR/VOCdevkit/VOC2012"]},{"cell_type":"markdown","metadata":{"id":"wU8ZaG48hxvD"},"source":["### A. Split the dataset into train/val/test <a class=\"anchor\" id=\"head-2-1\"></a>"]},{"cell_type":"markdown","metadata":{"id":"X4O1bDOQhxvD"},"source":["Pascal VOC Dataset is converted to our format (for classification) and then to train/val/test in the next two blocks."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3sRYTZ-xhxvE"},"outputs":[],"source":["from os.path import join as join_path\n","import os\n","import glob\n","import re\n","import shutil\n","\n","DATA_DIR=os.environ.get('DATA_DIR')\n","source_dir = join_path(DATA_DIR, \"VOCdevkit/VOC2012\")\n","target_dir = join_path(DATA_DIR, \"formatted\")\n","\n","\n","suffix = '_trainval.txt'\n","classes_dir = join_path(source_dir, \"ImageSets\", \"Main\")\n","images_dir = join_path(source_dir, \"JPEGImages\")\n","classes_files = glob.glob(classes_dir+\"/*\"+suffix)\n","for file in classes_files:\n","    # get the filename and make output class folder\n","    classname = os.path.basename(file)\n","    if classname.endswith(suffix):\n","        classname = classname[:-len(suffix)]\n","        target_dir_path = join_path(target_dir, classname)\n","        if not os.path.exists(target_dir_path):\n","            os.makedirs(target_dir_path)\n","    else:\n","        continue\n","    print(classname)\n","\n","\n","    with open(file) as f:\n","        content = f.readlines()\n","\n","\n","    for line in content:\n","        tokens = re.split('\\s+', line)\n","        if tokens[1] == '1':\n","            # copy this image into target dir_path\n","            target_file_path = join_path(target_dir_path, tokens[0] + '.jpg')\n","            src_file_path = join_path(images_dir, tokens[0] + '.jpg')\n","            shutil.copyfile(src_file_path, target_file_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s4A69FhQhxvE"},"outputs":[],"source":["import os\n","import glob\n","import shutil\n","from random import shuffle\n","from tqdm import tqdm\n","\n","DATA_DIR=os.environ.get('DATA_DIR')\n","SOURCE_DIR=os.path.join(DATA_DIR, 'formatted')\n","TARGET_DIR=os.path.join(DATA_DIR,'split')\n","# list dir\n","print(os.walk(SOURCE_DIR))\n","dir_list = next(os.walk(SOURCE_DIR))[1]\n","# for each dir, create a new dir in split\n","for dir_i in tqdm(dir_list):\n","        newdir_train = os.path.join(TARGET_DIR, 'train', dir_i)\n","        newdir_val = os.path.join(TARGET_DIR, 'val', dir_i)\n","        newdir_test = os.path.join(TARGET_DIR, 'test', dir_i)\n","        \n","        if not os.path.exists(newdir_train):\n","                os.makedirs(newdir_train)\n","        if not os.path.exists(newdir_val):\n","                os.makedirs(newdir_val)\n","        if not os.path.exists(newdir_test):\n","                os.makedirs(newdir_test)\n","\n","        img_list = glob.glob(os.path.join(SOURCE_DIR, dir_i, '*.jpg'))\n","        # shuffle data\n","        shuffle(img_list)\n","\n","        for j in range(int(len(img_list)*0.7)):\n","                shutil.copy2(img_list[j], os.path.join(TARGET_DIR, 'train', dir_i))\n","\n","        for j in range(int(len(img_list)*0.7), int(len(img_list)*0.8)):\n","                shutil.copy2(img_list[j], os.path.join(TARGET_DIR, 'val', dir_i))\n","                \n","        for j in range(int(len(img_list)*0.8), len(img_list)):\n","                shutil.copy2(img_list[j], os.path.join(TARGET_DIR, 'test', dir_i))\n","                \n","print('Done splitting dataset.')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o3Vc_8iPhxvF"},"outputs":[],"source":["!ls $DATA_DIR/split/test/cat"]},{"cell_type":"markdown","metadata":{"id":"i-rdo1SohxvF"},"source":["### B. Download pretrained models <a class=\"anchor\" id=\"head-2-2\"></a>"]},{"cell_type":"markdown","metadata":{"id":"KnHXYYw0hxvF"},"source":[" We will use NGC CLI to get the pre-trained models. For more details, go to ngc.nvidia.com and click the SETUP on the navigation bar."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2228,"status":"ok","timestamp":1658620967446,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"-VLIhbgXhxvF","outputId":"adbaf1df-7a40-46aa-8d52-070a9234886d"},"outputs":[],"source":["# Installing NGC CLI on the local machine.\n","## Download and install\n","%env LOCAL_PROJECT_DIR=/ngc_content/\n","%env CLI=ngccli_cat_linux.zip\n","!sudo mkdir -p $LOCAL_PROJECT_DIR/ngccli && sudo chmod -R 777 $LOCAL_PROJECT_DIR\n","\n","# Remove any previously existing CLI installations\n","!sudo rm -rf $LOCAL_PROJECT_DIR/ngccli/*\n","!wget \"https://ngc.nvidia.com/downloads/$CLI\" -P $LOCAL_PROJECT_DIR/ngccli\n","!unzip -u -q \"$LOCAL_PROJECT_DIR/ngccli/$CLI\" -d $LOCAL_PROJECT_DIR/ngccli/\n","!rm $LOCAL_PROJECT_DIR/ngccli/*.zip \n","os.environ[\"PATH\"]=\"{}/ngccli/ngc-cli:{}\".format(os.getenv(\"LOCAL_PROJECT_DIR\", \"\"), os.getenv(\"PATH\", \"\"))\n","!cp /usr/lib/x86_64-linux-gnu/libstdc++.so.6 $LOCAL_PROJECT_DIR/ngccli/ngc-cli/libstdc++.so.6"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2871,"status":"ok","timestamp":1658620974638,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"e8LMxzryhxvF","outputId":"b6fa3f01-b63b-490b-cbec-41ab799fb6c2","scrolled":true},"outputs":[],"source":["!ngc registry model list nvidia/tao/pretrained_classification:*"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":164,"status":"ok","timestamp":1658620979930,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"6Swj3isyhxvF"},"outputs":[],"source":["!mkdir -p $EXPERIMENT_DIR/pretrained_resnet18/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9748,"status":"ok","timestamp":1658620992763,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"qopJpwV_hxvG","outputId":"df17cd17-6fc6-4778-d939-9687e3eb8ab7"},"outputs":[],"source":["# Pull pretrained model from NGC\n","!ngc registry model download-version nvidia/tao/pretrained_classification:resnet18 --dest $EXPERIMENT_DIR/pretrained_resnet18"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":439,"status":"ok","timestamp":1658620993185,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"9dCQQa0NhxvG","outputId":"04ebc0ff-d5bc-4918-aa82-82ec0dea2bbd"},"outputs":[],"source":["print(\"Check that model is downloaded into dir.\")\n","!ls -l $EXPERIMENT_DIR/pretrained_resnet18/pretrained_classification_vresnet18"]},{"cell_type":"markdown","metadata":{"id":"_26rCobXcri1"},"source":["## 2. Setup GPU environment\n"]},{"cell_type":"markdown","metadata":{"id":"k7Cx1_lMded7"},"source":["### 2.1 Connect to GPU Instance <a class=\"anchor\" id=\"head-2-1\"></a>\n","\n","1. Move any data saved to the Colab Instance storage to Google Drive  \n","2. Change Runtime type to GPU by Runtime(Top Left tab)->Change Runtime Type->GPU(Hardware Accelerator)\n","3.   Then click on Connect (Top Right)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"yl8BoM0Jhzh9"},"source":["### 2.2 Mounting Google drive <a class=\"anchor\" id=\"head-2-2\"></a>\n","Mount your Google drive storage to this Colab instance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vk2m-N4Nh0Sd"},"outputs":[],"source":["try:\n","    import google.colab\n","    %env GOOGLE_COLAB=1\n","    from google.colab import drive\n","    drive.mount('/content/drive', force_remount=True)\n","except:\n","    %env GOOGLE_COLAB=0\n","    print(\"Warning: Not a Colab Environment\")"]},{"cell_type":"markdown","metadata":{"id":"MBV_YWiTc_KM"},"source":["### 2.3 Setup Python Environment <a class=\"anchor\" id=\"head-2-3\"></a>\n","Setup the environment necessary to run the TAO Networks by running the bash script"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s2Xygw-y8fjm"},"outputs":[],"source":["import os\n","if os.environ[\"GOOGLE_COLAB\"] == \"1\":\n","    os.environ[\"bash_script\"] = \"setup_env.sh\"\n","else:\n","    os.environ[\"bash_script\"] = \"setup_env_desktop.sh\"\n","\n","!sed -i \"s|PATH_TO_COLAB_NOTEBOOKS|$COLAB_NOTEBOOKS_PATH|g\" $COLAB_NOTEBOOKS_PATH/tensorflow/$bash_script\n","\n","!sh $COLAB_NOTEBOOKS_PATH/tensorflow/$bash_script"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if os.environ.get(\"PYTHONPATH\",\"\") == \"\":\n","    os.environ[\"PYTHONPATH\"] = \"\"\n","os.environ[\"PYTHONPATH\"]+=\":/opt/nvidia/\"\n","if os.environ[\"GOOGLE_COLAB\"] == \"1\":\n","    os.environ[\"PYTHONPATH\"]+=\":/usr/local/lib/python3.6/dist-packages/third_party/nvml\"\n","else:\n","    os.environ[\"PYTHONPATH\"]+=\":/home_duplicate/rarunachalam/miniconda3/envs/tf_py_36/lib/python3.6/site-packages/third_party/nvml\" # FIX MINICONDA PATH"]},{"cell_type":"markdown","metadata":{"id":"Fl8fSfXseED3"},"source":["### 2.4 Reset env variables (Use the same paths which was set in Step 0) <a class=\"anchor\" id=\"head-2-4\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l_T2vBdzeIcO"},"outputs":[],"source":["# Setting up env variables for cleaner command line commands.\n","import os\n","\n","%env TAO_DOCKER_DISABLE=1\n","\n","%env KEY=nvidia_tlt\n","%env NUM_GPUS=1\n","\n","# Change the paths according to your directory structure, these are just examples\n","%env COLAB_NOTEBOOKS_PATH=/content/drive/MyDrive\n","if not os.path.exists(os.environ[\"COLAB_NOTEBOOKS_PATH\"]):\n","    raise Exception(\"Error, enter the path of the colab notebooks repo correctly\")\n","%env EXPERIMENT_DIR=/content/drive/MyDrive/results/classification\n","%env DATA_DIR=/content/drive/MyDrive/tf_data/classification_data/\n","SPECS_DIR=f\"{os.environ['COLAB_NOTEBOOKS_PATH']}/tensorflow/classification/specs\"\n","%env SPECS_DIR={SPECS_DIR}\n","# Showing list of specification files.\n","!ls -rlt $SPECS_DIR"]},{"cell_type":"markdown","metadata":{"id":"PC7BeDZshxvG"},"source":["## 3. Provide training specification <a class=\"anchor\" id=\"head-3\"></a>\n","* Training dataset\n","* Validation dataset\n","* Pre-trained models\n","* Other training (hyper-)parameters such as batch size, number of epochs, learning rate etc."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Dbe4zB_hxvG","scrolled":true},"outputs":[],"source":["!sed -i \"s|TAO_DATA_PATH|$DATA_DIR/|g\" $SPECS_DIR/classification_spec.cfg\n","!sed -i \"s|EXPERIMENT_DIR_PATH|$EXPERIMENT_DIR/|g\" $SPECS_DIR/classification_spec.cfg\n","!cat $SPECS_DIR/classification_spec.cfg"]},{"cell_type":"markdown","metadata":{"id":"ZF_63FqBhxvG"},"source":["## 4. Run TAO training <a class=\"anchor\" id=\"head-4\"></a>\n","* Provide the sample spec file and the output directory location for models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"79_fDoqxhxvG","scrolled":true},"outputs":[],"source":["!tao classification_tf1 train -e $SPECS_DIR/classification_spec.cfg -r $EXPERIMENT_DIR/output -k $KEY"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mpu0CH5ShxvH"},"outputs":[],"source":["print(\"To run this training in data parallelism using multiple GPU's, please uncomment the line below and \"\n","      \"update the --gpus parameter to the number of GPU's you wish to use.\")\n","# !tao classification_tf1 train -e $SPECS_DIR/classification_spec.cfg \\\n","#                       -r $EXPERIMENT_DIR/output \\\n","#                       -k $KEY --gpus 2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pEaTcEKihxvI"},"outputs":[],"source":["print(\"\"\"\n","      To run this training in model parallelism using multiple GPU's, please uncomment the line below and update the\n","      --gpus parameter to the number of GPU's you wish to use. Also add related parameters in training_config to\n","      enable model parallelism. E.g., \n","\n","             model_parallelism: 50\n","             model_parallelism: 50\n","\n","\"\"\")\n","\n","#!tao classification_tf1 train -e $SPECS_DIR/classification_spec.cfg \\\n","#                       -r $EXPERIMENT_DIR/output \\\n","#                       -k $KEY --gpus 2 \\\n","#                       -np 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lc1WvRYJhxvK"},"outputs":[],"source":["print(\"To resume from a checkpoint, use --init_epoch along with your checkpoint configured in the spec file.\")\n","print(\"Please make sure that the model_path in the spec file is now updated to the '.tlt' file of the corresponding\"\n","      \"epoch you wish to resume from. You may choose from the files found under, '$EXPERIMENT_DIR/output/weights' folder.\")\n","# !tao classification_tf1 train -e $SPECS_DIR/classification_spec.cfg \\\n","#                        -r $EXPERIMENT_DIR/output \\\n","#                        -k $KEY --gpus 2 \\\n","#                        --init_epoch N"]},{"cell_type":"markdown","metadata":{"id":"wE0cOeL3hxvL"},"source":["## 5. Evaluate trained models <a class=\"anchor\" id=\"head-5\"></a>\n","\n","In this step, we assume that the training is complete and the model from the final epoch (`resnet_010.tlt`) is available. If you would like to run evaluation on an earlier model, please edit the spec file at `$SPECS_DIR/classification_spec.cfg` to point to the intended model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7l0J-JmWhxvL","scrolled":true},"outputs":[],"source":["!tao classification_tf1 evaluate -e $SPECS_DIR/classification_spec.cfg -k $KEY"]},{"cell_type":"markdown","metadata":{"id":"BD0t8fkwhxvL"},"source":["## 6. Prune trained models <a class=\"anchor\" id=\"head-6\"></a>\n","* Specify pre-trained model\n","* Equalization criterion\n","* Threshold for pruning\n","* Exclude prediction layer that you don't want pruned (e.g. predictions)\n","\n","Usually, you just need to adjust `-pth` (threshold) for accuracy and model size trade off. Higher `pth` gives you smaller model (and thus higher inference speed) but worse accuracy. The threshold to use is depend on the dataset. A pth value 0.68 is just a starting point. If the retrain accuracy is good, you can increase this value to get smaller models. Otherwise, lower this value to get better accuracy."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZjnvpJREhxvL"},"outputs":[],"source":["# Defining the checkpoint epoch number of the model to be used for the pruning.\n","# This should be lesser than the number of epochs training has been run for, in case training was interrupted earlier.\n","# By default, the default final model is at epoch 010.\n","%env EPOCH=003\n","!mkdir -p $EXPERIMENT_DIR/output/resnet_pruned\n","!tao classification_tf1 prune -m $EXPERIMENT_DIR/output/weights/resnet_$EPOCH.tlt \\\n","           -o $EXPERIMENT_DIR/output/resnet_pruned/resnet18_nopool_bn_pruned.tlt \\\n","           -eq union \\\n","           -pth 0.6 \\\n","           -k $KEY \\\n","           --results_dir $EXPERIMENT_DIR/logs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zgcs8JuDhxvM"},"outputs":[],"source":["print('Pruned model:')\n","print('------------')\n","!ls -rlt $EXPERIMENT_DIR/output/resnet_pruned"]},{"cell_type":"markdown","metadata":{"id":"-OWKjLHjhxvM"},"source":["## 7. Retrain pruned models <a class=\"anchor\" id=\"head-7\"></a>\n","* Model needs to be re-trained to bring back accuracy after pruning\n","* Specify re-training specification"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cs8HDEwshxvM"},"outputs":[],"source":["!sed -i \"s|TAO_DATA_PATH|$DATA_DIR/|g\" $SPECS_DIR/classification_retrain_spec.cfg\n","!sed -i \"s|EXPERIMENT_DIR_PATH|$EXPERIMENT_DIR/|g\" $SPECS_DIR/classification_retrain_spec.cfg\n","!cat $SPECS_DIR/classification_retrain_spec.cfg"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XceCQKQzhxvN"},"outputs":[],"source":["!tao classification_tf1 train -e $SPECS_DIR/classification_retrain_spec.cfg \\\n","                      -r $EXPERIMENT_DIR/output_retrain \\\n","                      -k $KEY"]},{"cell_type":"markdown","metadata":{"id":"9hZhkoOEhxvN"},"source":["## 8. Testing the model! <a class=\"anchor\" id=\"head-8\"></a>\n","\n","In this step, we assume that the training is complete and the model from the final epoch (`resnet_010.tlt`) is available. If you would like to run evaluation on an earlier model, please edit the spec file at `$SPECS_DIR/classification_retrain_spec.cfg` to point to the intended model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9XGYkITuhxvN"},"outputs":[],"source":["!tao classification_tf1 evaluate -e $SPECS_DIR/classification_retrain_spec.cfg -k $KEY"]},{"cell_type":"markdown","metadata":{"id":"re_b-_grhxvN"},"source":["## 9. Visualize Inferences <a class=\"anchor\" id=\"head-9\"></a>"]},{"cell_type":"markdown","metadata":{"id":"xIjgWAg3hxvN"},"source":["To see the output results of our model on test images, we can use the `tlt-infer` tool. Note that using models trained for higher epochs will usually result in better results. We'll run inference with the directory mode. You can also use the single image mode."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AAxzcaxKhxvN"},"outputs":[],"source":["# Defining the checkpoint epoch number to use for the subsequent steps.\n","# This should be lesser than the number of epochs training has been run for, in case training was interrupted earlier.\n","# By default, the default final model is at epoch 010.\n","%env EPOCH=010"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y_99T8iThxvN"},"outputs":[],"source":["!tao classification_tf1 inference -e $SPECS_DIR/classification_retrain_spec.cfg \\\n","                          -m $EXPERIMENT_DIR/output_retrain/weights/resnet_$EPOCH.tlt \\\n","                          -k $KEY -b 32 -d $DATA_DIR/split/test/person \\\n","                          -cm $EXPERIMENT_DIR/output_retrain/classmap.json"]},{"cell_type":"markdown","metadata":{"id":"1dpFLJUfhxvO"},"source":["As explained in Getting Started Guide, this outputs a results.csv file in the same directory. We can use a simple python program to see the visualize the output of csv file."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0P_Rx4ZehxvO"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from PIL import Image \n","import os\n","import csv\n","from math import ceil\n","\n","DATA_DIR = os.environ.get('DATA_DIR')\n","csv_path = os.path.join(DATA_DIR, 'split', 'test', 'person', 'result.csv')\n","results = []\n","with open(csv_path) as csv_file:\n","    csv_reader = csv.reader(csv_file, delimiter=',')\n","    for row in csv_reader:\n","        results.append((row[0], row[1]))\n","\n","w,h = 200,200\n","fig = plt.figure(figsize=(30,30))\n","columns = 5\n","rows = 1\n","for i in range(1, columns*rows + 1):\n","    ax = fig.add_subplot(rows, columns,i)\n","    print(results[i][0])\n","    img = Image.open(results[i][0])\n","    img = img.resize((w,h), Image.ANTIALIAS)\n","    plt.imshow(img)\n","    ax.set_title(results[i][1], fontsize=40)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"classification.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.9.12 ('base': conda)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"vscode":{"interpreter":{"hash":"851240db530085c397391f2f949356c4eb3a8832b55aabc3de74eae9cba050e3"}}},"nbformat":4,"nbformat_minor":0}
