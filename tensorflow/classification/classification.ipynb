{"cells":[{"cell_type":"markdown","metadata":{"id":"LGLBrzF8hKgS"},"source":["## Switch to CPU Instance (Only on Non Colab-Pro instance)\n","\n","1. Switch to CPU Instance for until Step 2 for non GPU dependent tasks\n","2. This increases your time available for the GPU dependent tasks on a Colab instance\n","2. Change Runtime type to CPU by Runtime(Top Left tab)->Change Runtime Type->None(Hardware Accelerator)\n","3.   Then click on Connect (Top Right)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SjpjyNg5c2V9"},"source":["## Mounting Google drive\n","Mount your Google drive storage to this Colab instance"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18056,"status":"ok","timestamp":1658620835786,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"EvUVkYw0hzqG","outputId":"a8f580a7-bd55-4a9c-8620-c0795752fbc9"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"markdown","metadata":{"id":"YKV53RnEhxu7"},"source":["# TAO Image Classification \n","\n","Transfer learning is the process of transferring learned features from one application to another. It is a commonly used training technique where you use a model trained on one task and re-train to use it on a different task. \n","\n","Train Adapt Optimize (TAO) Toolkit  is a simple and easy-to-use Python based AI toolkit for taking purpose-built AI models and customizing them with users' own data.\n","\n","<img align=\"center\" src=\"https://developer.nvidia.com/sites/default/files/akamai/embedded-transfer-learning-toolkit-software-stack-1200x670px.png\" width=\"1080\"> "]},{"cell_type":"markdown","metadata":{"id":"_sgNEt9Mhxu-"},"source":["## Learning Objectives\n","In this notebook, you will learn how to leverage the simplicity and convenience of TAO to:\n","\n","* Take a pretrained resnet18 model and finetune on a sample dataset converted from PascalVOC\n","* Prune the finetuned model\n","* Retrain the pruned model to recover lost accuracy\n","* Export the pruned model\n","* Run Inference on the trained model\n","* Export the pruned and retrained model to a .etlt file for deployment to DeepStream\n","\n","### Table of Contents\n","This notebook shows an example use case for classification using the Train Adapt Optimize (TAO) Toolkit.\n","\n","0. [Set up env variables](#head-0)\n","1. [Prepare dataset and pretrained model](#head-1)\n","    1. [Split the dataset into train/test/val](#head-1-1)\n","    2. [Download pre-trained model](#head-1-2)\n","2. [Setup GPU Environment](#head-2)\n","3. [Provide training specification](#head-2)\n","4. [Run TAO training](#head-3)\n","5. [Evaluate trained models](#head-4)\n","6. [Prune trained models](#head-5)\n","7. [Retrain pruned models](#head-6)\n","8. [Testing the model](#head-7)\n","9. [Visualize inferences](#head-8)\n"]},{"cell_type":"markdown","metadata":{"id":"tBi_JDq3hxu-"},"source":["## 0. Set up env variables <a class=\"anchor\" id=\"head-0\"></a>\n","When using the purpose-built pretrained models from NGC, please make sure to set the `$KEY` environment variable to the key as mentioned in the model overview. Failing to do so, can lead to errors when trying to load them as pretrained models.\n","\n","The following notebook requires the user to set an env variable called the `$LOCAL_PROJECT_DIR` as the path to the users workspace. Please note that the dataset to run this notebook is expected to reside in the `$LOCAL_PROJECT_DIR/data`, while the TAO experiment generated collaterals will be output to `$LOCAL_PROJECT_DIR/classification`. More information on how to set up the dataset and the supported steps in the TAO workflow are provided in the subsequent cells.\n","\n","*Note: Please make sure to remove any stray artifacts/files from the `$USER_EXPERIMENT_DIR` or `$DATA_DOWNLOAD_DIR` paths as mentioned below, that may have been generated from previous experiments. Having checkpoint files etc may interfere with creating a training graph for a new experiment.*\n","\n","*Note: This notebook currently is by default set up to run training using 1 GPU. To use more GPU's please update the env variable `$NUM_GPUS` accordingly*"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":353,"status":"ok","timestamp":1658620849523,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"vw5T4INKhxu_","outputId":"226ac190-4e1f-4032-bc0d-315598903bf0"},"outputs":[],"source":["# Setting up env variables for cleaner command line commands.\n","import os\n","\n","%env KEY=nvidia_tlt\n","%env NUM_GPUS=1\n","%env EXPERIMENT_DIR=/results/classification\n","%env DATA_DIR=/content/drive/MyDrive/tf_data/classification_data/\n","\n","# Set this path if you don't run the notebook from the samples directory.\n","# %env NOTEBOOK_ROOT=~/tao-samples/classification\n","\n","%env SPECS_DIR=/content/drive/MyDrive/ColabNotebooks/tensorflow/classification/specs\n","\n","# Showing list of specification files.\n","!ls -rlt $LOCAL_SPECS_DIR"]},{"cell_type":"markdown","metadata":{"id":"mguwyH6dhxvC"},"source":["## 1. Prepare datasets and pre-trained model <a class=\"anchor\" id=\"head-2\"></a>"]},{"cell_type":"markdown","metadata":{"id":"oAvz0tgVhxvC"},"source":["We will be using the pascal VOC dataset for the tutorial. To find more details please visit \n","http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html#devkit. Please download the dataset present at http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar to $DATA_DOWNLOAD_DIR."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YvLSpMachxvC"},"outputs":[],"source":["# Check that file is present\n","import os\n","DATA_DIR = os.environ.get('DATA_DIR')\n","print(DATA_DIR)\n","if not os.path.isfile(os.path.join(DATA_DIR , 'VOCtrainval_11-May-2012.tar')):\n","    print('tar file for dataset not found. Please download.')\n","else:\n","    print('Found dataset.')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Udd0_37KhxvC"},"outputs":[],"source":["# unpack \n","!tar -xvf $DATA_DIR/VOCtrainval_11-May-2012.tar -C $DATA_DIR "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ojwdhEEmhxvC"},"outputs":[],"source":["# verify\n","!ls $DATA_DIR/VOCdevkit/VOC2012"]},{"cell_type":"markdown","metadata":{"id":"wU8ZaG48hxvD"},"source":["### A. Split the dataset into train/val/test <a class=\"anchor\" id=\"head-2-1\"></a>"]},{"cell_type":"markdown","metadata":{"id":"X4O1bDOQhxvD"},"source":["Pascal VOC Dataset is converted to our format (for classification) and then to train/val/test in the next two blocks."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8fH7vzThhxvD"},"outputs":[],"source":["# install pip requirements\n","!pip3 install tqdm\n","!pip3 install matplotlib==3.3.3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3sRYTZ-xhxvE"},"outputs":[],"source":["from os.path import join as join_path\n","import os\n","import glob\n","import re\n","import shutil\n","\n","DATA_DIR=os.environ.get('DATA_DIR')\n","source_dir = join_path(DATA_DIR, \"VOCdevkit/VOC2012\")\n","target_dir = join_path(DATA_DIR, \"formatted\")\n","\n","\n","suffix = '_trainval.txt'\n","classes_dir = join_path(source_dir, \"ImageSets\", \"Main\")\n","images_dir = join_path(source_dir, \"JPEGImages\")\n","classes_files = glob.glob(classes_dir+\"/*\"+suffix)\n","for file in classes_files:\n","    # get the filename and make output class folder\n","    classname = os.path.basename(file)\n","    if classname.endswith(suffix):\n","        classname = classname[:-len(suffix)]\n","        target_dir_path = join_path(target_dir, classname)\n","        if not os.path.exists(target_dir_path):\n","            os.makedirs(target_dir_path)\n","    else:\n","        continue\n","    print(classname)\n","\n","\n","    with open(file) as f:\n","        content = f.readlines()\n","\n","\n","    for line in content:\n","        tokens = re.split('\\s+', line)\n","        if tokens[1] == '1':\n","            # copy this image into target dir_path\n","            target_file_path = join_path(target_dir_path, tokens[0] + '.jpg')\n","            src_file_path = join_path(images_dir, tokens[0] + '.jpg')\n","            shutil.copyfile(src_file_path, target_file_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s4A69FhQhxvE"},"outputs":[],"source":["import os\n","import glob\n","import shutil\n","from random import shuffle\n","from tqdm import tqdm\n","\n","DATA_DIR=os.environ.get('DATA_DIR')\n","print(DATA_DIR)\n","SOURCE_DIR=os.path.join(DATA_DIR, 'formatted')\n","TARGET_DIR=os.path.join(DATA_DIR,'split')\n","# list dir\n","print(os.walk(SOURCE_DIR))\n","dir_list = next(os.walk(SOURCE_DIR))[1]\n","# for each dir, create a new dir in split\n","for dir_i in tqdm(dir_list):\n","        newdir_train = os.path.join(TARGET_DIR, 'train', dir_i)\n","        newdir_val = os.path.join(TARGET_DIR, 'val', dir_i)\n","        newdir_test = os.path.join(TARGET_DIR, 'test', dir_i)\n","        \n","        if not os.path.exists(newdir_train):\n","                os.makedirs(newdir_train)\n","        if not os.path.exists(newdir_val):\n","                os.makedirs(newdir_val)\n","        if not os.path.exists(newdir_test):\n","                os.makedirs(newdir_test)\n","\n","        img_list = glob.glob(os.path.join(SOURCE_DIR, dir_i, '*.jpg'))\n","        # shuffle data\n","        shuffle(img_list)\n","\n","        for j in range(int(len(img_list)*0.7)):\n","                shutil.copy2(img_list[j], os.path.join(TARGET_DIR, 'train', dir_i))\n","\n","        for j in range(int(len(img_list)*0.7), int(len(img_list)*0.8)):\n","                shutil.copy2(img_list[j], os.path.join(TARGET_DIR, 'val', dir_i))\n","                \n","        for j in range(int(len(img_list)*0.8), len(img_list)):\n","                shutil.copy2(img_list[j], os.path.join(TARGET_DIR, 'test', dir_i))\n","                \n","print('Done splitting dataset.')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o3Vc_8iPhxvF"},"outputs":[],"source":["!ls $DATA_DIR/split/test/cat"]},{"cell_type":"markdown","metadata":{"id":"i-rdo1SohxvF"},"source":["### B. Download pretrained models <a class=\"anchor\" id=\"head-2-2\"></a>"]},{"cell_type":"markdown","metadata":{"id":"KnHXYYw0hxvF"},"source":[" We will use NGC CLI to get the pre-trained models. For more details, go to ngc.nvidia.com and click the SETUP on the navigation bar."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2228,"status":"ok","timestamp":1658620967446,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"-VLIhbgXhxvF","outputId":"adbaf1df-7a40-46aa-8d52-070a9234886d"},"outputs":[],"source":["# Installing NGC CLI on the local machine.\n","## Download and install\n","%env LOCAL_PROJECT_DIR=/content/\n","%env CLI=ngccli_cat_linux.zip\n","!mkdir -p $LOCAL_PROJECT_DIR/ngccli\n","\n","# Remove any previously existing CLI installations\n","!rm -rf $LOCAL_PROJECT_DIR/ngccli/*\n","!wget \"https://ngc.nvidia.com/downloads/$CLI\" -P $LOCAL_PROJECT_DIR/ngccli\n","!unzip -u -q \"$LOCAL_PROJECT_DIR/ngccli/$CLI\" -d $LOCAL_PROJECT_DIR/ngccli/\n","!rm $LOCAL_PROJECT_DIR/ngccli/*.zip \n","os.environ[\"PATH\"]=\"{}/ngccli/ngc-cli:{}\".format(os.getenv(\"LOCAL_PROJECT_DIR\", \"\"), os.getenv(\"PATH\", \"\"))\n","!cp /usr/lib/x86_64-linux-gnu/libstdc++.so.6 /content/ngccli/ngc-cli/libstdc++.so.6"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2871,"status":"ok","timestamp":1658620974638,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"e8LMxzryhxvF","outputId":"b6fa3f01-b63b-490b-cbec-41ab799fb6c2","scrolled":true},"outputs":[],"source":["!ngc registry model list nvidia/tao/pretrained_classification:*"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":164,"status":"ok","timestamp":1658620979930,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"6Swj3isyhxvF"},"outputs":[],"source":["!mkdir -p $EXPERIMENT_DIR/pretrained_resnet18/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9748,"status":"ok","timestamp":1658620992763,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"qopJpwV_hxvG","outputId":"df17cd17-6fc6-4778-d939-9687e3eb8ab7"},"outputs":[],"source":["# Pull pretrained model from NGC\n","!ngc registry model download-version nvidia/tao/pretrained_classification:resnet18 --dest $EXPERIMENT_DIR/pretrained_resnet18"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":439,"status":"ok","timestamp":1658620993185,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"9dCQQa0NhxvG","outputId":"04ebc0ff-d5bc-4918-aa82-82ec0dea2bbd"},"outputs":[],"source":["print(\"Check that model is downloaded into dir.\")\n","!ls -l $EXPERIMENT_DIR/pretrained_resnet18/pretrained_classification_vresnet18"]},{"cell_type":"markdown","metadata":{"id":"_26rCobXcri1"},"source":["## 2. Setup GPU environment\n"]},{"cell_type":"markdown","metadata":{"id":"k7Cx1_lMded7"},"source":["### Connect to GPU Instance\n","\n","1. Move any data saved to the Colab Instance storage to Google Drive  \n","2. Change Runtime type to GPU by Runtime(Top Left tab)->Change Runtime Type->GPU(Hardware Accelerator)\n","3.   Then click on Connect (Top Right)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"yl8BoM0Jhzh9"},"source":["### Mounting Google drive\n","Mount your Google drive storage to this Colab instance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vk2m-N4Nh0Sd"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"MBV_YWiTc_KM"},"source":["### Setup Python Environment\n","Setup the environment necessary to run the TAO Networks by running the bash script"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s2Xygw-y8fjm"},"outputs":[],"source":["!sh /content/drive/MyDrive/ColabNotebooks/tensorflow/setup_env.sh"]},{"cell_type":"markdown","metadata":{"id":"Fl8fSfXseED3"},"source":["### Reset env variables"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l_T2vBdzeIcO"},"outputs":[],"source":["# Setting up env variables for cleaner command line commands.\n","import os\n","\n","%env KEY=nvidia_tlt\n","%env NUM_GPUS=1\n","%env EXPERIMENT_DIR=/results/classification\n","%env DATA_DIR=/content/drive/MyDrive/tf_data/classification_data/\n","\n","# Set this path if you don't run the notebook from the samples directory.\n","# %env NOTEBOOK_ROOT=~/tao-samples/classification\n","\n","%env SPECS_DIR=/content/drive/MyDrive/ColabNotebooks/tensorflow/classification/specs\n","\n","# Showing list of specification files.\n","!ls -rlt $LOCAL_SPECS_DIR"]},{"cell_type":"markdown","metadata":{"id":"PC7BeDZshxvG"},"source":["## 3. Provide training specification <a class=\"anchor\" id=\"head-3\"></a>\n","* Training dataset\n","* Validation dataset\n","* Pre-trained models\n","* Other training (hyper-)parameters such as batch size, number of epochs, learning rate etc."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Dbe4zB_hxvG","scrolled":true},"outputs":[],"source":["!cat $SPECS_DIR/classification_spec.cfg"]},{"cell_type":"markdown","metadata":{"id":"ZF_63FqBhxvG"},"source":["## 4. Run TAO training <a class=\"anchor\" id=\"head-4\"></a>\n","* Provide the sample spec file and the output directory location for models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"79_fDoqxhxvG","scrolled":true},"outputs":[],"source":["!tao classification train -e $SPECS_DIR/classification_spec.cfg -r $EXPERIMENT_DIR/output -k $KEY"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mpu0CH5ShxvH"},"outputs":[],"source":["print(\"To run this training in data parallelism using multiple GPU's, please uncomment the line below and \"\n","      \"update the --gpus parameter to the number of GPU's you wish to use.\")\n","# !tao classification train -e $SPECS_DIR/classification_spec.cfg \\\n","#                       -r $USER_EXPERIMENT_DIR/output \\\n","#                       -k $KEY --gpus 2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pEaTcEKihxvI"},"outputs":[],"source":["print(\"\"\"\n","      To run this training in model parallelism using multiple GPU's, please uncomment the line below and update the\n","      --gpus parameter to the number of GPU's you wish to use. Also add related parameters in training_config to\n","      enable model parallelism. E.g., \n","\n","             model_parallelism: 50\n","             model_parallelism: 50\n","\n","\"\"\")\n","\n","#!tao classification train -e $SPECS_DIR/classification_spec.cfg \\\n","#                       -r $USER_EXPERIMENT_DIR/output \\\n","#                       -k $KEY --gpus 2 \\\n","#                       -np 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lc1WvRYJhxvK"},"outputs":[],"source":["print(\"To resume from a checkpoint, use --init_epoch along with your checkpoint configured in the spec file.\")\n","print(\"Please make sure that the model_path in the spec file is now updated to the '.tlt' file of the corresponding\"\n","      \"epoch you wish to resume from. You may choose from the files found under, '$USER_EXPERIMENT_DIR/output/weights' folder.\")\n","# !tao classification train -e $SPECS_DIR/classification_spec.cfg \\\n","#                        -r $USER_EXPERIMENT_DIR/output \\\n","#                        -k $KEY --gpus 2 \\\n","#                        --init_epoch N"]},{"cell_type":"markdown","metadata":{"id":"wE0cOeL3hxvL"},"source":["## 5. Evaluate trained models <a class=\"anchor\" id=\"head-5\"></a>\n","\n","In this step, we assume that the training is complete and the model from the final epoch (`resnet_080.tlt`) is available. If you would like to run evaluation on an earlier model, please edit the spec file at `$SPECS_DIR/classification_spec.cfg` to point to the intended model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7l0J-JmWhxvL","scrolled":true},"outputs":[],"source":["!tao classification evaluate -e $SPECS_DIR/classification_spec.cfg -k $KEY"]},{"cell_type":"markdown","metadata":{"id":"BD0t8fkwhxvL"},"source":["## 6. Prune trained models <a class=\"anchor\" id=\"head-6\"></a>\n","* Specify pre-trained model\n","* Equalization criterion\n","* Threshold for pruning\n","* Exclude prediction layer that you don't want pruned (e.g. predictions)\n","\n","Usually, you just need to adjust `-pth` (threshold) for accuracy and model size trade off. Higher `pth` gives you smaller model (and thus higher inference speed) but worse accuracy. The threshold to use is depend on the dataset. A pth value 0.68 is just a starting point. If the retrain accuracy is good, you can increase this value to get smaller models. Otherwise, lower this value to get better accuracy."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZjnvpJREhxvL"},"outputs":[],"source":["# Defining the checkpoint epoch number of the model to be used for the pruning.\n","# This should be lesser than the number of epochs training has been run for, incase training was interrupted earlier.\n","# By default, the default final model is at epoch 080.\n","%env EPOCH=003\n","!mkdir -p $EXPERIMENT_DIR/output/resnet_pruned\n","!tao classification prune -m $EXPERIMENT_DIR/output/weights/resnet_$EPOCH.tlt \\\n","           -o $EXPERIMENT_DIR/output/resnet_pruned/resnet18_nopool_bn_pruned.tlt \\\n","           -eq union \\\n","           -pth 0.6 \\\n","           -k $KEY \\\n","           --results_dir $EXPERIMENT_DIR/logs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zgcs8JuDhxvM"},"outputs":[],"source":["print('Pruned model:')\n","print('------------')\n","!ls -rlt $EXPERIMENT_DIR/output/resnet_pruned"]},{"cell_type":"markdown","metadata":{"id":"-OWKjLHjhxvM"},"source":["## 7. Retrain pruned models <a class=\"anchor\" id=\"head-7\"></a>\n","* Model needs to be re-trained to bring back accuracy after pruning\n","* Specify re-training specification"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cs8HDEwshxvM"},"outputs":[],"source":["!cat $SPECS_DIR/classification_retrain_spec.cfg"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XceCQKQzhxvN"},"outputs":[],"source":["!tao classification train -e $SPECS_DIR/classification_retrain_spec.cfg \\\n","                      -r $EXPERIMENT_DIR/output_retrain \\\n","                      -k $KEY"]},{"cell_type":"markdown","metadata":{"id":"9hZhkoOEhxvN"},"source":["## 8. Testing the model! <a class=\"anchor\" id=\"head-8\"></a>\n","\n","In this step, we assume that the training is complete and the model from the final epoch (`resnet_080.tlt`) is available. If you would like to run evaluation on an earlier model, please edit the spec file at `$SPECS_DIR/classification_retrain_spec.cfg` to point to the intended model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9XGYkITuhxvN"},"outputs":[],"source":["!tao classification evaluate -e $SPECS_DIR/classification_retrain_spec.cfg -k $KEY"]},{"cell_type":"markdown","metadata":{"id":"re_b-_grhxvN"},"source":["## 9. Visualize Inferences <a class=\"anchor\" id=\"head-9\"></a>"]},{"cell_type":"markdown","metadata":{"id":"xIjgWAg3hxvN"},"source":["To see the output results of our model on test images, we can use the `tlt-infer` tool. Note that using models trained for higher epochs will usually result in better results. We'll run inference with the directory mode. You can also use the single image mode."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AAxzcaxKhxvN"},"outputs":[],"source":["# Defining the checkpoint epoch number to use for the subsequent steps.\n","# This should be lesser than the number of epochs training has been run for, incase training was interrupted earlier.\n","# By default, the default final model is at epoch 080.\n","%env EPOCH=003"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y_99T8iThxvN"},"outputs":[],"source":["!tao classification inference -e $SPECS_DIR/classification_retrain_spec.cfg \\\n","                          -m $EXPERIMENT_DIR/output_retrain/weights/resnet_$EPOCH.tlt \\\n","                          -k $KEY -b 32 -d $DATA_DIR/split/test/person \\\n","                          -cm $EXPERIMENT_DIR/output_retrain/classmap.json"]},{"cell_type":"markdown","metadata":{"id":"1dpFLJUfhxvO"},"source":["As explained in Getting Started Guide, this outputs a results.csv file in the same directory. We can use a simple python program to see the visualize the output of csv file."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0P_Rx4ZehxvO"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from PIL import Image \n","import os\n","import csv\n","from math import ceil\n","\n","DATA_DIR = os.environ.get('DATA_DIR')\n","csv_path = os.path.join(DATA_DIR, 'split', 'test', 'person', 'result.csv')\n","results = []\n","with open(csv_path) as csv_file:\n","    csv_reader = csv.reader(csv_file, delimiter=',')\n","    for row in csv_reader:\n","        results.append((row[0], row[1]))\n","\n","w,h = 200,200\n","fig = plt.figure(figsize=(30,30))\n","columns = 5\n","rows = 1\n","for i in range(1, columns*rows + 1):\n","    ax = fig.add_subplot(rows, columns,i)\n","    print(results[i][0])\n","    img = Image.open(results[i][0])\n","    img = img.resize((w,h), Image.ANTIALIAS)\n","    plt.imshow(img)\n","    ax.set_title(results[i][1], fontsize=40)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"classification.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"nbformat":4,"nbformat_minor":0}
