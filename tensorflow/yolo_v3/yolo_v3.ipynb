{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Get the TensorRT tar file before running this Notebook\n","\n","1. Visit https://developer.nvidia.com/tensorrt\n","2. Clicking `Download now` from step one directs you to https://developer.nvidia.com/nvidia-tensorrt-download where you have to Login/Join Now for Nvidia Developer Program Membership\n","3. Now, in the download page: Choose TensorRT 8 in available versions\n","4. Agree to Terms and Conditions\n","5. Click on TensorRT 8.5 GA to expand the available options\n","6. Click on 'TensorRT 8.5 GA for Linux x86_64 and CUDA 11.0, 11.1, 11.2, 11.3, 11.4, 11.5, 11.6, 11.7 and 11.8 TAR Package' to dowload the TAR file\n","7. Upload the the tar file to your Google Drive"]},{"cell_type":"markdown","metadata":{"id":"LGLBrzF8hKgS"},"source":["## Connect to GPU Instance\n","\n","1. Change Runtime type to GPU by Runtime(Top Left tab)->Change Runtime Type->GPU(Hardware Accelerator)\n","1. Then click on Connect (Top Right)"]},{"cell_type":"markdown","metadata":{"id":"SjpjyNg5c2V9"},"source":["## Mounting Google drive\n","Mount your Google drive storage to this Colab instance"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18056,"status":"ok","timestamp":1658620835786,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"EvUVkYw0hzqG","outputId":"a8f580a7-bd55-4a9c-8620-c0795752fbc9"},"outputs":[],"source":["import sys\n","if 'google.colab' in sys.modules:\n","    %env GOOGLE_COLAB=1\n","    from google.colab import drive\n","    drive.mount('/content/drive', force_remount=True)\n","else:\n","    %env GOOGLE_COLAB=0\n","    print(\"Warning: Not a Colab Environment\")"]},{"cell_type":"markdown","metadata":{"id":"RRnP3kxvBC8-"},"source":["# Object Detection using TAO YOLOv3\n","\n","Transfer learning is the process of transferring learned features from one application to another. It is a commonly used training technique where you use a model trained on one task and re-train to use it on a different task. \n","\n","Train Adapt Optimize (TAO) Toolkit  is a simple and easy-to-use Python based AI toolkit for taking purpose-built AI models and customizing them with users' own data.\n","\n","<img align=\"center\" src=\"https://developer.nvidia.com/sites/default/files/akamai/TAO/tlt-tao-toolkit-bring-your-own-model-diagram.png\" width=\"1080\">\n"]},{"cell_type":"markdown","metadata":{"id":"5kmYiZ1bBC9A"},"source":["## Learning Objectives\n","In this notebook, you will learn how to leverage the simplicity and convenience of TAO to:\n","\n","* Take a pretrained ResNet-18 model and train a ResNet-18 YOLO v3 model on the KITTI dataset\n","* Prune the trained YOLO v3 model\n","* Retrain the pruned model to recover lost accuracy\n","* Export the pruned model\n","* Quantize the pruned model using QAT\n","* Run Inference on the trained model\n","* Export the pruned, quantized and retrained model to a .etlt file for deployment to DeepStream\n","\n","## Table of Contents\n","\n","This notebook shows an example usecase of YOLO v3 object detection using Train Adapt Optimize (TAO) Toolkit.\n","\n","0. [Set up env variables](#head-0)\n","1. [Prepare dataset and pre-trained model](#head-1) <br>\n","     1.1 [Download the dataset](#head-1-1)<br>\n","     1.2 [Verify the downloaded dataset](#head-1-2)<br>\n","     1.3 [Download pretrained model](#head-1-3)\n","2. [Setup GPU environment](#head-2) <br>\n","    2.1 [Setup Python environment](#head-2-1) <br>\n","3. [Generate TF Records](#head-3)\n","4. [Provide training specification](#head-4)\n","5. [Run TAO training](#head-5)\n","6. [Evaluate trained models](#head-6)\n","7. [Prune trained models](#head-7)\n","8. [Retrain pruned models](#head-8)\n","9. [Evaluate retrained model](#head-9)\n","10. [Visualize inferences](#head-10)"]},{"cell_type":"markdown","metadata":{"id":"m3qpiMQQBC9B"},"source":["#### Note\n","1. This notebook currently is by default set up to run training using 1 GPU. To use more GPU's please update the env variable `$NUM_GPUS` accordingly\n","1. This notebook uses KITTI dataset by default, which should be around ~12 GB. If you are limited by Google-Drive storage, we recommend to:\n","\n","    i. Download the dataset onto the local system\n","\n","    ii. Run the utility script at $COLAB_NOTEBOOKS/tensorflow/utils/generate_kitti_subset.py in your local system\n","\n","    iii. This generates a subset of coco dataset with number of sample images you wish for\n","\n","    iv. Upload this subset onto Google Drive\n","\n","1. Using the default config/spec file provided in this notebook, each weight file size of yolo_v3 created during training will be ~220 MB\n","\n","## 0. Set up env variables and set FIXME parameters <a class=\"anchor\" id=\"head-0\"></a>\n","\n","*Note: This notebook currently is by default set up to run training using 1 GPU. To use more GPU's please update the env variable `$NUM_GPUS` accordingly*\n","\n","#### FIXME\n","1. NUM_GPUS - set this to <= number of GPU's availble on the instance\n","1. COLAB_NOTEBOOKS_PATH - for Google Colab environment, set this path where you want to clone the repo to; for local system environment, set this path to the already cloned repo\n","1. EXPERIMENT_DIR - set this path to a folder location where pretrained models, checkpoints and log files during different model actions will be saved\n","1. delete_existing_experiments - set to True to remove existing pretrained models, checkpoints and log files of a previous experiment\n","1. DATA_DIR - set this path to a folder location where you want to dataset to be present\n","1. delete_existing_data - set this to True to remove existing preprocessed and original data\n","1. trt_tar_path - set this path of the uploaded TensorRT tar.gz file after browser download\n","1. trt_untar_folder_path - set to path of the folder where the TensoRT tar.gz file has to be untarred into\n","1. trt_version - set this to the version of TRT you have downloaded"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1657385981271,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"-cV0ihM7BC9B","outputId":"00638bb1-8986-48fd-d640-b2c15473f2f0"},"outputs":[],"source":["# Setting up env variables for cleaner command line commands.\n","import os\n","\n","%env TAO_DOCKER_DISABLE=1\n","\n","%env KEY=nvidia_tlt\n","#FIXME1\n","%env NUM_GPUS=1\n","\n","#FIXME2\n","%env COLAB_NOTEBOOKS_PATH=/content/drive/MyDrive/nvidia-tao\n","if os.environ[\"GOOGLE_COLAB\"] == \"1\":\n","    if not os.path.exists(os.path.join(os.environ[\"COLAB_NOTEBOOKS_PATH\"])):\n","\n","      !git clone https://github.com/NVIDIA-AI-IOT/nvidia-tao.git $COLAB_NOTEBOOKS_PATH\n","else:\n","    if not os.path.exists(os.environ[\"COLAB_NOTEBOOKS_PATH\"]):\n","        raise Exception(\"Error, enter the path of the colab notebooks repo correctly\")\n","\n","#FIXME3\n","%env EXPERIMENT_DIR=/content/drive/MyDrive/results/yolo_v3\n","#FIXME4\n","delete_existing_experiments = True\n","#FIXME5\n","%env DATA_DIR=/content/drive/MyDrive/kitti_data/\n","#FIXME6\n","delete_existing_data = False\n","\n","if delete_existing_experiments:\n","    !sudo rm -rf $EXPERIMENT_DIR\n","if delete_existing_data:\n","    !sudo rm -rf $DATA_DIR\n","\n","SPECS_DIR=f\"{os.environ['COLAB_NOTEBOOKS_PATH']}/tensorflow/yolo_v3/specs\"\n","%env SPECS_DIR={SPECS_DIR}\n","# Showing list of specification files.\n","!ls -rlt $SPECS_DIR\n","\n","!sudo mkdir -p $DATA_DIR && sudo chmod -R 777 $DATA_DIR\n","!sudo mkdir -p $EXPERIMENT_DIR && sudo chmod -R 777 $EXPERIMENT_DIR"]},{"cell_type":"markdown","metadata":{"id":"1Mz8Qfj0BC9E"},"source":["## 1. Prepare dataset and pre-trained model <a class=\"anchor\" id=\"head-1\"></a>"]},{"cell_type":"markdown","metadata":{},"source":["We will be using NVIDIA created Synthetic Object detection data based on KITTI dataset format in this notebook. To find more details about kitti format, please visit [here](https://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=2d).\n","\n","**If using custom dataset; it should follow this dataset structure**\n","```\n","$DATA_DIR/training\n","├── images\n","│   ├── image_name_1.jpg\n","│   ├── image_name_2.jpg\n","|   ├── ...\n","└── labels\n","    ├── image_name_1.txt\n","    ├── image_name_2.txt\n","    ├── ...\n","$DATA_DIR/val\n","├── images\n","│   ├── image_name_5.jpg\n","│   ├── image_name_6.jpg\n","|   ├── ...\n","└── labels\n","    ├── image_name_5.txt\n","    ├── image_name_6.txt\n","    ├── ...\n","```\n","The file name should be same for images and labels folders"]},{"cell_type":"markdown","metadata":{},"source":["### 1.1 Download the dataset <a class=\"anchor\" id=\"head-1-1\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!python3 -m pip install awscli\n","!aws s3 cp --no-sign-request s3://tao-object-detection-synthetic-dataset/tao_od_synthetic_train.tar.gz $DATA_DIR/\n","!aws s3 cp --no-sign-request s3://tao-object-detection-synthetic-dataset/tao_od_synthetic_val.tar.gz $DATA_DIR/\n","\n","!mkdir -p $DATA_DIR/train/ && rm -rf $DATA_DIR/train/*\n","!mkdir -p $DATA_DIR/val/ && rm -rf $DATA_DIR/val/*\n","\n","!tar -xzf $DATA_DIR/tao_od_synthetic_train.tar.gz -C $DATA_DIR/train/\n","!tar -xzf $DATA_DIR/tao_od_synthetic_val.tar.gz -C $DATA_DIR/val/"]},{"cell_type":"markdown","metadata":{"id":"rE3RUmPKBC9H"},"source":["### 1.3 Download pre-trained model <a class=\"anchor\" id=\"head-1-3\"></a>"]},{"cell_type":"markdown","metadata":{"id":"Bc-uEW3sBC9I"},"source":["We will use NGC CLI to get the pre-trained models. For more details, go to [ngc.nvidia.com](ngc.nvidia.com) and click the SETUP on the navigation bar."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1173,"status":"ok","timestamp":1657386127191,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"ZiEnfMaABC9I","outputId":"68c9907c-2acf-444f-e85a-e0365dfe388a"},"outputs":[],"source":["# Installing NGC CLI on the local machine.\n","## Download and install\n","%env LOCAL_PROJECT_DIR=/ngc_content/\n","%env CLI=ngccli_cat_linux.zip\n","!sudo mkdir -p $LOCAL_PROJECT_DIR/ngccli && sudo chmod -R 777 $LOCAL_PROJECT_DIR\n","\n","# Remove any previously existing CLI installations\n","!sudo rm -rf $LOCAL_PROJECT_DIR/ngccli/*\n","!wget \"https://ngc.nvidia.com/downloads/$CLI\" -P $LOCAL_PROJECT_DIR/ngccli\n","!unzip -u -q \"$LOCAL_PROJECT_DIR/ngccli/$CLI\" -d $LOCAL_PROJECT_DIR/ngccli/\n","!rm $LOCAL_PROJECT_DIR/ngccli/*.zip \n","os.environ[\"PATH\"]=\"{}/ngccli/ngc-cli:{}\".format(os.getenv(\"LOCAL_PROJECT_DIR\", \"\"), os.getenv(\"PATH\", \"\"))\n","!cp /usr/lib/x86_64-linux-gnu/libstdc++.so.6 $LOCAL_PROJECT_DIR/ngccli/ngc-cli/libstdc++.so.6"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5320,"status":"ok","timestamp":1657386132508,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"XOTxpKlbBC9I","outputId":"d17ba291-563d-4439-d987-adb48f2ba7eb"},"outputs":[],"source":["!ngc registry model list nvidia/tao/pretrained_object_detection:*"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1657386132509,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"fcKjy393BC9I"},"outputs":[],"source":["!mkdir -p $EXPERIMENT_DIR/pretrained_resnet18/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16228,"status":"ok","timestamp":1657386148730,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"kZDf_SsUBC9J","outputId":"4064dd83-4dda-413e-b1b4-f3d919dec72d"},"outputs":[],"source":["# Pull pretrained model from NGC\n","!ngc registry model download-version nvidia/tao/pretrained_object_detection:resnet18 \\\n","                    --dest $EXPERIMENT_DIR/pretrained_resnet18"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1657386148730,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"5iWJZ5UqBC9J","outputId":"8c86147e-c8d1-48f9-fed6-471eb1c9a55b"},"outputs":[],"source":["print(\"Check that model is downloaded into dir.\")\n","!ls -l $EXPERIMENT_DIR/pretrained_resnet18/pretrained_object_detection_vresnet18"]},{"cell_type":"markdown","metadata":{"id":"_26rCobXcri1"},"source":["## 2. Setup GPU environment <a class=\"anchor\" id=\"head-2\"></a>\n"]},{"cell_type":"markdown","metadata":{"id":"MBV_YWiTc_KM"},"source":["### 2.1 Setup Python environment <a class=\"anchor\" id=\"head-2-1\"></a>\n","Setup the environment necessary to run the TAO Networks by running the bash script"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s2Xygw-y8fjm"},"outputs":[],"source":["# FIXME 7: set this path of the uploaded TensorRT tar.gz file after browser download\n","trt_tar_path=\"/content/drive/MyDrive/TensorRT-8.5.1.7.Linux.x86_64-gnu.cuda-11.8.cudnn8.6.tar.gz\"\n","\n","import os\n","if not os.path.exists(trt_tar_path):\n","  raise Exception(\"TAR file not found in the provided path\")\n","\n","# FIXME 8: set to path of the folder where the TensoRT tar.gz file has to be untarred into\n","%env trt_untar_folder_path=/content/trt_untar\n","# FIXME 9: set this to the version of TRT you have downloaded\n","%env trt_version=8.5.1.7\n","\n","!mkdir -p $trt_untar_folder_path\n","\n","import os\n","\n","untar = True\n","for fname in os.listdir(os.environ.get(\"trt_untar_folder_path\", None)):\n","  if fname.startswith(\"TensorRT-\"+os.environ.get(\"trt_version\")) and not fname.endswith(\".tar.gz\"):\n","    untar = False\n","\n","if untar:\n","  !tar -xzf $trt_tar_path -C /content/trt_untar"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","if os.environ[\"GOOGLE_COLAB\"] == \"1\":\n","    os.environ[\"bash_script\"] = \"setup_env.sh\"\n","else:\n","    os.environ[\"bash_script\"] = \"setup_env_desktop.sh\"\n","\n","!sed -i \"s|PATH_TO_TRT|$trt_untar_folder_path|g\"$COLAB_NOTEBOOKS_PATH/tensorflow/$bash_script\n","!sed -i \"s|TRT_VERSION|$trt_version|g\" $COLAB_NOTEBOOKS_PATH/tensorflow/$bash_script\n","!sed -i \"s|PATH_TO_COLAB_NOTEBOOKS|$COLAB_NOTEBOOKS_PATH|g\" $COLAB_NOTEBOOKS_PATH/tensorflow/$bash_script\n","\n","!sh $COLAB_NOTEBOOKS_PATH/tensorflow/$bash_script"]},{"cell_type":"markdown","metadata":{"id":"eBGWmAf8BC9G"},"source":["## 3. Generate TF records <a class=\"anchor\" id=\"head-3\"></a>"]},{"cell_type":"markdown","metadata":{"id":"n-l8grK8BC9H"},"source":["Either TFRecord dataset format or the raw KITTI format(with directories of images and KITTI labels) can be used for training/validation. TFRecord dataset format is the recommended one as it is generally faster than reading numerious KITTI label files. However, in some cases, especially when training with small resolutions like 416x416, the experimental result shows TFRecord dataset is almost the same or slightly slower than the other format. So by default, we will use TFRecord dataset in this notebook. As a reference, we still provide the spec files for raw KITTI dataset format."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hzDngpyOBC9H"},"outputs":[],"source":["!sed -i \"s|TAO_DATA_PATH|$DATA_DIR/|g\" $SPECS_DIR/tfrecords_kitti_trainval.txt\n","!sed -i \"s|EXPERIMENT_DIR_PATH|$EXPERIMENT_DIR/|g\" $SPECS_DIR/tfrecords_kitti_trainval.txt\n","\n","print(\"TFrecords conversion spec file for training\")\n","!cat $SPECS_DIR/tfrecords_kitti_trainval.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":139107,"status":"ok","timestamp":1657386126025,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"fblO1_3vBC9H","outputId":"ab5d75b6-64a2-4677-a013-bbedaf61e645"},"outputs":[],"source":["# KITTI trainval\n","!tao model yolo_v3 dataset_convert -d $SPECS_DIR/tfrecords_kitti_trainval.txt \\\n","                     -o $DATA_DIR/tfrecords/kitti_trainval/kitti_trainval"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X-AhhZcBBC9H"},"outputs":[],"source":["# If you use your own dataset, you will need to run the code below to generate the best anchor shape\n","\n","# !tao model yolo_v3 kmeans -l $DATA_DIR/train/labels \\\n","#                     -i $DATA_DIR/train/images \\\n","#                     -n 9 \\\n","#                     -x 1280 \\\n","#                     -y 720\n","\n","# The anchor shape generated by this script is sorted. Write the first 3 into small_anchor_shape in the config\n","# file. Write middle 3 into mid_anchor_shape. Write last 3 into big_anchor_shape."]},{"cell_type":"markdown","metadata":{"id":"RYHWllQpBC9K"},"source":["## 4. Provide training specification <a class=\"anchor\" id=\"head-4\"></a>\n","* Augmentation parameters for on-the-fly data augmentation\n","* Other training (hyper-)parameters such as batch size, number of epochs, learning rate etc.\n","* Whether to use quantization aware training (QAT)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5kHblN6ZBC9K"},"outputs":[],"source":["# Provide pretrained model path\n","!sed -i \"s|TAO_DATA_PATH|$DATA_DIR/|g\" $SPECS_DIR/yolo_v3_train_resnet18_tfrecord.txt\n","!sed -i \"s|EXPERIMENT_DIR_PATH|$EXPERIMENT_DIR/|g\" $SPECS_DIR/yolo_v3_train_resnet18_tfrecord.txt\n","\n","# To enable QAT training on sample spec file, uncomment following lines\n","# !sed -i \"s/enable_qat: false/enable_qat: true/g\" $SPECS_DIR/yolo_v3_train_resnet18_tfrecord.txt\n","# !sed -i \"s/enable_qat: false/enable_qat: true/g\" $SPECS_DIR/yolo_v3_retrain_resnet18_tfrecord.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u1fwGYqbBC9K"},"outputs":[],"source":["# By default, the sample spec file disables QAT training. You can force non-QAT training by running lines below\n","# !sed -i \"s/enable_qat: true/enable_qat: false/g\" $SPECS_DIR/yolo_v3_train_resnet18_tfrecord.txt\n","# !sed -i \"s/enable_qat: true/enable_qat: false/g\" $SPECS_DIR/yolo_v3_retrain_resnet18_tfrecord.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cZzwm4PjBC9K"},"outputs":[],"source":["!cat $SPECS_DIR/yolo_v3_train_resnet18_tfrecord.txt"]},{"cell_type":"markdown","metadata":{"id":"T2_AcSVRBC9K"},"source":["## 5. Run TAO training <a class=\"anchor\" id=\"head-5\"></a>\n","* Provide the sample spec file and the output directory location for models\n","* WARNING: training will take several hours or one day to complete"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1657386148731,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"3sJKVS4DBC9L"},"outputs":[],"source":["!mkdir -p $EXPERIMENT_DIR/experiment_dir_unpruned"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1013489,"status":"ok","timestamp":1657387348988,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"3PvvDHXcBC9L","outputId":"6985301f-6ed5-4e69-bf6d-6fb2999a707b","scrolled":true},"outputs":[],"source":["print(\"To run with multigpu, please change --gpus based on the number of available GPUs in your machine.\")\n","!tao model yolo_v3 train -e $SPECS_DIR/yolo_v3_train_resnet18_tfrecord.txt \\\n","                   -r $EXPERIMENT_DIR/experiment_dir_unpruned \\\n","                   -k $KEY \\\n","                   --gpus 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ktIIHX2mBC9L"},"outputs":[],"source":["print(\"To resume from checkpoint, please change pretrain_model_path to resume_model_path in config file.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1657387348989,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"6VPatwjMBC9M","outputId":"54071c87-c892-49d2-80ad-3e67317f3a1f"},"outputs":[],"source":["print('Model for each epoch:')\n","print('---------------------')\n","!ls -ltrh $EXPERIMENT_DIR/experiment_dir_unpruned/weights"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1657387348989,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"720WSWFaBC9M","outputId":"2ee8e336-3e35-4bef-a737-abf0c1a81090"},"outputs":[],"source":["# Now check the evaluation stats in the csv file and pick the model with highest eval accuracy.\n","!cat $EXPERIMENT_DIR/experiment_dir_unpruned/yolov3_training_log_resnet18.csv\n","%env EPOCH=080"]},{"cell_type":"markdown","metadata":{"id":"6OBDVIylBC9M"},"source":["## 6. Evaluate trained models <a class=\"anchor\" id=\"head-6\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33484,"status":"ok","timestamp":1657387382468,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"60YTRCATBC9M","outputId":"c4b11d38-b3ca-4689-da66-30f3651d9ae0","scrolled":true},"outputs":[],"source":["!tao model yolo_v3 evaluate -e $SPECS_DIR/yolo_v3_train_resnet18_tfrecord.txt \\\n","                      -m $EXPERIMENT_DIR/experiment_dir_unpruned/weights/yolov3_resnet18_epoch_$EPOCH.tlt \\\n","                      -k $KEY"]},{"cell_type":"markdown","metadata":{"id":"cSvCjq3YBC9M"},"source":["## 7. Prune trained models <a class=\"anchor\" id=\"head-7\"></a>\n","* Specify pre-trained model\n","* Equalization criterion (`Only for resnets as they have element wise operations or MobileNets.`)\n","* Threshold for pruning.\n","* A key to save and load the model\n","* Output directory to store the model\n","\n","Usually, you just need to adjust `-pth` (threshold) for accuracy and model size trade off. Higher `pth` gives you smaller model (and thus higher inference speed) but worse accuracy. The threshold value depends on the dataset and the model. `0.5` in the block below is just a start point. If the retrain accuracy is good, you can increase this value to get smaller models. Otherwise, lower this value to get better accuracy."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1657387382469,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"VlMZY6t0BC9N"},"outputs":[],"source":["!mkdir -p $EXPERIMENT_DIR/experiment_dir_pruned"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":269471,"status":"ok","timestamp":1657387651931,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"zWz368nABC9N","outputId":"ceda7c7f-7068-4e1e-b566-c8a9ec8c6683","scrolled":true},"outputs":[],"source":["!tao model yolo_v3 prune -m $EXPERIMENT_DIR/experiment_dir_unpruned/weights/yolov3_resnet18_epoch_$EPOCH.tlt \\\n","                   -e $SPECS_DIR/yolo_v3_train_resnet18_tfrecord.txt \\\n","                   -o $EXPERIMENT_DIR/experiment_dir_pruned/yolov3_resnet18_pruned.tlt \\\n","                   -eq intersection \\\n","                   -pth 0.1 \\\n","                   -k $KEY"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32,"status":"ok","timestamp":1657387651931,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"QrZd6kxCBC9N","outputId":"9c7a91d2-0245-4992-885d-ace89c64d652"},"outputs":[],"source":["!ls -rlt $EXPERIMENT_DIR/experiment_dir_pruned/"]},{"cell_type":"markdown","metadata":{"id":"3PzxledvBC9N"},"source":["## 8. Retrain pruned models <a class=\"anchor\" id=\"head-8\"></a>\n","* Model needs to be re-trained to bring back accuracy after pruning\n","* Specify re-training specification\n","* WARNING: training will take several hours or one day to complete"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MsP8xWs1BC9N","scrolled":true},"outputs":[],"source":["# Printing the retrain spec file. \n","# Here we have updated the spec file to include the newly pruned model as a pretrained weights.\n","!sed -i \"s|TAO_DATA_PATH|$DATA_DIR/|g\" $SPECS_DIR/yolo_v3_retrain_resnet18_tfrecord.txt\n","!sed -i \"s|EXPERIMENT_DIR_PATH|$EXPERIMENT_DIR/|g\" $SPECS_DIR/yolo_v3_retrain_resnet18_tfrecord.txt\n","!cat $SPECS_DIR/yolo_v3_retrain_resnet18_tfrecord.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1657387651932,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"FmfrnS5bBC9N"},"outputs":[],"source":["!mkdir -p $EXPERIMENT_DIR/experiment_dir_retrain"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":741420,"status":"ok","timestamp":1657388393343,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"aqkcwz4ABC9N","outputId":"0cf71387-13b5-4272-9d0f-832d661c57f3"},"outputs":[],"source":["# Retraining using the pruned model as pretrained weights \n","!tao model yolo_v3 train --gpus 1 \\\n","                   -e $SPECS_DIR/yolo_v3_retrain_resnet18_tfrecord.txt \\\n","                   -r $EXPERIMENT_DIR/experiment_dir_retrain \\\n","                   -k $KEY"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":38,"status":"ok","timestamp":1657388393344,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"ceoiOR50BC9N","outputId":"29357a2d-c5a1-4e86-f292-2be9115cd9ee"},"outputs":[],"source":["# Listing the newly retrained model.\n","!ls -rlt $EXPERIMENT_DIR/experiment_dir_retrain/weights"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":447,"status":"ok","timestamp":1657388441106,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"AcPsiGxCBC9O","outputId":"a3e5ffbf-ed1d-4f64-8b35-d936cf4497c1"},"outputs":[],"source":["# Now check the evaluation stats in the csv file and pick the model with highest eval accuracy.\n","!cat $EXPERIMENT_DIR/experiment_dir_retrain/yolov3_training_log_resnet18.csv\n","%env EPOCH=050"]},{"cell_type":"markdown","metadata":{"id":"mua9cyw6BC9O"},"source":["## 9. Evaluate retrained model <a class=\"anchor\" id=\"head-9\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":35757,"status":"ok","timestamp":1657388485048,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"asgQyZrwBC9O","outputId":"7ea1e341-8034-4d96-ea26-cca7083566eb"},"outputs":[],"source":["!tao model yolo_v3 evaluate -e $SPECS_DIR/yolo_v3_retrain_resnet18_tfrecord.txt \\\n","                      -m $EXPERIMENT_DIR/experiment_dir_retrain/weights/yolov3_resnet18_epoch_$EPOCH.tlt \\\n","                      -k $KEY"]},{"cell_type":"markdown","metadata":{"id":"9PZV9ZjZBC9O"},"source":["## 10. Visualize inferences <a class=\"anchor\" id=\"head-10\"></a>\n","In this section, we run the `infer` tool to generate inferences on the trained models and visualize the results."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":33,"status":"ok","timestamp":1657388485050,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"-ijYQlwIBC9O"},"outputs":[],"source":["# Copy some test images\n","!mkdir -p $DATA_DIR/test_samples\n","!cp $DATA_DIR//val/images/* $DATA_DIR/test_samples/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":42571,"status":"ok","timestamp":1657388527606,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"5sGGK8nWBC9O","outputId":"dcaf724c-697c-4097-edfc-caf8a448de34"},"outputs":[],"source":["# Running inference for detection on n images\n","!tao model yolo_v3 inference -i $DATA_DIR/test_samples \\\n","                       -o $EXPERIMENT_DIR/yolo_infer_images \\\n","                       -e $SPECS_DIR/yolo_v3_retrain_resnet18_tfrecord.txt \\\n","                       -m $EXPERIMENT_DIR/experiment_dir_retrain/weights/yolov3_resnet18_epoch_$EPOCH.tlt \\\n","                       -l $EXPERIMENT_DIR/yolo_infer_labels \\\n","                       -k $KEY"]},{"cell_type":"markdown","metadata":{"id":"Zv0-v7QnBC9P"},"source":["The `inference` tool produces two outputs. \n","1. Overlain images in `$EXPERIMENT_DIR/yolo_infer_images`\n","2. Frame by frame bbox labels in kitti format located in `$EXPERIMENT_DIR/yolo_infer_labels`"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1284,"status":"ok","timestamp":1657388528881,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"8jGNFDDNBC9P","outputId":"251bcc79-b182-489e-c3da-2588d8900287"},"outputs":[],"source":["# Simple grid visualizer\n","import matplotlib.pyplot as plt\n","import os\n","from math import ceil\n","valid_image_ext = ['.jpg', '.png', '.jpeg', '.ppm']\n","\n","def visualize_images(image_dir, num_cols=4, num_images=10):\n","    output_path = os.path.join(os.environ['EXPERIMENT_DIR'], image_dir)\n","    num_rows = int(ceil(float(num_images) / float(num_cols)))\n","    f, axarr = plt.subplots(num_rows, num_cols, figsize=[80,30])\n","    f.tight_layout()\n","    a = [os.path.join(output_path, image) for image in os.listdir(output_path) \n","         if os.path.splitext(image)[1].lower() in valid_image_ext]\n","    for idx, img_path in enumerate(a[:num_images]):\n","        col_id = idx % num_cols\n","        row_id = idx // num_cols\n","        img = plt.imread(img_path)\n","        axarr[row_id, col_id].imshow(img) "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":624},"executionInfo":{"elapsed":3919,"status":"error","timestamp":1657388766369,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"pM9CjZGCBC9P","outputId":"a7a47aff-3a2b-42f1-ac68-74f6af74f16d"},"outputs":[],"source":["!mkdir -p $EXPERIMENT_DIR/yolo_infer_images\n","# Visualizing the sample images.\n","OUTPUT_PATH = 'yolo_infer_images' # relative path from $EXPERIMENT_DIR.\n","COLS = 3 # number of columns in the visualizer grid.\n","IMAGES = 9 # number of images to visualize.\n","\n","visualize_images(OUTPUT_PATH, num_cols=COLS, num_images=IMAGES)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"yolo_v3.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.10.2 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.2"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":0}
