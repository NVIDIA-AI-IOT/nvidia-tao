{"cells":[{"cell_type":"markdown","metadata":{"id":"LGLBrzF8hKgS"},"source":["## Switch to CPU Instance (Advisable only for Non Colab-Pro instance)\n","\n","1. Switch to CPU Instance for until Step 2 for non GPU dependent tasks\n","2. This increases your time available for the GPU dependent tasks on a Colab instance\n","2. Change Runtime type to CPU by Runtime(Top Left tab)->Change Runtime Type->None(Hardware Accelerator)\n","3.   Then click on Connect (Top Right)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SjpjyNg5c2V9"},"source":["## Mounting Google drive\n","Mount your Google drive storage to this Colab instance"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18056,"status":"ok","timestamp":1658620835786,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"EvUVkYw0hzqG","outputId":"a8f580a7-bd55-4a9c-8620-c0795752fbc9"},"outputs":[],"source":["try:\n","    import google.colab\n","    %env GOOGLE_COLAB=1\n","    from google.colab import drive\n","    drive.mount('/content/drive', force_remount=True)\n","except:\n","    %env GOOGLE_COLAB=0\n","    print(\"Warning: Not a Colab Environment\")"]},{"cell_type":"markdown","metadata":{"id":"RRnP3kxvBC8-"},"source":["# Object Detection using TAO YOLOv3\n","\n","Transfer learning is the process of transferring learned features from one application to another. It is a commonly used training technique where you use a model trained on one task and re-train to use it on a different task. \n","\n","Train Adapt Optimize (TAO) Toolkit  is a simple and easy-to-use Python based AI toolkit for taking purpose-built AI models and customizing them with users' own data.\n","\n","<img align=\"center\" src=\"https://developer.nvidia.com/sites/default/files/akamai/embedded-transfer-learning-toolkit-software-stack-1200x670px.png\" width=\"1080\">\n"]},{"cell_type":"markdown","metadata":{"id":"5kmYiZ1bBC9A"},"source":["## Learning Objectives\n","In this notebook, you will learn how to leverage the simplicity and convenience of TAO to:\n","\n","* Take a pretrained ResNet-18 model and train a ResNet-18 YOLO v3 model on the KITTI dataset\n","* Prune the trained YOLO v3 model\n","* Retrain the pruned model to recover lost accuracy\n","* Export the pruned model\n","* Quantize the pruned model using QAT\n","* Run Inference on the trained model\n","* Export the pruned, quantized and retrained model to a .etlt file for deployment to DeepStream\n","\n","## Table of Contents\n","\n","This notebook shows an example usecase of YOLO v3 object detection using Train Adapt Optimize (TAO) Toolkit.\n","\n","0. [Set up env variables](#head-0)\n","1. [Prepare dataset and pre-trained model](#head-1) <br>\n","     1.1 [Download the dataset](#head-1-1)<br>\n","     1.2 [Verify the downloaded dataset](#head-1-2)<br>\n","     1.3 [Download pretrained model](#head-1-3)\n","2. [Setup GPU environment](#head-2) <br>\n","    2.1 [Connect to GPU Instance](#head-2-1) <br>\n","    2.2 [Mounting Google drive](#head-2-2) <br>\n","    2.3 [Setup Python environment](#head-2-3) <br>\n","    2.4 [Reset env variables](#head-2-4) <br>\n","3. [Generate TF Records](#head-3)\n","4. [Provide training specification](#head-4)\n","5. [Run TAO training](#head-5)\n","6. [Evaluate trained models](#head-6)\n","7. [Prune trained models](#head-7)\n","8. [Retrain pruned models](#head-8)\n","9. [Evaluate retrained model](#head-9)\n","10. [Visualize inferences](#head-10)"]},{"cell_type":"markdown","metadata":{"id":"m3qpiMQQBC9B"},"source":["## 0. Set up env variables<a class=\"anchor\" id=\"head-0\"></a>\n","\n","When using the purpose-built pretrained models from NGC, please make sure to set the `$KEY` environment variable to the key as mentioned in the model overview. Failing to do so, can lead to errors when trying to load them as pretrained models.\n","\n","*Note: Please make sure to remove any stray artifacts/files from the `$EXPERIMENT_DIR` or `$DATA_DIR` paths as mentioned below, that may have been generated from previous experiments. Having checkpoint files etc may interfere with creating a training graph for a new experiment.*"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1657385981271,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"-cV0ihM7BC9B","outputId":"00638bb1-8986-48fd-d640-b2c15473f2f0"},"outputs":[],"source":["# Setting up env variables for cleaner command line commands.\n","import os\n","\n","%env TAO_DOCKER_DISABLE=1\n","\n","%env KEY=nvidia_tlt\n","%env NUM_GPUS=1\n","\n","# Change the paths according to your directory structure, these are just examples\n","%env COLAB_NOTEBOOKS_PATH=/content/drive/MyDrive/ColabNotebooks\n","if not os.path.exists(os.environ[\"COLAB_NOTEBOOKS_PATH\"]):\n","    raise(\"Error, enter the path of the colab notebooks repo correctly\")\n","%env EXPERIMENT_DIR=/results/yolo_v3\n","%env DATA_DIR=/content/drive/MyDrive/kitti_data/\n","\n","SPECS_DIR=f\"{os.environ['COLAB_NOTEBOOKS_PATH']}/tensorflow/yolo_v3/specs\"\n","%env SPECS_DIR={SPECS_DIR}\n","# Showing list of specification files.\n","!ls -rlt $SPECS_DIR\n","\n","!sudo mkdir -p $DATA_DIR && sudo chmod -R 777 $DATA_DIR\n","!sudo mkdir -p $EXPERIMENT_DIR && sudo chmod -R 777 $EXPERIMENT_DIR"]},{"cell_type":"markdown","metadata":{"id":"1Mz8Qfj0BC9E"},"source":["## 1. Prepare dataset and pre-trained model <a class=\"anchor\" id=\"head-1\"></a>"]},{"cell_type":"markdown","metadata":{"id":"ZupJuSctBC9E"},"source":[" We will be using the KITTI detection dataset for the tutorial. To find more details please visit\n"," http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=2d. Please download the KITTI detection images (http://www.cvlibs.net/download.php?file=data_object_image_2.zip) and labels (http://www.cvlibs.net/download.php?file=data_object_label_2.zip) to $DATA_DIR.\n"," \n"," The data will then be extracted to have\n"," * training images in `$DATA_DIR/training/image_2`\n"," * training labels in `$DATA_DIR/training/label_2`\n"," * testing images in `$DATA_DIR/testing/image_2`\n"," \n","You may use this notebook with your own dataset as well. To use this example with your own dataset, please follow the same directory structure as mentioned below.\n","\n","*Note: There are no labels for the testing images, therefore we use it just to visualize inferences for the trained model.*"]},{"cell_type":"markdown","metadata":{"id":"iAyBRB_kBC9E"},"source":["### 1.1 Download the dataset <a class=\"anchor\" id=\"head-1-1\"></a>"]},{"cell_type":"markdown","metadata":{"id":"csEmxnTPBC9E"},"source":["Once you have gotten the download links in your email, please populate them in place of the `KITTI_IMAGES_DOWNLOAD_URL` and the `KITTI_LABELS_DOWNLOAD_URL`. This next cell, will download the data and place in `$DATA_DIR`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X2MZKF4UBC9F"},"outputs":[],"source":["import os\n","!mkdir -p $DATA_DIR\n","os.environ[\"URL_IMAGES\"]=KITTI_IMAGES_DOWNLOAD_URL\n","!if [ ! -f $DATA_DIR/data_object_image_2.zip ]; then wget $URL_IMAGES -O $DATA_DIR/data_object_image_2.zip; else echo \"image archive already downloaded\"; fi \n","os.environ[\"URL_LABELS\"]=KITTI_LABELS_DOWNLOAD_URL\n","!if [ ! -f $DATA_DIR/data_object_label_2.zip ]; then wget $URL_LABELS -O $DATA_DIR/data_object_label_2.zip; else echo \"label archive already downloaded\"; fi "]},{"cell_type":"markdown","metadata":{"id":"VhHcQmfmBC9F"},"source":["### 1.2 Verify the downloaded dataset <a class=\"anchor\" id=\"head-1-2\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h1DwvxbHBC9F"},"outputs":[],"source":["# Check the dataset is present\n","!mkdir -p $DATA_DIR\n","!if [ ! -f $DATA_DIR/data_object_image_2.zip ]; then echo 'Image zip file not found, please download.'; else echo 'Found Image zip file.';fi\n","!if [ ! -f $DATA_DIR/data_object_label_2.zip ]; then echo 'Label zip file not found, please download.'; else echo 'Found Labels zip file.';fi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yuzv3u8qBC9F"},"outputs":[],"source":["# This may take a while: verify integrity of zip files \n","!sha256sum $DATA_DIR/data_object_image_2.zip | cut -d ' ' -f 1 | grep -xq '^351c5a2aa0cd9238b50174a3a62b846bc5855da256b82a196431d60ff8d43617$' ; \\\n","if test $? -eq 0; then echo \"images OK\"; else echo \"images corrupt, re-download!\" && rm -f $DATA_DIR/data_object_image_2.zip; fi \n","!sha256sum $DATA_DIR/data_object_label_2.zip | cut -d ' ' -f 1 | grep -xq '^4efc76220d867e1c31bb980bbf8cbc02599f02a9cb4350effa98dbb04aaed880$' ; \\\n","if test $? -eq 0; then echo \"labels OK\"; else echo \"labels corrupt, re-download!\" && rm -f $DATA_DIR/data_object_label_2.zip; fi "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j_6bWsR4BC9G"},"outputs":[],"source":["# unpack \n","!unzip -u $DATA_DIR/data_object_image_2.zip -d $DATA_DIR\n","!unzip -u $DATA_DIR/data_object_label_2.zip -d $DATA_DIR"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5260,"status":"ok","timestamp":1657385986931,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"77EKNy_5BC9G","outputId":"d5c8dff0-6262-400e-cc2a-647f1e1d6333"},"outputs":[],"source":["# verify\n","import os\n","\n","DATA_DIR = os.environ.get('DATA_DIR')\n","num_training_images = len(os.listdir(os.path.join(DATA_DIR, \"training/image_2\")))\n","num_training_labels = len(os.listdir(os.path.join(DATA_DIR, \"training/label_2\")))\n","num_testing_images = len(os.listdir(os.path.join(DATA_DIR, \"testing/image_2\")))\n","print(\"Number of images in the train/val set. {}\".format(num_training_images))\n","print(\"Number of labels in the train/val set. {}\".format(num_training_labels))\n","print(\"Number of images in the test set. {}\".format(num_testing_images))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6qn_tCkkBC9G"},"outputs":[],"source":["# Sample kitti label.\n","!cat $DATA_DIR/training/label_2/000110.txt"]},{"cell_type":"markdown","metadata":{"id":"rE3RUmPKBC9H"},"source":["### 1.3 Download pre-trained model <a class=\"anchor\" id=\"head-1-3\"></a>"]},{"cell_type":"markdown","metadata":{"id":"Bc-uEW3sBC9I"},"source":["We will use NGC CLI to get the pre-trained models. For more details, go to [ngc.nvidia.com](ngc.nvidia.com) and click the SETUP on the navigation bar."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1173,"status":"ok","timestamp":1657386127191,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"ZiEnfMaABC9I","outputId":"68c9907c-2acf-444f-e85a-e0365dfe388a"},"outputs":[],"source":["# Installing NGC CLI on the local machine.\n","## Download and install\n","%env LOCAL_PROJECT_DIR=/ngc_content/\n","%env CLI=ngccli_cat_linux.zip\n","!mkdir -p $LOCAL_PROJECT_DIR/ngccli\n","\n","# Remove any previously existing CLI installations\n","!rm -rf $LOCAL_PROJECT_DIR/ngccli/*\n","!wget \"https://ngc.nvidia.com/downloads/$CLI\" -P $LOCAL_PROJECT_DIR/ngccli\n","!unzip -u -q \"$LOCAL_PROJECT_DIR/ngccli/$CLI\" -d $LOCAL_PROJECT_DIR/ngccli/\n","!rm $LOCAL_PROJECT_DIR/ngccli/*.zip \n","os.environ[\"PATH\"]=\"{}/ngccli/ngc-cli:{}\".format(os.getenv(\"LOCAL_PROJECT_DIR\", \"\"), os.getenv(\"PATH\", \"\"))\n","!cp /usr/lib/x86_64-linux-gnu/libstdc++.so.6 $LOCAL_PROJECT_DIR/ngccli/ngc-cli/libstdc++.so.6"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5320,"status":"ok","timestamp":1657386132508,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"XOTxpKlbBC9I","outputId":"d17ba291-563d-4439-d987-adb48f2ba7eb"},"outputs":[],"source":["!ngc registry model list nvidia/tao/pretrained_object_detection:*"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1657386132509,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"fcKjy393BC9I"},"outputs":[],"source":["!mkdir -p $EXPERIMENT_DIR/pretrained_resnet18/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16228,"status":"ok","timestamp":1657386148730,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"kZDf_SsUBC9J","outputId":"4064dd83-4dda-413e-b1b4-f3d919dec72d"},"outputs":[],"source":["# Pull pretrained model from NGC\n","!ngc registry model download-version nvidia/tao/pretrained_object_detection:resnet18 \\\n","                    --dest $EXPERIMENT_DIR/pretrained_resnet18"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1657386148730,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"5iWJZ5UqBC9J","outputId":"8c86147e-c8d1-48f9-fed6-471eb1c9a55b"},"outputs":[],"source":["print(\"Check that model is downloaded into dir.\")\n","!ls -l $EXPERIMENT_DIR/pretrained_resnet18/pretrained_object_detection_vresnet18"]},{"cell_type":"markdown","metadata":{"id":"_26rCobXcri1"},"source":["## 2. Setup GPU environment <a class=\"anchor\" id=\"head-2\"></a>\n"]},{"cell_type":"markdown","metadata":{"id":"k7Cx1_lMded7"},"source":["### 2.1 Connect to GPU Instance <a class=\"anchor\" id=\"head-2-1\"></a>\n","\n","1. Move any data saved to the Colab Instance storage to Google Drive  \n","2. Change Runtime type to GPU by Runtime(Top Left tab)->Change Runtime Type->GPU(Hardware Accelerator)\n","3.   Then click on Connect (Top Right)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"yl8BoM0Jhzh9"},"source":["### 2.2 Mounting Google drive <a class=\"anchor\" id=\"head-2-2\"></a>\n","Mount your Google drive storage to this Colab instance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vk2m-N4Nh0Sd"},"outputs":[],"source":["try:\n","    import google.colab\n","    %env GOOGLE_COLAB=1\n","    from google.colab import drive\n","    drive.mount('/content/drive', force_remount=True)\n","except:\n","    %env GOOGLE_COLAB=0\n","    print(\"Warning: Not a Colab Environment\")"]},{"cell_type":"markdown","metadata":{"id":"MBV_YWiTc_KM"},"source":["### 2.3 Setup Python environment <a class=\"anchor\" id=\"head-2-3\"></a>\n","Setup the environment necessary to run the TAO Networks by running the bash script"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s2Xygw-y8fjm"},"outputs":[],"source":["#FIXME\n","%env GENERAL_WHL_PATH=/content/drive/MyDrive/tf/general_whl\n","#FIXME\n","%env CODEBASE_WHL_PATH=/content/drive/MyDrive/tf/codebase_whl\n","\n","import os\n","if os.path.exists(os.environ[\"GENERAL_WHL_PATH\"]) and os.path.exists(os.environ[\"GENERAL_WHL_PATH\"]):\n","    if os.environ[\"GOOGLE_COLAB\"] == \"1\":\n","        os.environ[\"bash_script\"] = \"setup_env.sh\"\n","    else:\n","        os.environ[\"bash_script\"] = \"setup_env_desktop.sh\"\n","\n","    !sed -i \"s|PATH_TO_GENERAL_WHL|$GENERAL_WHL_PATH|g\" $COLAB_NOTEBOOKS_PATH/tensorflow/$bash_script\n","    !sed -i \"s|PATH_TO_CODEBASE_WHL|$CODEBASE_WHL_PATH|g\" $COLAB_NOTEBOOKS_PATH/tensorflow/$bash_script\n","    !sed -i \"s|PATH_TO_COLAB_NOTEBOOKS|$COLAB_NOTEBOOKS_PATH|g\" $COLAB_NOTEBOOKS_PATH/tensorflow/$bash_script\n","\n","    !sh $COLAB_NOTEBOOKS_PATH/tensorflow/$bash_script\n","else:\n","    raise(\"Error, enter the whl paths correctly\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if os.environ.get(\"PYTHONPATH\",\"\") == \"\":\n","    os.environ[\"PYTHONPATH\"] = \"\"\n","os.environ[\"PYTHONPATH\"]+=\":/opt/nvidia/\"\n","if os.environ[\"GOOGLE_COLAB\"] == \"1\":\n","    os.environ[\"PYTHONPATH\"]+=\":/usr/local/lib/python3.6/dist-packages/third_party/nvml\"\n","else:\n","    os.environ[\"PYTHONPATH\"]+=\":/home_duplicate/rarunachalam/miniconda3/envs/tf_py_36/lib/python3.6/site-packages/third_party/nvml\" # FIX MINICONDA PATH"]},{"cell_type":"markdown","metadata":{"id":"Fl8fSfXseED3"},"source":["### 2.4 Reset env variables <a class=\"anchor\" id=\"head-2-4\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l_T2vBdzeIcO"},"outputs":[],"source":["# Setting up env variables for cleaner command line commands.\n","import os\n","\n","%env TAO_DOCKER_DISABLE=1\n","\n","%env KEY=nvidia_tlt\n","%env NUM_GPUS=1\n","\n","# Change the paths according to your directory structure, these are just examples\n","%env COLAB_NOTEBOOKS_PATH=/content/drive/MyDrive/ColabNotebooks\n","if not os.path.exists(os.environ[\"COLAB_NOTEBOOKS_PATH\"]):\n","    raise(\"Error, enter the path of the colab notebooks repo correctly\")\n","%env EXPERIMENT_DIR=/results/yolo_v3\n","%env DATA_DIR=/content/drive/MyDrive/kitti_data/\n","\n","SPECS_DIR=f\"{os.environ['COLAB_NOTEBOOKS_PATH']}/tensorflow/yolo_v3/specs\"\n","%env SPECS_DIR={SPECS_DIR}\n","# Showing list of specification files.\n","!ls -rlt $SPECS_DIR"]},{"cell_type":"markdown","metadata":{"id":"eBGWmAf8BC9G"},"source":["## 3. Generate TF records <a class=\"anchor\" id=\"head-3\"></a>"]},{"cell_type":"markdown","metadata":{"id":"n-l8grK8BC9H"},"source":["Either TFRecord dataset format or the raw KITTI format(with directories of images and KITTI labels) can be used for training/validation. TFRecord dataset format is the recommended one as it is generally faster than reading numerious KITTI label files. However, in some cases, especially when training with small resolutions like 416x416, the experimental result shows TFRecord dataset is almost the same or slightly slower than the other format. So by default, we will use TFRecord dataset in this notebook. As a reference, we still provide the spec files for raw KITTI dataset format."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hzDngpyOBC9H"},"outputs":[],"source":["!sed -i \"s|TAO_DATA_PATH|$DATA_DIR/|g\" $SPECS_DIR/tfrecords_kitti_trainval.txt\n","!sed -i \"s|EXPERIMENT_DIR_PATH|$EXPERIMENT_DIR/|g\" $SPECS_DIR/tfrecords_kitti_trainval.txt\n","\n","print(\"TFrecords conversion spec file for training\")\n","!cat $SPECS_DIR/tfrecords_kitti_trainval.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":139107,"status":"ok","timestamp":1657386126025,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"fblO1_3vBC9H","outputId":"ab5d75b6-64a2-4677-a013-bbedaf61e645"},"outputs":[],"source":["# KITTI trainval\n","!tao yolo_v3 dataset_convert -d $SPECS_DIR/tfrecords_kitti_trainval.txt \\\n","                     -o $DATA_DIR/tfrecords/kitti_trainval/kitti_trainval"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X-AhhZcBBC9H"},"outputs":[],"source":["# If you use your own dataset, you will need to run the code below to generate the best anchor shape\n","\n","# !tao yolo_v3 kmeans -l $DATA_DIR/training/label_2 \\\n","#                     -i $DATA_DIR/training/image_2 \\\n","#                     -n 9 \\\n","#                     -x 1248 \\\n","#                     -y 384\n","\n","# The anchor shape generated by this script is sorted. Write the first 3 into small_anchor_shape in the config\n","# file. Write middle 3 into mid_anchor_shape. Write last 3 into big_anchor_shape."]},{"cell_type":"markdown","metadata":{"id":"RYHWllQpBC9K"},"source":["## 4. Provide training specification <a class=\"anchor\" id=\"head-4\"></a>\n","* Augmentation parameters for on-the-fly data augmentation\n","* Other training (hyper-)parameters such as batch size, number of epochs, learning rate etc.\n","* Whether to use quantization aware training (QAT)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5kHblN6ZBC9K"},"outputs":[],"source":["# Provide pretrained model path\n","!sed -i \"s|TAO_DATA_PATH|$DATA_DIR/|g\" $SPECS_DIR/yolo_v3_train_resnet18_tfrecord.txt\n","!sed -i \"s|EXPERIMENT_DIR_PATH|$EXPERIMENT_DIR/|g\" $SPECS_DIR/yolo_v3_train_resnet18_tfrecord.txt\n","\n","# To enable QAT training on sample spec file, uncomment following lines\n","# !sed -i \"s/enable_qat: false/enable_qat: true/g\" $SPECS_DIR/yolo_v3_train_resnet18_tfrecord.txt\n","# !sed -i \"s/enable_qat: false/enable_qat: true/g\" $SPECS_DIR/yolo_v3_retrain_resnet18_tfrecord.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u1fwGYqbBC9K"},"outputs":[],"source":["# By default, the sample spec file disables QAT training. You can force non-QAT training by running lines below\n","# !sed -i \"s/enable_qat: true/enable_qat: false/g\" $SPECS_DIR/yolo_v3_train_resnet18_tfrecord.txt\n","# !sed -i \"s/enable_qat: true/enable_qat: false/g\" $SPECS_DIR/yolo_v3_retrain_resnet18_tfrecord.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cZzwm4PjBC9K"},"outputs":[],"source":["!cat $SPECS_DIR/yolo_v3_train_resnet18_tfrecord.txt"]},{"cell_type":"markdown","metadata":{"id":"T2_AcSVRBC9K"},"source":["## 5. Run TAO training <a class=\"anchor\" id=\"head-5\"></a>\n","* Provide the sample spec file and the output directory location for models\n","* WARNING: training will take several hours or one day to complete"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1657386148731,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"3sJKVS4DBC9L"},"outputs":[],"source":["!mkdir -p $EXPERIMENT_DIR/experiment_dir_unpruned"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1013489,"status":"ok","timestamp":1657387348988,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"3PvvDHXcBC9L","outputId":"6985301f-6ed5-4e69-bf6d-6fb2999a707b","scrolled":true},"outputs":[],"source":["print(\"To run with multigpu, please change --gpus based on the number of available GPUs in your machine.\")\n","!tao yolo_v3 train -e $SPECS_DIR/yolo_v3_train_resnet18_tfrecord.txt \\\n","                   -r $EXPERIMENT_DIR/experiment_dir_unpruned \\\n","                   -k $KEY \\\n","                   --gpus 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ktIIHX2mBC9L"},"outputs":[],"source":["print(\"To resume from checkpoint, please change pretrain_model_path to resume_model_path in config file.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1657387348989,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"6VPatwjMBC9M","outputId":"54071c87-c892-49d2-80ad-3e67317f3a1f"},"outputs":[],"source":["print('Model for each epoch:')\n","print('---------------------')\n","!ls -ltrh $EXPERIMENT_DIR/experiment_dir_unpruned/weights"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1657387348989,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"384WSWFaBC9M","outputId":"2ee8e336-3e35-4bef-a737-abf0c1a81090"},"outputs":[],"source":["# Now check the evaluation stats in the csv file and pick the model with highest eval accuracy.\n","!cat $EXPERIMENT_DIR/experiment_dir_unpruned/yolov3_training_log_resnet18.csv\n","%env EPOCH=080"]},{"cell_type":"markdown","metadata":{"id":"6OBDVIylBC9M"},"source":["## 6. Evaluate trained models <a class=\"anchor\" id=\"head-6\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33484,"status":"ok","timestamp":1657387382468,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"60YTRCATBC9M","outputId":"c4b11d38-b3ca-4689-da66-30f3651d9ae0","scrolled":true},"outputs":[],"source":["!tao yolo_v3 evaluate -e $SPECS_DIR/yolo_v3_train_resnet18_tfrecord.txt \\\n","                      -m $EXPERIMENT_DIR/experiment_dir_unpruned/weights/yolov3_resnet18_epoch_$EPOCH.tlt \\\n","                      -k $KEY"]},{"cell_type":"markdown","metadata":{"id":"cSvCjq3YBC9M"},"source":["## 7. Prune trained models <a class=\"anchor\" id=\"head-7\"></a>\n","* Specify pre-trained model\n","* Equalization criterion (`Only for resnets as they have element wise operations or MobileNets.`)\n","* Threshold for pruning.\n","* A key to save and load the model\n","* Output directory to store the model\n","\n","Usually, you just need to adjust `-pth` (threshold) for accuracy and model size trade off. Higher `pth` gives you smaller model (and thus higher inference speed) but worse accuracy. The threshold value depends on the dataset and the model. `0.5` in the block below is just a start point. If the retrain accuracy is good, you can increase this value to get smaller models. Otherwise, lower this value to get better accuracy."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1657387382469,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"VlMZY6t0BC9N"},"outputs":[],"source":["!mkdir -p $EXPERIMENT_DIR/experiment_dir_pruned"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":269471,"status":"ok","timestamp":1657387651931,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"zWz368nABC9N","outputId":"ceda7c7f-7068-4e1e-b566-c8a9ec8c6683","scrolled":true},"outputs":[],"source":["!tao yolo_v3 prune -m $EXPERIMENT_DIR/experiment_dir_unpruned/weights/yolov3_resnet18_epoch_$EPOCH.tlt \\\n","                   -e $SPECS_DIR/yolo_v3_train_resnet18_tfrecord.txt \\\n","                   -o $EXPERIMENT_DIR/experiment_dir_pruned/yolov3_resnet18_pruned.tlt \\\n","                   -eq intersection \\\n","                   -pth 0.1 \\\n","                   -k $KEY"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32,"status":"ok","timestamp":1657387651931,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"QrZd6kxCBC9N","outputId":"9c7a91d2-0245-4992-885d-ace89c64d652"},"outputs":[],"source":["!ls -rlt $EXPERIMENT_DIR/experiment_dir_pruned/"]},{"cell_type":"markdown","metadata":{"id":"3PzxledvBC9N"},"source":["## 8. Retrain pruned models <a class=\"anchor\" id=\"head-8\"></a>\n","* Model needs to be re-trained to bring back accuracy after pruning\n","* Specify re-training specification\n","* WARNING: training will take several hours or one day to complete"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MsP8xWs1BC9N","scrolled":true},"outputs":[],"source":["# Printing the retrain spec file. \n","# Here we have updated the spec file to include the newly pruned model as a pretrained weights.\n","!sed -i \"s|TAO_DATA_PATH|$DATA_DIR/|g\" $SPECS_DIR/yolo_v3_retrain_resnet18_tfrecord.txt\n","!sed -i \"s|EXPERIMENT_DIR_PATH|$EXPERIMENT_DIR/|g\" $SPECS_DIR/yolo_v3_retrain_resnet18_tfrecord.txt\n","!cat $SPECS_DIR/yolo_v3_retrain_resnet18_tfrecord.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1657387651932,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"FmfrnS5bBC9N"},"outputs":[],"source":["!mkdir -p $EXPERIMENT_DIR/experiment_dir_retrain"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":741420,"status":"ok","timestamp":1657388393343,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"aqkcwz4ABC9N","outputId":"0cf71387-13b5-4272-9d0f-832d661c57f3"},"outputs":[],"source":["# Retraining using the pruned model as pretrained weights \n","!tao yolo_v3 train --gpus 1 \\\n","                   -e $SPECS_DIR/yolo_v3_retrain_resnet18_tfrecord.txt \\\n","                   -r $EXPERIMENT_DIR/experiment_dir_retrain \\\n","                   -k $KEY"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":38,"status":"ok","timestamp":1657388393344,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"ceoiOR50BC9N","outputId":"29357a2d-c5a1-4e86-f292-2be9115cd9ee"},"outputs":[],"source":["# Listing the newly retrained model.\n","!ls -rlt $EXPERIMENT_DIR/experiment_dir_retrain/weights"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":447,"status":"ok","timestamp":1657388441106,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"AcPsiGxCBC9O","outputId":"a3e5ffbf-ed1d-4f64-8b35-d936cf4497c1"},"outputs":[],"source":["# Now check the evaluation stats in the csv file and pick the model with highest eval accuracy.\n","!cat $EXPERIMENT_DIR/experiment_dir_retrain/yolov3_training_log_resnet18.csv\n","%env EPOCH=050"]},{"cell_type":"markdown","metadata":{"id":"mua9cyw6BC9O"},"source":["## 9. Evaluate retrained model <a class=\"anchor\" id=\"head-9\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":35757,"status":"ok","timestamp":1657388485048,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"asgQyZrwBC9O","outputId":"7ea1e341-8034-4d96-ea26-cca7083566eb"},"outputs":[],"source":["!tao yolo_v3 evaluate -e $SPECS_DIR/yolo_v3_retrain_resnet18_tfrecord.txt \\\n","                      -m $EXPERIMENT_DIR/experiment_dir_retrain/weights/yolov3_resnet18_epoch_$EPOCH.tlt \\\n","                      -k $KEY"]},{"cell_type":"markdown","metadata":{"id":"9PZV9ZjZBC9O"},"source":["## 10. Visualize inferences <a class=\"anchor\" id=\"head-10\"></a>\n","In this section, we run the `infer` tool to generate inferences on the trained models and visualize the results."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":33,"status":"ok","timestamp":1657388485050,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"-ijYQlwIBC9O"},"outputs":[],"source":["# Copy some test images\n","!mkdir -p $DATA_DIR/test_samples\n","!cp $DATA_DIR/testing/image_2/000* $DATA_DIR/test_samples/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":42571,"status":"ok","timestamp":1657388527606,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"5sGGK8nWBC9O","outputId":"dcaf724c-697c-4097-edfc-caf8a448de34"},"outputs":[],"source":["# Running inference for detection on n images\n","!tao yolo_v3 inference -i $DATA_DIR/test_samples \\\n","                       -o $EXPERIMENT_DIR/yolo_infer_images \\\n","                       -e $SPECS_DIR/yolo_v3_retrain_resnet18_tfrecord.txt \\\n","                       -m $EXPERIMENT_DIR/experiment_dir_retrain/weights/yolov3_resnet18_epoch_$EPOCH.tlt \\\n","                       -l $EXPERIMENT_DIR/yolo_infer_labels \\\n","                       -k $KEY"]},{"cell_type":"markdown","metadata":{"id":"Zv0-v7QnBC9P"},"source":["The `inference` tool produces two outputs. \n","1. Overlain images in `$EXPERIMENT_DIR/yolo_infer_images`\n","2. Frame by frame bbox labels in kitti format located in `$EXPERIMENT_DIR/yolo_infer_labels`"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1284,"status":"ok","timestamp":1657388528881,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"8jGNFDDNBC9P","outputId":"251bcc79-b182-489e-c3da-2588d8900287"},"outputs":[],"source":["# Simple grid visualizer\n","import matplotlib.pyplot as plt\n","import os\n","from math import ceil\n","valid_image_ext = ['.jpg', '.png', '.jpeg', '.ppm']\n","\n","def visualize_images(image_dir, num_cols=4, num_images=10):\n","    output_path = os.path.join(os.environ['EXPERIMENT_DIR'], image_dir)\n","    num_rows = int(ceil(float(num_images) / float(num_cols)))\n","    f, axarr = plt.subplots(num_rows, num_cols, figsize=[80,30])\n","    f.tight_layout()\n","    a = [os.path.join(output_path, image) for image in os.listdir(output_path) \n","         if os.path.splitext(image)[1].lower() in valid_image_ext]\n","    for idx, img_path in enumerate(a[:num_images]):\n","        col_id = idx % num_cols\n","        row_id = idx // num_cols\n","        img = plt.imread(img_path)\n","        axarr[row_id, col_id].imshow(img) "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":624},"executionInfo":{"elapsed":3919,"status":"error","timestamp":1657388766369,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"pM9CjZGCBC9P","outputId":"a7a47aff-3a2b-42f1-ac68-74f6af74f16d"},"outputs":[],"source":["!mkdir -p $EXPERIMENT_DIR/yolo_infer_images\n","# Visualizing the sample images.\n","OUTPUT_PATH = 'yolo_infer_images' # relative path from $EXPERIMENT_DIR.\n","COLS = 3 # number of columns in the visualizer grid.\n","IMAGES = 9 # number of images to visualize.\n","\n","visualize_images(OUTPUT_PATH, num_cols=COLS, num_images=IMAGES)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"yolo_v3.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.10.2 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.2"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":0}
