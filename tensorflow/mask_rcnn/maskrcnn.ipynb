{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Get the TensorRT tar file before running this Notebook\n","\n","1. Visit https://developer.nvidia.com/tensorrt\n","2. Clicking `Download now` from step one directs you to https://developer.nvidia.com/nvidia-tensorrt-download where you have to Login/Join Now for Nvidia Developer Program Membership\n","3. Now, in the download page: Choose TensorRT 8 in available versions\n","4. Agree to Terms and Conditions\n","5. Click on TensorRT 8.6 GA to expand the available options\n","6. Click on 'TensorRT 8.6 GA for Linux x86_64 and CUDA 12.0 and 12.1 TAR Package' to dowload the TAR file\n","7. Upload the the tar file to your Google Drive"]},{"cell_type":"markdown","metadata":{"id":"LGLBrzF8hKgS"},"source":["## Connect to GPU Instance\n","\n","1. Change Runtime type to GPU by Runtime(Top Left tab)->Change Runtime Type->GPU(Hardware Accelerator)\n","1. Then click on Connect (Top Right)"]},{"cell_type":"markdown","metadata":{"id":"SjpjyNg5c2V9"},"source":["## Mounting Google drive\n","Mount your Google drive storage to this Colab instance"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18056,"status":"ok","timestamp":1658620835786,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"EvUVkYw0hzqG","outputId":"a8f580a7-bd55-4a9c-8620-c0795752fbc9"},"outputs":[],"source":["import sys\n","if 'google.colab' in sys.modules:\n","    %env GOOGLE_COLAB=1\n","    from google.colab import drive\n","    drive.mount('/content/drive', force_remount=True)\n","else:\n","    %env GOOGLE_COLAB=0\n","    print(\"Warning: Not a Colab Environment\")"]},{"cell_type":"markdown","metadata":{"id":"pzntFQFG9bto"},"source":["# Instance Segmentation using TAO MaskRCNN\n","\n","Transfer learning is the process of transferring learned features from one application to another. It is a commonly used training technique where you use a model trained on one task and re-train to use it on a different task. \n","\n","Train Adapt Optimize (TAO) Toolkit  is a simple and easy-to-use Python based AI toolkit for taking purpose-built AI models and customizing them with users' own data.\n","\n","<img align=\"center\" src=\"https://developer.nvidia.com/sites/default/files/akamai/TAO/tlt-tao-toolkit-bring-your-own-model-diagram.png\" width=\"1080\"> "]},{"cell_type":"markdown","metadata":{"id":"BaRelNDw9btr"},"source":["## Learning Objectives\n","In this notebook, you will learn how to leverage the simplicity and convenience of TAO to:\n","\n","* Take a pretrained resnet50 model and train a MaskRCNN model on COCO dataset\n","* Evaluate the trained model\n","* Run Inference with the trained model and visualize the result\n","* Export the trained model to a .etlt file for deployment to DeepStream\n","\n","### Table of Contents\n","This notebook shows an example use case for instance segmentation using the Train Adapt Optimize (TAO) Toolkit.\n","\n","0. [Set up env variables](#head-0)\n","1. [Prepare dataset and pre-trained model](#head-1)\n","2. [Setup GPU environment](#head-2) <br>\n","    2.1 [Setup Python environment](#head-2-1) <br>\n","3. [Generate tfrecords](#head-3)\n","4. [Provide training specification](#head-4)\n","5. [Run TAO training](#head-5)\n","6. [Evaluate trained models](#head-6)\n","7. [Prune trained model](#head-7)\n","8. [Retrain pruned models](#head-8)\n","9. [Evaluate retrained model](#head-9)\n","10. [Visualize inferences](#head-10)"]},{"cell_type":"markdown","metadata":{"id":"fDyiwOAQ9bts"},"source":["#### Note\n","1. This notebook currently is by default set up to run training using 1 GPU. To use more GPU's please update the env variable `$NUM_GPUS` accordingly\n","2. This notebook uses COCO dataset by default, which should be around ~25 GB. If you are limited by Google-Drive storage, we recommend to:\n","\n","    i. Download the dataset onto the local system\n","\n","    ii. Run the utility script at $COLAB_NOTEBOOKS/tensorflow/utils/generate_coco_subset.py in your local system\n","\n","    iii. This generates a subset of coco dataset with number of sample images you wish for\n","\n","    iv. Upload this subset onto Google Drive\n","\n","3. Using the default config/spec file provided in this notebook, each weight file size of mask-rcnn created during training will be ~354 MB\n","\n","## 0. Set up env variables and set FIXME parameters <a class=\"anchor\" id=\"head-0\"></a>\n","\n","*Note: This notebook currently is by default set up to run training using 1 GPU. To use more GPU's please update the env variable `$NUM_GPUS` accordingly*\n","\n","#### FIXME\n","1. NUM_GPUS - set this to <= number of GPU's availble on the instance\n","1. COLAB_NOTEBOOKS_PATH - for Google Colab environment, set this path where you want to clone the repo to; for local system environment, set this path to the already cloned repo\n","1. EXPERIMENT_DIR - set this path to a folder location where pretrained models, checkpoints and log files during different model actions will be saved\n","1. delete_existing_experiments - set to True to remove existing pretrained models, checkpoints and log files of a previous experiment\n","1. DATA_DIR - set this path to a folder location where you want to dataset to be present\n","1. delete_existing_data - set this to True to remove existing preprocessed and original data\n","1. trt_tar_path - set this path of the uploaded TensorRT tar.gz file after browser download\n","1. trt_untar_folder_path - set to path of the folder where the TensoRT tar.gz file has to be untarred into\n","1. trt_version - set this to the version of TRT you have downloaded"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":196,"status":"ok","timestamp":1657338706629,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"atlBUHEV9btt","outputId":"c46e224b-7c34-4a46-abda-929936533655"},"outputs":[],"source":["# Setting up env variables for cleaner command line commands.\n","import os\n","\n","%env TAO_DOCKER_DISABLE=1\n","\n","%env KEY=nvidia_tlt\n","#FIXME1\n","%env NUM_GPUS=1\n","\n","#FIXME2\n","%env COLAB_NOTEBOOKS_PATH=/content/drive/MyDrive/nvidia-tao\n","if os.environ[\"GOOGLE_COLAB\"] == \"1\":\n","    if not os.path.exists(os.path.join(os.environ[\"COLAB_NOTEBOOKS_PATH\"])):\n","\n","      !git clone https://github.com/NVIDIA-AI-IOT/nvidia-tao.git $COLAB_NOTEBOOKS_PATH\n","else:\n","    if not os.path.exists(os.environ[\"COLAB_NOTEBOOKS_PATH\"]):\n","        raise Exception(\"Error, enter the path of the colab notebooks repo correctly\")\n","\n","#FIXME3\n","%env EXPERIMENT_DIR=/content/drive/MyDrive/results/mask_rcnn\n","#FIXME4\n","delete_existing_experiments = True\n","#FIXME5\n","%env DATA_DIR=/content/drive/MyDrive/coco_data/\n","#FIXME6\n","delete_existing_data = False\n","\n","if delete_existing_experiments:\n","    !sudo rm -rf $EXPERIMENT_DIR\n","if delete_existing_data:\n","    !sudo rm -rf $DATA_DIR\n","\n","SPECS_DIR=f\"{os.environ['COLAB_NOTEBOOKS_PATH']}/tensorflow/mask_rcnn/specs\"\n","%env SPECS_DIR={SPECS_DIR}\n","# Showing list of specification files.\n","!ls -rlt $SPECS_DIR\n","\n","!sudo mkdir -p $DATA_DIR && sudo chmod -R 777 $DATA_DIR\n","!sudo mkdir -p $EXPERIMENT_DIR && sudo chmod -R 777 $EXPERIMENT_DIR"]},{"cell_type":"markdown","metadata":{"id":"WJx_yoZ39btv"},"source":["## 1. Prepare dataset and pre-trained model <a class=\"anchor\" id=\"head-1\"></a>"]},{"cell_type":"markdown","metadata":{"id":"Y3fwDUWu9btw"},"source":[" We will be using the COCO dataset for the tutorial. The following script will download COCO dataset automatically and convert it to TFRecords. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":193,"status":"ok","timestamp":1657338706815,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"EW78Zi2A9btw","outputId":"0401d897-3c52-47c8-83f4-f4f6551aa5de","scrolled":true},"outputs":[],"source":["# Download and preprocess data\n","!bash $SPECS_DIR/download_coco.sh $DATA_DIR"]},{"cell_type":"markdown","metadata":{"id":"Ihz-F_DC9btw"},"source":["Note that the dataset conversion scripts provided in `specs` are intended for the standard COCO dataset. If your data doesn't have `caption` groundtruth or test set, you can modify `download_and_preprocess_coco.sh` and `create_coco_tf_record.py` by commenting out corresponding variables."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":178,"status":"ok","timestamp":1657338706990,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"8Erlawaz9btw","outputId":"2518741e-ac57-423d-e3b3-ad10f47edafa"},"outputs":[],"source":["# verify\n","!ls -l $DATA_DIR/raw-data"]},{"cell_type":"markdown","metadata":{"id":"ths8klL19btw"},"source":["### Download pretrained model from NGC"]},{"cell_type":"markdown","metadata":{"id":"nPba2vTj9btx"},"source":[" We will use NGC CLI to get the pre-trained models. For more details, go to ngc.nvidia.com and click the SETUP on the navigation bar."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1800,"status":"ok","timestamp":1657338708786,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"C_GkXOAu9btx","outputId":"fd422cd7-70e2-4371-c1b9-4bd7442d529a"},"outputs":[],"source":["# Installing NGC CLI on the local machine.\n","## Download and install\n","%env LOCAL_PROJECT_DIR=/ngc_content/\n","%env CLI=ngccli_cat_linux.zip\n","!sudo mkdir -p $LOCAL_PROJECT_DIR/ngccli && sudo chmod -R 777 $LOCAL_PROJECT_DIR\n","\n","# Remove any previously existing CLI installations\n","!sudo rm -rf $LOCAL_PROJECT_DIR/ngccli/*\n","!wget --content-disposition 'https://api.ngc.nvidia.com/v2/resources/nvidia/ngc-apps/ngc_cli/versions/3.23.0/files/ngccli_linux.zip' -P $LOCAL_PROJECT_DIR/ngccli -O $LOCAL_PROJECT_DIR/ngccli/$CLI\n","!unzip -u -q \"$LOCAL_PROJECT_DIR/ngccli/$CLI\" -d $LOCAL_PROJECT_DIR/ngccli/\n","!rm $LOCAL_PROJECT_DIR/ngccli/*.zip\n","os.environ[\"PATH\"]=\"{}/ngccli/ngc-cli:{}\".format(os.getenv(\"LOCAL_PROJECT_DIR\", \"\"), os.getenv(\"PATH\", \"\"))\n","!cp /usr/lib/x86_64-linux-gnu/libstdc++.so.6 $LOCAL_PROJECT_DIR/ngccli/ngc-cli/libstdc++.so.6"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2987,"status":"ok","timestamp":1657338711767,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"FqROYQIE9btx","outputId":"e83c655e-bef4-4140-8722-6b28fdde6bae"},"outputs":[],"source":["!ngc registry model list nvidia/tao/pretrained_instance_segmentation:*"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1657338711767,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"tNXI-Id59btx"},"outputs":[],"source":["!mkdir -p $EXPERIMENT_DIR/pretrained_resnet50/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9335,"status":"ok","timestamp":1657338721097,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"SmzChNzr9btx","outputId":"02494668-fad4-4473-88bd-7a57093b5659"},"outputs":[],"source":["# Pull pretrained model from NGC\n","!ngc registry model download-version nvidia/tao/pretrained_instance_segmentation:resnet50 --dest $EXPERIMENT_DIR/pretrained_resnet50"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1657338721098,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"dlYJKHq29bty","outputId":"e7882395-6c46-4bf7-d493-21e7558ee08b"},"outputs":[],"source":["print(\"Check that model is downloaded into dir.\")\n","!ls -l $EXPERIMENT_DIR/pretrained_resnet50/pretrained_instance_segmentation_vresnet50"]},{"cell_type":"markdown","metadata":{"id":"_26rCobXcri1"},"source":["## 2. Setup GPU environment <a class=\"anchor\" id=\"head-2\"></a>\n"]},{"cell_type":"markdown","metadata":{"id":"MBV_YWiTc_KM"},"source":["### 2.1 Setup Python environment <a class=\"anchor\" id=\"head-2-1\"></a>\n","Setup the environment necessary to run the TAO Networks by running the bash script"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s2Xygw-y8fjm"},"outputs":[],"source":["# FIXME 7: set this path of the uploaded TensorRT tar.gz file after browser download\n","trt_tar_path=\"/content/drive/MyDrive/TensorRT-8.6.1.6.Linux.x86_64-gnu.cuda-12.0.tar.gz\"\n","\n","import os\n","if not os.path.exists(trt_tar_path):\n","  raise Exception(\"TAR file not found in the provided path\")\n","\n","# FIXME 8: set to path of the folder where the TensoRT tar.gz file has to be untarred into\n","%env trt_untar_folder_path=/content/trt_untar\n","# FIXME 9: set this to the version of TRT you have downloaded\n","%env trt_version=8.6.1.6\n","\n","!sudo mkdir -p $trt_untar_folder_path && sudo chmod -R 777 $trt_untar_folder_path/\n","\n","import os\n","\n","untar = True\n","for fname in os.listdir(os.environ.get(\"trt_untar_folder_path\", None)):\n","  if fname.startswith(\"TensorRT-\"+os.environ.get(\"trt_version\")) and not fname.endswith(\".tar.gz\"):\n","    untar = False\n","\n","if untar:\n","  !tar -xzf $trt_tar_path -C /content/trt_untar\n","\n","if os.environ.get(\"LD_LIBRARY_PATH\",\"\") == \"\":\n","  os.environ[\"LD_LIBRARY_PATH\"] = \"\"\n","trt_lib_path = f':{os.environ.get(\"trt_untar_folder_path\")}/TensorRT-{os.environ.get(\"trt_version\")}/lib'\n","os.environ[\"LD_LIBRARY_PATH\"]+=trt_lib_path"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","if os.environ[\"GOOGLE_COLAB\"] == \"1\":\n","    os.environ[\"bash_script\"] = \"setup_env.sh\"\n","else:\n","    os.environ[\"bash_script\"] = \"setup_env_desktop.sh\"\n","\n","!sed -i \"s|PATH_TO_TRT|$trt_untar_folder_path|g\" $COLAB_NOTEBOOKS_PATH/tensorflow/$bash_script\n","!sed -i \"s|TRT_VERSION|$trt_version|g\" $COLAB_NOTEBOOKS_PATH/tensorflow/$bash_script\n","!sed -i \"s|PATH_TO_COLAB_NOTEBOOKS|$COLAB_NOTEBOOKS_PATH|g\" $COLAB_NOTEBOOKS_PATH/tensorflow/$bash_script\n","\n","!sh $COLAB_NOTEBOOKS_PATH/tensorflow/$bash_script"]},{"cell_type":"markdown","metadata":{"id":"-V-E7Feo9bty"},"source":["## 3. Generate tfrecords <a class=\"anchor\" id=\"head-3\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[" # convert training data to TFRecords\n","!tao model mask_rcnn dataset_convert -i $DATA_DIR/raw-data/train2017 \\\n","                               -a $DATA_DIR/raw-data/annotations/instances_train2017.json \\\n","                               -o $DATA_DIR --include_masks -t train -s 256"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[" # convert validation data to TFRecords\n","!tao model mask_rcnn dataset_convert -i $DATA_DIR/raw-data/val2017 \\\n","                               -a $DATA_DIR/raw-data/annotations/instances_val2017.json \\\n","                               -o $DATA_DIR --include_masks -t val -s 32"]},{"cell_type":"markdown","metadata":{"id":"-V-E7Feo9bty"},"source":["## 4. Provide training specification <a class=\"anchor\" id=\"head-4\"></a>\n","* Tfrecords for the train datasets\n","    * In order to use the newly generated tfrecords, update the dataset_config parameter in the spec file at `$SPECS_DIR/maskrcnn_train_resnet50.txt` \n","Note that the learning rate in the spec file is set for 4 GPU training. If you have N gpus, you should divide LR by 4/N.\n","* Pre-trained models\n","* Augmentation parameters for on the fly data augmentation\n","* Other training (hyper-)parameters such as batch size, number of epochs, learning rate etc.\n","* **Note that the sample spec is not meant to produce SOTA accuracy on COCO. To reproduce SOTA, you might want to use TAO to train an ImageNet model first and change the total_steps to 100K or above. In one experiment, we got 37+% AP and 34% mask_AP with 8GPU training for 100K.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t0cOyxMq9bty","scrolled":true},"outputs":[],"source":["!sed -i \"s|TAO_DATA_PATH|$DATA_DIR/|g\" $SPECS_DIR/maskrcnn_train_resnet50.txt\n","!sed -i \"s|EXPERIMENT_DIR_PATH|$EXPERIMENT_DIR/|g\" $SPECS_DIR/maskrcnn_train_resnet50.txt\n","!cat $SPECS_DIR/maskrcnn_train_resnet50.txt"]},{"cell_type":"markdown","metadata":{"id":"OpOx2qA09bty"},"source":["## 5. Train a MaskRCNN model <a class=\"anchor\" id=\"head-5\"></a>\n","* Provide the sample spec file and the output directory location for models\n","* Evaluation uses COCO metrics. For more info, please refer to: https://cocodataset.org/#detection-eval\n","* WARNING: training will take several hours or one day to complete"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":225,"status":"ok","timestamp":1657338721318,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"GoOCsnhI9bty"},"outputs":[],"source":["!rm -rf $EXPERIMENT_DIR/experiment_dir_unpruned && mkdir -p $EXPERIMENT_DIR/experiment_dir_unpruned"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9098451,"status":"ok","timestamp":1657354444112,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"1sUCAZHF9bty","outputId":"ed54f8ba-c0a7-4030-d38a-90abab782dd2","scrolled":true},"outputs":[],"source":["print(\"For multi-GPU, change --gpus based on your machine.\")\n","!tao model mask_rcnn train -e $SPECS_DIR/maskrcnn_train_resnet50.txt \\\n","                     -d $EXPERIMENT_DIR/experiment_dir_unpruned\\\n","                     -k $KEY \\\n","                     --gpus 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AJKgusnV9btz"},"outputs":[],"source":["print(\"To resume training from a checkpoint, simply run the same training script. It will pick up from where it's left.\")\n","# !tao model mask_rcnn train -e $SPECS_DIR/maskrcnn_train_resnet50.txt \\\n","#                      -d $EXPERIMENT_DIR/experiment_dir_unpruned\\\n","#                      -k $KEY \\\n","#                      --gpus 1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":182,"status":"ok","timestamp":1657354527137,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"Uf_ErL3v9btz","outputId":"02455705-bd0b-4d59-9b75-fef3c684b313"},"outputs":[],"source":["print('Model for each epoch:')\n","print('---------------------')\n","!ls -ltrh $EXPERIMENT_DIR/experiment_dir_unpruned/"]},{"cell_type":"markdown","metadata":{"id":"GeQ0GmiS9btz"},"source":["## 6. Evaluate trained models <a class=\"anchor\" id=\"head-6\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":205,"status":"ok","timestamp":1657354535465,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"w1rivlxb9btz","outputId":"1333f191-0134-430d-d50d-5ff2b91f0116"},"outputs":[],"source":["%env NUM_EPOCH=10"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":173394,"status":"ok","timestamp":1657354710133,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"qQIc2DpO9btz","outputId":"83e1584a-8ffb-49bc-f524-604e4fde6d6e","scrolled":true},"outputs":[],"source":["!tao model mask_rcnn evaluate -e $SPECS_DIR/maskrcnn_train_resnet50.txt \\\n","                        -m $EXPERIMENT_DIR/experiment_dir_unpruned/model.epoch-$10.tlt \\\n","                        -k $KEY"]},{"cell_type":"markdown","metadata":{"id":"jJ6auUw89bt0"},"source":["## 7. Prune <a class=\"anchor\" id=\"head-7\"></a>"]},{"cell_type":"markdown","metadata":{"id":"LYS-wXjO9bt0"},"source":["- Specify pre-trained model\n","- Equalization criterion (Only for resnets as they have element wise operations or MobileNets.)\n","- Threshold for pruning.\n","- A key to save and load the model\n","- Output directory to store the model\n","\n","Usually, you just need to adjust -pth (threshold) for accuracy and model size trade off. Higher pth gives you smaller model (and thus higher inference speed) but worse accuracy. The threshold value depends on the dataset and the model. 0.4 in the block below is just a start point. If the retrain accuracy is good, you can increase this value to get smaller models. Otherwise, lower this value to get better accuracy."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1657354710134,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"8MB5Am0K9bt0"},"outputs":[],"source":["# Create an output directory to save the pruned model.\n","!mkdir -p $EXPERIMENT_DIR/experiment_dir_pruned"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":247736,"status":"ok","timestamp":1657354957863,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"Vo9IcvWG9bt1","outputId":"17917769-bb6e-4a52-e7fe-5d20ac930e35"},"outputs":[],"source":["!tao model mask_rcnn prune -m $EXPERIMENT_DIR/experiment_dir_unpruned/model.epoch-$NUM_EPOCH.tlt \\\n","                     -o $EXPERIMENT_DIR/experiment_dir_pruned \\\n","                     -pth 0.7 \\\n","                     -k $KEY"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1657354957863,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"srnhLp7Z9bt1","outputId":"d79e312e-43e1-4150-9616-253097301b69"},"outputs":[],"source":["!ls -l $EXPERIMENT_DIR/experiment_dir_pruned"]},{"cell_type":"markdown","metadata":{"id":"KLPxBwPk9bt1"},"source":["**Note** that you should retrain the pruned model first, as it cannot be directly used for evaluation or inference. "]},{"cell_type":"markdown","metadata":{"id":"BMFbux_J9bt2"},"source":["## 8. Retrain pruned models <a class=\"anchor\" id=\"head-8\"></a>"]},{"cell_type":"markdown","metadata":{"id":"Bg2DGP-m9bt2"},"source":["- Model needs to be re-trained to bring back accuracy after pruning\n","- Specify re-training specification\n","- WARNING: training will take several hours or one day to complete"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vN2v4lVc9bt2"},"outputs":[],"source":["!sed -i \"s|TAO_DATA_PATH|$DATA_DIR/|g\" $SPECS_DIR/maskrcnn_retrain_resnet50.txt\n","!sed -i \"s|EXPERIMENT_DIR_PATH|$EXPERIMENT_DIR/|g\" $SPECS_DIR/maskrcnn_retrain_resnet50.txt\n","!cat $SPECS_DIR/maskrcnn_retrain_resnet50.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1657354469556,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"xHs4obTb9bt2"},"outputs":[],"source":["!mkdir -p $EXPERIMENT_DIR/experiment_dir_retrain"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8665,"status":"ok","timestamp":1657354966520,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"AJGiifIy9bt3","outputId":"70a03dbd-ba82-4302-8a03-eb46dc1b4cbb"},"outputs":[],"source":["!tao model mask_rcnn train -e $SPECS_DIR/maskrcnn_retrain_resnet50.txt \\\n","                     -d $EXPERIMENT_DIR/experiment_dir_retrain\\\n","                     -k $KEY \\\n","                     --gpus 1"]},{"cell_type":"markdown","metadata":{"id":"q2rR7wIZ9bt3"},"source":["## 9. Evaluate retrained model <a class=\"anchor\" id=\"head-9\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1657354478349,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"mvppaQjb9bt3","outputId":"7db89e0e-f8c6-44b0-c5ef-4042708a6b8a"},"outputs":[],"source":["%env NUM_EPOCH=10"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8684,"status":"ok","timestamp":1657354487029,"user":{"displayName":"tao-team-google-admins nvidia","userId":"12428471056663112246"},"user_tz":420},"id":"a7wAaAt09bt4","outputId":"865d1986-7ee3-4e96-a641-d254f06a8740"},"outputs":[],"source":["!tao model mask_rcnn evaluate -e $SPECS_DIR/maskrcnn_retrain_resnet50.txt \\\n","                        -m $EXPERIMENT_DIR/experiment_dir_retrain/model.epoch-$NUM_EPOCH.tlt \\\n","                        -k $KEY"]},{"cell_type":"markdown","metadata":{"id":"sXp4jFSu9bt4"},"source":["## 10. Visualize inferences <a class=\"anchor\" id=\"head-10\"></a>\n","In this section, we run the `infer` tool to generate inferences on the trained models and visualize the results. The `infer` tool produces annotated image outputs. You can choose to draw bounding boxes only or draw both bboxes and masks."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YLvOMsks9bt4","scrolled":true},"outputs":[],"source":["# Running inference for detection on n images\n","!tao model mask_rcnn inference -i $DATA_DIR/val/images \\\n","                         -r $EXPERIMENT_DIR/maskrcnn_annotated_images \\\n","                         -e $SPECS_DIR/maskrcnn_train_resnet50.txt \\\n","                         -m $EXPERIMENT_DIR/experiment_dir_unpruned/model.epoch-$NUM_EPOCH.tlt \\\n","                         -t 0.5 \\\n","                         -k $KEY \\\n","                         --include_mask"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ge5hsnU99bt4"},"outputs":[],"source":["# Simple grid visualizer\n","import matplotlib.pyplot as plt\n","import os\n","from math import ceil\n","valid_image_ext = ['.jpg']\n","\n","def visualize_images(image_dir, num_cols=4, num_images=10):\n","    output_path = os.path.join(os.environ['EXPERIMENT_DIR'], image_dir)\n","    num_rows = int(ceil(float(num_images) / float(num_cols)))\n","    f, axarr = plt.subplots(num_rows, num_cols, figsize=[80,30])\n","    f.tight_layout()\n","    a = [os.path.join(output_path, image) for image in os.listdir(output_path) \n","         if os.path.splitext(image)[1].lower() in valid_image_ext]\n","    for idx, img_path in enumerate(a[:num_images]):\n","        col_id = idx % num_cols\n","        row_id = idx // num_cols\n","        img = plt.imread(img_path)\n","        axarr[row_id, col_id].imshow(img) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"03AP0N2l9bt4","scrolled":true},"outputs":[],"source":["# Visualizing the sample images.\n","OUTPUT_PATH = 'maskrcnn_annotated_images/images_annotated' # relative path from $EXPERIMENT_DIR.\n","COLS = 2 # number of columns in the visualizer grid.\n","IMAGES = 4 # number of images to visualize.\n","\n","visualize_images(OUTPUT_PATH, num_cols=COLS, num_images=IMAGES)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["q2rR7wIZ9bt3"],"name":"maskrcnn.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.10.2 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.18"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":0}
